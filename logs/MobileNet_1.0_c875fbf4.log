09:25:33: Namespace(batch_size=100, bench=True, data='cifar', decay_frequency=25000, decay_schedule='cosine', dense=True, density=1.0, epochs=100, fp16=False, growth='momentum', iters=1, l1=0.0, l2=0.0005, log_interval=100, lr=0.1, max_threads=10, model='MobileNet', momentum=0.9, no_cuda=False, optimizer='sgd', prune='magnitude', prune_rate=0.7, redistribution='momentum', resume=None, save_features=False, save_model='./models/model.pt', seed=17, start_epoch=1, test_batch_size=100, valid_split=0.1, verbose=True)
09:25:33: 


09:25:33: ================================================================================
09:25:33: 
Iteration start: 1/1

09:25:34: MobileNet(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (dw2_1): DepthWiseBlock(
    (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
    (bn_dw): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw2_2): DepthWiseBlock(
    (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
    (bn_dw): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw3_1): DepthWiseBlock(
    (conv_dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
    (bn_dw): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw3_2): DepthWiseBlock(
    (conv_dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
    (bn_dw): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw4_1): DepthWiseBlock(
    (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
    (bn_dw): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw4_2): DepthWiseBlock(
    (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
    (bn_dw): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_1): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_2): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_3): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_4): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_5): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_6): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw6): DepthWiseBlock(
    (conv_dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)
    (bn_dw): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (fc): Linear(in_features=1024, out_features=100, bias=True)
)
09:25:34: ============================================================
09:25:34: MobileNet
09:25:34: ============================================================
09:25:34: ============================================================
09:25:34: Prune mode: magnitude
09:25:34: Growth mode: momentum
09:25:34: Redistribution mode: momentum
09:25:34: ============================================================
09:25:35: Train Epoch: 1 [0/45000 (0%)]	Loss: -0.048067
09:25:40: Train Epoch: 1 [10000/45000 (22%)]	Loss: -14982227951616.000000
09:25:44: Train Epoch: 1 [20000/45000 (44%)]	Loss: nan
09:25:48: Train Epoch: 1 [30000/45000 (67%)]	Loss: nan
09:25:53: Train Epoch: 1 [40000/45000 (89%)]	Loss: nan
09:25:57: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:25:57: Current learning rate: 0.1. Time taken for epoch: 23.15 seconds.

09:25:57: Train Epoch: 2 [0/45000 (0%)]	Loss: nan
09:26:03: Train Epoch: 2 [10000/45000 (22%)]	Loss: nan
09:26:07: Train Epoch: 2 [20000/45000 (44%)]	Loss: nan
09:26:11: Train Epoch: 2 [30000/45000 (67%)]	Loss: nan
09:26:16: Train Epoch: 2 [40000/45000 (89%)]	Loss: nan
09:26:20: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:26:20: Current learning rate: 0.1. Time taken for epoch: 23.02 seconds.

09:26:21: Train Epoch: 3 [0/45000 (0%)]	Loss: nan
09:26:25: Train Epoch: 3 [10000/45000 (22%)]	Loss: nan
09:26:30: Train Epoch: 3 [20000/45000 (44%)]	Loss: nan
09:26:34: Train Epoch: 3 [30000/45000 (67%)]	Loss: nan
09:26:38: Train Epoch: 3 [40000/45000 (89%)]	Loss: nan
09:26:42: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:26:42: Current learning rate: 0.1. Time taken for epoch: 21.97 seconds.

09:26:43: Train Epoch: 4 [0/45000 (0%)]	Loss: nan
09:26:47: Train Epoch: 4 [10000/45000 (22%)]	Loss: nan
09:26:52: Train Epoch: 4 [20000/45000 (44%)]	Loss: nan
09:26:57: Train Epoch: 4 [30000/45000 (67%)]	Loss: nan
09:27:01: Train Epoch: 4 [40000/45000 (89%)]	Loss: nan
09:27:05: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:27:05: Current learning rate: 0.1. Time taken for epoch: 23.02 seconds.

09:27:06: Train Epoch: 5 [0/45000 (0%)]	Loss: nan
09:27:10: Train Epoch: 5 [10000/45000 (22%)]	Loss: nan
09:27:15: Train Epoch: 5 [20000/45000 (44%)]	Loss: nan
09:27:20: Train Epoch: 5 [30000/45000 (67%)]	Loss: nan
09:27:24: Train Epoch: 5 [40000/45000 (89%)]	Loss: nan
09:27:28: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:27:28: Current learning rate: 0.1. Time taken for epoch: 22.47 seconds.

09:27:28: Train Epoch: 6 [0/45000 (0%)]	Loss: nan
09:27:33: Train Epoch: 6 [10000/45000 (22%)]	Loss: nan
09:27:37: Train Epoch: 6 [20000/45000 (44%)]	Loss: nan
09:27:41: Train Epoch: 6 [30000/45000 (67%)]	Loss: nan
09:27:46: Train Epoch: 6 [40000/45000 (89%)]	Loss: nan
09:27:49: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:27:50: Current learning rate: 0.1. Time taken for epoch: 21.96 seconds.

09:27:50: Train Epoch: 7 [0/45000 (0%)]	Loss: nan
09:27:55: Train Epoch: 7 [10000/45000 (22%)]	Loss: nan
09:28:00: Train Epoch: 7 [20000/45000 (44%)]	Loss: nan
09:28:04: Train Epoch: 7 [30000/45000 (67%)]	Loss: nan
09:28:09: Train Epoch: 7 [40000/45000 (89%)]	Loss: nan
09:28:13: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:28:13: Current learning rate: 0.1. Time taken for epoch: 23.19 seconds.

09:28:13: Train Epoch: 8 [0/45000 (0%)]	Loss: nan
09:28:18: Train Epoch: 8 [10000/45000 (22%)]	Loss: nan
09:28:22: Train Epoch: 8 [20000/45000 (44%)]	Loss: nan
09:28:27: Train Epoch: 8 [30000/45000 (67%)]	Loss: nan
09:28:31: Train Epoch: 8 [40000/45000 (89%)]	Loss: nan
09:28:35: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:28:35: Current learning rate: 0.1. Time taken for epoch: 21.96 seconds.

09:28:35: Train Epoch: 9 [0/45000 (0%)]	Loss: nan
09:28:40: Train Epoch: 9 [10000/45000 (22%)]	Loss: nan
09:28:44: Train Epoch: 9 [20000/45000 (44%)]	Loss: nan
09:28:49: Train Epoch: 9 [30000/45000 (67%)]	Loss: nan
09:28:53: Train Epoch: 9 [40000/45000 (89%)]	Loss: nan
09:28:57: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:28:57: Current learning rate: 0.1. Time taken for epoch: 22.67 seconds.

09:28:58: Train Epoch: 10 [0/45000 (0%)]	Loss: nan
09:29:03: Train Epoch: 10 [10000/45000 (22%)]	Loss: nan
09:29:07: Train Epoch: 10 [20000/45000 (44%)]	Loss: nan
09:29:11: Train Epoch: 10 [30000/45000 (67%)]	Loss: nan
09:29:16: Train Epoch: 10 [40000/45000 (89%)]	Loss: nan
09:29:20: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:29:20: Current learning rate: 0.1. Time taken for epoch: 22.56 seconds.

09:29:21: Train Epoch: 11 [0/45000 (0%)]	Loss: nan
09:29:25: Train Epoch: 11 [10000/45000 (22%)]	Loss: nan
09:29:29: Train Epoch: 11 [20000/45000 (44%)]	Loss: nan
09:29:34: Train Epoch: 11 [30000/45000 (67%)]	Loss: nan
09:29:38: Train Epoch: 11 [40000/45000 (89%)]	Loss: nan
09:29:42: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:29:42: Current learning rate: 0.1. Time taken for epoch: 21.89 seconds.

09:29:42: Train Epoch: 12 [0/45000 (0%)]	Loss: nan
09:29:47: Train Epoch: 12 [10000/45000 (22%)]	Loss: nan
09:29:51: Train Epoch: 12 [20000/45000 (44%)]	Loss: nan
09:29:56: Train Epoch: 12 [30000/45000 (67%)]	Loss: nan
09:30:01: Train Epoch: 12 [40000/45000 (89%)]	Loss: nan
09:30:05: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:30:05: Current learning rate: 0.1. Time taken for epoch: 23.21 seconds.

09:30:05: Train Epoch: 13 [0/45000 (0%)]	Loss: nan
09:30:10: Train Epoch: 13 [10000/45000 (22%)]	Loss: nan
09:30:15: Train Epoch: 13 [20000/45000 (44%)]	Loss: nan
09:30:19: Train Epoch: 13 [30000/45000 (67%)]	Loss: nan
09:30:24: Train Epoch: 13 [40000/45000 (89%)]	Loss: nan
09:30:27: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:30:27: Current learning rate: 0.1. Time taken for epoch: 22.35 seconds.

09:30:28: Train Epoch: 14 [0/45000 (0%)]	Loss: nan
09:30:32: Train Epoch: 14 [10000/45000 (22%)]	Loss: nan
09:30:37: Train Epoch: 14 [20000/45000 (44%)]	Loss: nan
09:30:41: Train Epoch: 14 [30000/45000 (67%)]	Loss: nan
09:30:46: Train Epoch: 14 [40000/45000 (89%)]	Loss: nan
09:30:49: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:30:49: Current learning rate: 0.1. Time taken for epoch: 21.99 seconds.

09:30:50: Train Epoch: 15 [0/45000 (0%)]	Loss: nan
09:30:55: Train Epoch: 15 [10000/45000 (22%)]	Loss: nan
09:30:59: Train Epoch: 15 [20000/45000 (44%)]	Loss: nan
09:31:04: Train Epoch: 15 [30000/45000 (67%)]	Loss: nan
09:31:08: Train Epoch: 15 [40000/45000 (89%)]	Loss: nan
09:31:13: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:31:13: Current learning rate: 0.1. Time taken for epoch: 23.20 seconds.

09:31:13: Train Epoch: 16 [0/45000 (0%)]	Loss: nan
09:31:18: Train Epoch: 16 [10000/45000 (22%)]	Loss: nan
09:31:22: Train Epoch: 16 [20000/45000 (44%)]	Loss: nan
09:31:27: Train Epoch: 16 [30000/45000 (67%)]	Loss: nan
09:31:31: Train Epoch: 16 [40000/45000 (89%)]	Loss: nan
09:31:35: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:31:35: Current learning rate: 0.1. Time taken for epoch: 22.35 seconds.

09:31:35: Train Epoch: 17 [0/45000 (0%)]	Loss: nan
09:31:40: Train Epoch: 17 [10000/45000 (22%)]	Loss: nan
09:31:44: Train Epoch: 17 [20000/45000 (44%)]	Loss: nan
09:31:49: Train Epoch: 17 [30000/45000 (67%)]	Loss: nan
09:31:53: Train Epoch: 17 [40000/45000 (89%)]	Loss: nan
09:31:57: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:31:57: Current learning rate: 0.1. Time taken for epoch: 22.55 seconds.

09:31:58: Train Epoch: 18 [0/45000 (0%)]	Loss: nan
09:32:03: Train Epoch: 18 [10000/45000 (22%)]	Loss: nan
09:32:07: Train Epoch: 18 [20000/45000 (44%)]	Loss: nan
09:32:12: Train Epoch: 18 [30000/45000 (67%)]	Loss: nan
09:32:16: Train Epoch: 18 [40000/45000 (89%)]	Loss: nan
09:32:20: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:32:20: Current learning rate: 0.1. Time taken for epoch: 22.61 seconds.

09:32:21: Train Epoch: 19 [0/45000 (0%)]	Loss: nan
09:32:25: Train Epoch: 19 [10000/45000 (22%)]	Loss: nan
09:32:30: Train Epoch: 19 [20000/45000 (44%)]	Loss: nan
09:32:34: Train Epoch: 19 [30000/45000 (67%)]	Loss: nan
09:32:38: Train Epoch: 19 [40000/45000 (89%)]	Loss: nan
09:32:42: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:32:42: Current learning rate: 0.1. Time taken for epoch: 22.10 seconds.

09:32:43: Train Epoch: 20 [0/45000 (0%)]	Loss: nan
09:32:47: Train Epoch: 20 [10000/45000 (22%)]	Loss: nan
09:32:52: Train Epoch: 20 [20000/45000 (44%)]	Loss: nan
09:32:57: Train Epoch: 20 [30000/45000 (67%)]	Loss: nan
09:33:02: Train Epoch: 20 [40000/45000 (89%)]	Loss: nan
09:33:05: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:33:05: Current learning rate: 0.1. Time taken for epoch: 23.02 seconds.

09:33:06: Train Epoch: 21 [0/45000 (0%)]	Loss: nan
09:33:10: Train Epoch: 21 [10000/45000 (22%)]	Loss: nan
09:33:15: Train Epoch: 21 [20000/45000 (44%)]	Loss: nan
09:33:19: Train Epoch: 21 [30000/45000 (67%)]	Loss: nan
09:33:24: Train Epoch: 21 [40000/45000 (89%)]	Loss: nan
09:33:27: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:33:27: Current learning rate: 0.1. Time taken for epoch: 22.29 seconds.

09:33:28: Train Epoch: 22 [0/45000 (0%)]	Loss: nan
09:33:33: Train Epoch: 22 [10000/45000 (22%)]	Loss: nan
09:33:37: Train Epoch: 22 [20000/45000 (44%)]	Loss: nan
09:33:41: Train Epoch: 22 [30000/45000 (67%)]	Loss: nan
09:33:46: Train Epoch: 22 [40000/45000 (89%)]	Loss: nan
09:33:49: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:33:49: Current learning rate: 0.1. Time taken for epoch: 21.97 seconds.

09:33:50: Train Epoch: 23 [0/45000 (0%)]	Loss: nan
09:33:55: Train Epoch: 23 [10000/45000 (22%)]	Loss: nan
09:34:00: Train Epoch: 23 [20000/45000 (44%)]	Loss: nan
09:34:04: Train Epoch: 23 [30000/45000 (67%)]	Loss: nan
09:34:09: Train Epoch: 23 [40000/45000 (89%)]	Loss: nan
09:34:13: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:34:13: Current learning rate: 0.1. Time taken for epoch: 23.33 seconds.

09:34:13: Train Epoch: 24 [0/45000 (0%)]	Loss: nan
09:34:18: Train Epoch: 24 [10000/45000 (22%)]	Loss: nan
09:34:22: Train Epoch: 24 [20000/45000 (44%)]	Loss: nan
09:34:27: Train Epoch: 24 [30000/45000 (67%)]	Loss: nan
09:34:31: Train Epoch: 24 [40000/45000 (89%)]	Loss: nan
09:34:35: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:34:35: Current learning rate: 0.1. Time taken for epoch: 21.84 seconds.

09:34:35: Train Epoch: 25 [0/45000 (0%)]	Loss: nan
09:34:40: Train Epoch: 25 [10000/45000 (22%)]	Loss: nan
09:34:44: Train Epoch: 25 [20000/45000 (44%)]	Loss: nan
09:34:49: Train Epoch: 25 [30000/45000 (67%)]	Loss: nan
09:34:53: Train Epoch: 25 [40000/45000 (89%)]	Loss: nan
09:34:57: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:34:57: Current learning rate: 0.1. Time taken for epoch: 22.56 seconds.

09:34:58: Train Epoch: 26 [0/45000 (0%)]	Loss: nan
09:35:03: Train Epoch: 26 [10000/45000 (22%)]	Loss: nan
09:35:07: Train Epoch: 26 [20000/45000 (44%)]	Loss: nan
09:35:12: Train Epoch: 26 [30000/45000 (67%)]	Loss: nan
09:35:17: Train Epoch: 26 [40000/45000 (89%)]	Loss: nan
09:35:20: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:35:20: Current learning rate: 0.1. Time taken for epoch: 23.17 seconds.

09:35:21: Train Epoch: 27 [0/45000 (0%)]	Loss: nan
09:35:26: Train Epoch: 27 [10000/45000 (22%)]	Loss: nan
09:35:30: Train Epoch: 27 [20000/45000 (44%)]	Loss: nan
09:35:34: Train Epoch: 27 [30000/45000 (67%)]	Loss: nan
09:35:38: Train Epoch: 27 [40000/45000 (89%)]	Loss: nan
09:35:42: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:35:42: Current learning rate: 0.1. Time taken for epoch: 21.88 seconds.

09:35:43: Train Epoch: 28 [0/45000 (0%)]	Loss: nan
09:35:47: Train Epoch: 28 [10000/45000 (22%)]	Loss: nan
09:35:52: Train Epoch: 28 [20000/45000 (44%)]	Loss: nan
09:35:57: Train Epoch: 28 [30000/45000 (67%)]	Loss: nan
09:36:01: Train Epoch: 28 [40000/45000 (89%)]	Loss: nan
09:36:05: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:36:05: Current learning rate: 0.1. Time taken for epoch: 22.81 seconds.

09:36:06: Train Epoch: 29 [0/45000 (0%)]	Loss: nan
09:36:10: Train Epoch: 29 [10000/45000 (22%)]	Loss: nan
09:36:15: Train Epoch: 29 [20000/45000 (44%)]	Loss: nan
09:36:19: Train Epoch: 29 [30000/45000 (67%)]	Loss: nan
09:36:24: Train Epoch: 29 [40000/45000 (89%)]	Loss: nan
09:36:27: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:36:27: Current learning rate: 0.1. Time taken for epoch: 22.19 seconds.

09:36:28: Train Epoch: 30 [0/45000 (0%)]	Loss: nan
09:36:33: Train Epoch: 30 [10000/45000 (22%)]	Loss: nan
09:36:37: Train Epoch: 30 [20000/45000 (44%)]	Loss: nan
09:36:41: Train Epoch: 30 [30000/45000 (67%)]	Loss: nan
09:36:46: Train Epoch: 30 [40000/45000 (89%)]	Loss: nan
09:36:50: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:36:50: Current learning rate: 0.1. Time taken for epoch: 22.37 seconds.

09:36:50: Train Epoch: 31 [0/45000 (0%)]	Loss: nan
09:36:55: Train Epoch: 31 [10000/45000 (22%)]	Loss: nan
09:37:00: Train Epoch: 31 [20000/45000 (44%)]	Loss: nan
09:37:04: Train Epoch: 31 [30000/45000 (67%)]	Loss: nan
09:37:09: Train Epoch: 31 [40000/45000 (89%)]	Loss: nan
09:37:13: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:37:13: Current learning rate: 0.1. Time taken for epoch: 23.23 seconds.

09:37:13: Train Epoch: 32 [0/45000 (0%)]	Loss: nan
09:37:18: Train Epoch: 32 [10000/45000 (22%)]	Loss: nan
09:37:22: Train Epoch: 32 [20000/45000 (44%)]	Loss: nan
09:37:27: Train Epoch: 32 [30000/45000 (67%)]	Loss: nan
09:37:31: Train Epoch: 32 [40000/45000 (89%)]	Loss: nan
09:37:35: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:37:35: Current learning rate: 0.1. Time taken for epoch: 21.88 seconds.

09:37:35: Train Epoch: 33 [0/45000 (0%)]	Loss: nan
09:37:40: Train Epoch: 33 [10000/45000 (22%)]	Loss: nan
09:37:44: Train Epoch: 33 [20000/45000 (44%)]	Loss: nan
09:37:49: Train Epoch: 33 [30000/45000 (67%)]	Loss: nan
09:37:53: Train Epoch: 33 [40000/45000 (89%)]	Loss: nan
09:37:57: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:37:57: Current learning rate: 0.1. Time taken for epoch: 22.65 seconds.

09:37:58: Train Epoch: 34 [0/45000 (0%)]	Loss: nan
09:38:03: Train Epoch: 34 [10000/45000 (22%)]	Loss: nan
09:38:07: Train Epoch: 34 [20000/45000 (44%)]	Loss: nan
09:38:11: Train Epoch: 34 [30000/45000 (67%)]	Loss: nan
09:38:16: Train Epoch: 34 [40000/45000 (89%)]	Loss: nan
09:38:20: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:38:20: Current learning rate: 0.1. Time taken for epoch: 22.68 seconds.

09:38:20: Train Epoch: 35 [0/45000 (0%)]	Loss: nan
09:38:25: Train Epoch: 35 [10000/45000 (22%)]	Loss: nan
09:38:30: Train Epoch: 35 [20000/45000 (44%)]	Loss: nan
09:38:34: Train Epoch: 35 [30000/45000 (67%)]	Loss: nan
09:38:38: Train Epoch: 35 [40000/45000 (89%)]	Loss: nan
09:38:42: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:38:42: Current learning rate: 0.1. Time taken for epoch: 21.89 seconds.

09:38:43: Train Epoch: 36 [0/45000 (0%)]	Loss: nan
09:38:47: Train Epoch: 36 [10000/45000 (22%)]	Loss: nan
09:38:52: Train Epoch: 36 [20000/45000 (44%)]	Loss: nan
09:38:57: Train Epoch: 36 [30000/45000 (67%)]	Loss: nan
09:39:01: Train Epoch: 36 [40000/45000 (89%)]	Loss: nan
09:39:05: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:39:05: Current learning rate: 0.1. Time taken for epoch: 23.20 seconds.

09:39:06: Train Epoch: 37 [0/45000 (0%)]	Loss: nan
09:39:10: Train Epoch: 37 [10000/45000 (22%)]	Loss: nan
09:39:15: Train Epoch: 37 [20000/45000 (44%)]	Loss: nan
09:39:20: Train Epoch: 37 [30000/45000 (67%)]	Loss: nan
09:39:24: Train Epoch: 37 [40000/45000 (89%)]	Loss: nan
09:39:28: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:39:28: Current learning rate: 0.1. Time taken for epoch: 22.73 seconds.

09:39:28: Train Epoch: 38 [0/45000 (0%)]	Loss: nan
09:39:33: Train Epoch: 38 [10000/45000 (22%)]	Loss: nan
09:39:37: Train Epoch: 38 [20000/45000 (44%)]	Loss: nan
09:39:42: Train Epoch: 38 [30000/45000 (67%)]	Loss: nan
09:39:46: Train Epoch: 38 [40000/45000 (89%)]	Loss: nan
09:39:50: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:39:50: Current learning rate: 0.1. Time taken for epoch: 22.16 seconds.

09:39:51: Train Epoch: 39 [0/45000 (0%)]	Loss: nan
09:39:56: Train Epoch: 39 [10000/45000 (22%)]	Loss: nan
09:40:00: Train Epoch: 39 [20000/45000 (44%)]	Loss: nan
09:40:05: Train Epoch: 39 [30000/45000 (67%)]	Loss: nan
09:40:09: Train Epoch: 39 [40000/45000 (89%)]	Loss: nan
09:40:13: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:40:13: Current learning rate: 0.1. Time taken for epoch: 23.38 seconds.

09:40:14: Train Epoch: 40 [0/45000 (0%)]	Loss: nan
09:40:19: Train Epoch: 40 [10000/45000 (22%)]	Loss: nan
09:40:23: Train Epoch: 40 [20000/45000 (44%)]	Loss: nan
09:40:27: Train Epoch: 40 [30000/45000 (67%)]	Loss: nan
09:40:32: Train Epoch: 40 [40000/45000 (89%)]	Loss: nan
09:40:36: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:40:36: Current learning rate: 0.1. Time taken for epoch: 22.15 seconds.

09:40:36: Train Epoch: 41 [0/45000 (0%)]	Loss: nan
09:40:41: Train Epoch: 41 [10000/45000 (22%)]	Loss: nan
09:40:45: Train Epoch: 41 [20000/45000 (44%)]	Loss: nan
09:40:50: Train Epoch: 41 [30000/45000 (67%)]	Loss: nan
09:40:54: Train Epoch: 41 [40000/45000 (89%)]	Loss: nan
09:40:58: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:40:58: Current learning rate: 0.1. Time taken for epoch: 22.81 seconds.

09:40:59: Train Epoch: 42 [0/45000 (0%)]	Loss: nan
09:41:04: Train Epoch: 42 [10000/45000 (22%)]	Loss: nan
09:41:08: Train Epoch: 42 [20000/45000 (44%)]	Loss: nan
09:41:13: Train Epoch: 42 [30000/45000 (67%)]	Loss: nan
09:41:17: Train Epoch: 42 [40000/45000 (89%)]	Loss: nan
09:41:21: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:41:21: Current learning rate: 0.1. Time taken for epoch: 22.70 seconds.

09:41:22: Train Epoch: 43 [0/45000 (0%)]	Loss: nan
09:41:26: Train Epoch: 43 [10000/45000 (22%)]	Loss: nan
09:41:31: Train Epoch: 43 [20000/45000 (44%)]	Loss: nan
09:41:35: Train Epoch: 43 [30000/45000 (67%)]	Loss: nan
09:41:39: Train Epoch: 43 [40000/45000 (89%)]	Loss: nan
09:41:43: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:41:43: Current learning rate: 0.1. Time taken for epoch: 22.18 seconds.

09:41:44: Train Epoch: 44 [0/45000 (0%)]	Loss: nan
09:41:49: Train Epoch: 44 [10000/45000 (22%)]	Loss: nan
09:41:53: Train Epoch: 44 [20000/45000 (44%)]	Loss: nan
09:42:34: Namespace(batch_size=100, bench=True, data='cifar', decay_frequency=25000, decay_schedule='cosine', dense=True, density=1.0, epochs=100, fp16=False, growth='momentum', iters=1, l1=0.0, l2=0.0005, log_interval=100, lr=0.1, max_threads=10, model='MobileNet', momentum=0.9, no_cuda=False, optimizer='sgd', prune='magnitude', prune_rate=0.7, redistribution='momentum', resume=None, save_features=False, save_model='./models/model.pt', seed=17, start_epoch=1, test_batch_size=100, valid_split=0.1, verbose=True)
09:42:34: Namespace(batch_size=100, bench=True, data='cifar', decay_frequency=25000, decay_schedule='cosine', dense=True, density=1.0, epochs=100, fp16=False, growth='momentum', iters=1, l1=0.0, l2=0.0005, log_interval=100, lr=0.1, max_threads=10, model='MobileNet', momentum=0.9, no_cuda=False, optimizer='sgd', prune='magnitude', prune_rate=0.7, redistribution='momentum', resume=None, save_features=False, save_model='./models/model.pt', seed=17, start_epoch=1, test_batch_size=100, valid_split=0.1, verbose=True)
09:42:34: 


09:42:34: 


09:42:34: ================================================================================
09:42:34: ================================================================================
09:42:34: 
Iteration start: 1/1

09:42:34: 
Iteration start: 1/1

09:42:35: MobileNet(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (dw2_1): DepthWiseBlock(
    (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
    (bn_dw): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw2_2): DepthWiseBlock(
    (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
    (bn_dw): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw3_1): DepthWiseBlock(
    (conv_dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
    (bn_dw): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw3_2): DepthWiseBlock(
    (conv_dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
    (bn_dw): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw4_1): DepthWiseBlock(
    (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
    (bn_dw): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw4_2): DepthWiseBlock(
    (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
    (bn_dw): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_1): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_2): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_3): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_4): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_5): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_6): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw6): DepthWiseBlock(
    (conv_dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)
    (bn_dw): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (fc): Linear(in_features=1024, out_features=100, bias=True)
)
09:42:35: MobileNet(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (dw2_1): DepthWiseBlock(
    (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
    (bn_dw): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw2_2): DepthWiseBlock(
    (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
    (bn_dw): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw3_1): DepthWiseBlock(
    (conv_dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
    (bn_dw): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw3_2): DepthWiseBlock(
    (conv_dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
    (bn_dw): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw4_1): DepthWiseBlock(
    (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
    (bn_dw): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw4_2): DepthWiseBlock(
    (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
    (bn_dw): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_1): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_2): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_3): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_4): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_5): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_6): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw6): DepthWiseBlock(
    (conv_dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)
    (bn_dw): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (fc): Linear(in_features=1024, out_features=100, bias=True)
)
09:42:35: ============================================================
09:42:35: ============================================================
09:42:35: MobileNet
09:42:35: MobileNet
09:42:35: ============================================================
09:42:35: ============================================================
09:42:35: ============================================================
09:42:35: ============================================================
09:42:35: Prune mode: magnitude
09:42:35: Prune mode: magnitude
09:42:35: Growth mode: momentum
09:42:35: Growth mode: momentum
09:42:35: Redistribution mode: momentum
09:42:35: Redistribution mode: momentum
09:42:35: ============================================================
09:42:35: ============================================================
09:42:36: Train Epoch: 1 [0/45000 (0%)]	Loss: -0.048067
09:42:36: Train Epoch: 1 [0/45000 (0%)]	Loss: -0.048067
09:42:40: Train Epoch: 1 [10000/45000 (22%)]	Loss: -14982227951616.000000
09:42:40: Train Epoch: 1 [10000/45000 (22%)]	Loss: -14982227951616.000000
09:42:45: Train Epoch: 1 [20000/45000 (44%)]	Loss: nan
09:42:45: Train Epoch: 1 [20000/45000 (44%)]	Loss: nan
09:42:49: Train Epoch: 1 [30000/45000 (67%)]	Loss: nan
09:42:49: Train Epoch: 1 [30000/45000 (67%)]	Loss: nan
09:42:53: Train Epoch: 1 [40000/45000 (89%)]	Loss: nan
09:42:53: Train Epoch: 1 [40000/45000 (89%)]	Loss: nan
09:42:58: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:42:58: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:42:58: Current learning rate: 0.1. Time taken for epoch: 22.69 seconds.

09:42:58: Current learning rate: 0.1. Time taken for epoch: 22.69 seconds.

09:42:58: Train Epoch: 2 [0/45000 (0%)]	Loss: nan
09:42:58: Train Epoch: 2 [0/45000 (0%)]	Loss: nan
09:43:03: Train Epoch: 2 [10000/45000 (22%)]	Loss: nan
09:43:03: Train Epoch: 2 [10000/45000 (22%)]	Loss: nan
09:43:08: Train Epoch: 2 [20000/45000 (44%)]	Loss: nan
09:43:08: Train Epoch: 2 [20000/45000 (44%)]	Loss: nan
09:43:12: Train Epoch: 2 [30000/45000 (67%)]	Loss: nan
09:43:12: Train Epoch: 2 [30000/45000 (67%)]	Loss: nan
09:43:17: Train Epoch: 2 [40000/45000 (89%)]	Loss: nan
09:43:17: Train Epoch: 2 [40000/45000 (89%)]	Loss: nan
09:43:21: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:43:21: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:43:21: Current learning rate: 0.1. Time taken for epoch: 22.81 seconds.

09:43:21: Current learning rate: 0.1. Time taken for epoch: 22.81 seconds.

09:43:21: Train Epoch: 3 [0/45000 (0%)]	Loss: nan
09:43:21: Train Epoch: 3 [0/45000 (0%)]	Loss: nan
09:43:26: Train Epoch: 3 [10000/45000 (22%)]	Loss: nan
09:43:26: Train Epoch: 3 [10000/45000 (22%)]	Loss: nan
09:43:30: Train Epoch: 3 [20000/45000 (44%)]	Loss: nan
09:43:30: Train Epoch: 3 [20000/45000 (44%)]	Loss: nan
09:43:34: Train Epoch: 3 [30000/45000 (67%)]	Loss: nan
09:43:34: Train Epoch: 3 [30000/45000 (67%)]	Loss: nan
09:43:39: Train Epoch: 3 [40000/45000 (89%)]	Loss: nan
09:43:39: Train Epoch: 3 [40000/45000 (89%)]	Loss: nan
09:43:43: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:43:43: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:43:43: Current learning rate: 0.1. Time taken for epoch: 22.02 seconds.

09:43:43: Current learning rate: 0.1. Time taken for epoch: 22.02 seconds.

09:43:43: Train Epoch: 4 [0/45000 (0%)]	Loss: nan
09:43:43: Train Epoch: 4 [0/45000 (0%)]	Loss: nan
09:43:48: Train Epoch: 4 [10000/45000 (22%)]	Loss: nan
09:43:48: Train Epoch: 4 [10000/45000 (22%)]	Loss: nan
09:43:52: Train Epoch: 4 [20000/45000 (44%)]	Loss: nan
09:43:52: Train Epoch: 4 [20000/45000 (44%)]	Loss: nan
09:43:57: Train Epoch: 4 [30000/45000 (67%)]	Loss: nan
09:43:57: Train Epoch: 4 [30000/45000 (67%)]	Loss: nan
09:44:02: Train Epoch: 4 [40000/45000 (89%)]	Loss: nan
09:44:02: Train Epoch: 4 [40000/45000 (89%)]	Loss: nan
09:44:05: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:44:05: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:44:05: Current learning rate: 0.1. Time taken for epoch: 22.86 seconds.

09:44:05: Current learning rate: 0.1. Time taken for epoch: 22.86 seconds.

09:44:06: Train Epoch: 5 [0/45000 (0%)]	Loss: nan
09:44:06: Train Epoch: 5 [0/45000 (0%)]	Loss: nan
09:44:11: Train Epoch: 5 [10000/45000 (22%)]	Loss: nan
09:44:11: Train Epoch: 5 [10000/45000 (22%)]	Loss: nan
09:44:15: Train Epoch: 5 [20000/45000 (44%)]	Loss: nan
09:44:15: Train Epoch: 5 [20000/45000 (44%)]	Loss: nan
09:44:20: Train Epoch: 5 [30000/45000 (67%)]	Loss: nan
09:44:20: Train Epoch: 5 [30000/45000 (67%)]	Loss: nan
09:44:24: Train Epoch: 5 [40000/45000 (89%)]	Loss: nan
09:44:24: Train Epoch: 5 [40000/45000 (89%)]	Loss: nan
09:44:28: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:44:28: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:44:28: Current learning rate: 0.1. Time taken for epoch: 22.35 seconds.

09:44:28: Current learning rate: 0.1. Time taken for epoch: 22.35 seconds.

09:44:28: Train Epoch: 6 [0/45000 (0%)]	Loss: nan
09:44:28: Train Epoch: 6 [0/45000 (0%)]	Loss: nan
09:44:33: Train Epoch: 6 [10000/45000 (22%)]	Loss: nan
09:44:33: Train Epoch: 6 [10000/45000 (22%)]	Loss: nan
09:44:37: Train Epoch: 6 [20000/45000 (44%)]	Loss: nan
09:44:37: Train Epoch: 6 [20000/45000 (44%)]	Loss: nan
09:44:42: Train Epoch: 6 [30000/45000 (67%)]	Loss: nan
09:44:42: Train Epoch: 6 [30000/45000 (67%)]	Loss: nan
09:44:46: Train Epoch: 6 [40000/45000 (89%)]	Loss: nan
09:44:46: Train Epoch: 6 [40000/45000 (89%)]	Loss: nan
09:44:50: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:44:50: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:44:50: Current learning rate: 0.1. Time taken for epoch: 22.27 seconds.

09:44:50: Current learning rate: 0.1. Time taken for epoch: 22.27 seconds.

09:44:51: Train Epoch: 7 [0/45000 (0%)]	Loss: nan
09:44:51: Train Epoch: 7 [0/45000 (0%)]	Loss: nan
09:44:56: Train Epoch: 7 [10000/45000 (22%)]	Loss: nan
09:44:56: Train Epoch: 7 [10000/45000 (22%)]	Loss: nan
09:45:00: Train Epoch: 7 [20000/45000 (44%)]	Loss: nan
09:45:00: Train Epoch: 7 [20000/45000 (44%)]	Loss: nan
09:45:05: Train Epoch: 7 [30000/45000 (67%)]	Loss: nan
09:45:05: Train Epoch: 7 [30000/45000 (67%)]	Loss: nan
09:45:09: Train Epoch: 7 [40000/45000 (89%)]	Loss: nan
09:45:09: Train Epoch: 7 [40000/45000 (89%)]	Loss: nan
09:45:13: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:45:13: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:45:13: Current learning rate: 0.1. Time taken for epoch: 23.27 seconds.

09:45:13: Current learning rate: 0.1. Time taken for epoch: 23.27 seconds.

09:45:14: Train Epoch: 8 [0/45000 (0%)]	Loss: nan
09:45:14: Train Epoch: 8 [0/45000 (0%)]	Loss: nan
09:45:19: Train Epoch: 8 [10000/45000 (22%)]	Loss: nan
09:45:19: Train Epoch: 8 [10000/45000 (22%)]	Loss: nan
09:45:23: Train Epoch: 8 [20000/45000 (44%)]	Loss: nan
09:45:23: Train Epoch: 8 [20000/45000 (44%)]	Loss: nan
09:45:27: Train Epoch: 8 [30000/45000 (67%)]	Loss: nan
09:45:27: Train Epoch: 8 [30000/45000 (67%)]	Loss: nan
09:45:32: Train Epoch: 8 [40000/45000 (89%)]	Loss: nan
09:45:32: Train Epoch: 8 [40000/45000 (89%)]	Loss: nan
09:45:35: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:45:35: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:45:35: Current learning rate: 0.1. Time taken for epoch: 22.09 seconds.

09:45:35: Current learning rate: 0.1. Time taken for epoch: 22.09 seconds.

09:45:36: Train Epoch: 9 [0/45000 (0%)]	Loss: nan
09:45:36: Train Epoch: 9 [0/45000 (0%)]	Loss: nan
09:45:41: Train Epoch: 9 [10000/45000 (22%)]	Loss: nan
09:45:41: Train Epoch: 9 [10000/45000 (22%)]	Loss: nan
09:45:45: Train Epoch: 9 [20000/45000 (44%)]	Loss: nan
09:45:45: Train Epoch: 9 [20000/45000 (44%)]	Loss: nan
09:45:49: Train Epoch: 9 [30000/45000 (67%)]	Loss: nan
09:45:49: Train Epoch: 9 [30000/45000 (67%)]	Loss: nan
09:45:54: Train Epoch: 9 [40000/45000 (89%)]	Loss: nan
09:45:54: Train Epoch: 9 [40000/45000 (89%)]	Loss: nan
09:45:58: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:45:58: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:45:58: Current learning rate: 0.1. Time taken for epoch: 22.65 seconds.

09:45:58: Current learning rate: 0.1. Time taken for epoch: 22.65 seconds.

09:45:59: Train Epoch: 10 [0/45000 (0%)]	Loss: nan
09:45:59: Train Epoch: 10 [0/45000 (0%)]	Loss: nan
09:46:04: Train Epoch: 10 [10000/45000 (22%)]	Loss: nan
09:46:04: Train Epoch: 10 [10000/45000 (22%)]	Loss: nan
09:46:08: Train Epoch: 10 [20000/45000 (44%)]	Loss: nan
09:46:08: Train Epoch: 10 [20000/45000 (44%)]	Loss: nan
09:46:13: Train Epoch: 10 [30000/45000 (67%)]	Loss: nan
09:46:13: Train Epoch: 10 [30000/45000 (67%)]	Loss: nan
09:46:17: Train Epoch: 10 [40000/45000 (89%)]	Loss: nan
09:46:17: Train Epoch: 10 [40000/45000 (89%)]	Loss: nan
09:46:21: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:46:21: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:46:21: Current learning rate: 0.1. Time taken for epoch: 22.63 seconds.

09:46:21: Current learning rate: 0.1. Time taken for epoch: 22.63 seconds.

09:46:21: Train Epoch: 11 [0/45000 (0%)]	Loss: nan
09:46:21: Train Epoch: 11 [0/45000 (0%)]	Loss: nan
09:46:26: Train Epoch: 11 [10000/45000 (22%)]	Loss: nan
09:46:26: Train Epoch: 11 [10000/45000 (22%)]	Loss: nan
09:46:30: Train Epoch: 11 [20000/45000 (44%)]	Loss: nan
09:46:30: Train Epoch: 11 [20000/45000 (44%)]	Loss: nan
09:46:35: Train Epoch: 11 [30000/45000 (67%)]	Loss: nan
09:46:35: Train Epoch: 11 [30000/45000 (67%)]	Loss: nan
09:46:39: Train Epoch: 11 [40000/45000 (89%)]	Loss: nan
09:46:39: Train Epoch: 11 [40000/45000 (89%)]	Loss: nan
09:46:43: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:46:43: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:46:43: Current learning rate: 0.1. Time taken for epoch: 22.07 seconds.

09:46:43: Current learning rate: 0.1. Time taken for epoch: 22.07 seconds.

09:46:43: Train Epoch: 12 [0/45000 (0%)]	Loss: nan
09:46:43: Train Epoch: 12 [0/45000 (0%)]	Loss: nan
09:46:48: Train Epoch: 12 [10000/45000 (22%)]	Loss: nan
09:46:48: Train Epoch: 12 [10000/45000 (22%)]	Loss: nan
09:46:52: Train Epoch: 12 [20000/45000 (44%)]	Loss: nan
09:46:52: Train Epoch: 12 [20000/45000 (44%)]	Loss: nan
09:46:58: Train Epoch: 12 [30000/45000 (67%)]	Loss: nan
09:46:58: Train Epoch: 12 [30000/45000 (67%)]	Loss: nan
09:47:02: Train Epoch: 12 [40000/45000 (89%)]	Loss: nan
09:47:02: Train Epoch: 12 [40000/45000 (89%)]	Loss: nan
09:47:06: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:47:06: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:47:06: Current learning rate: 0.1. Time taken for epoch: 23.42 seconds.

09:47:06: Current learning rate: 0.1. Time taken for epoch: 23.42 seconds.

09:47:07: Train Epoch: 13 [0/45000 (0%)]	Loss: nan
09:47:07: Train Epoch: 13 [0/45000 (0%)]	Loss: nan
09:47:11: Train Epoch: 13 [10000/45000 (22%)]	Loss: nan
09:47:11: Train Epoch: 13 [10000/45000 (22%)]	Loss: nan
09:47:16: Train Epoch: 13 [20000/45000 (44%)]	Loss: nan
09:47:16: Train Epoch: 13 [20000/45000 (44%)]	Loss: nan
09:47:21: Train Epoch: 13 [30000/45000 (67%)]	Loss: nan
09:47:21: Train Epoch: 13 [30000/45000 (67%)]	Loss: nan
09:47:25: Train Epoch: 13 [40000/45000 (89%)]	Loss: nan
09:47:25: Train Epoch: 13 [40000/45000 (89%)]	Loss: nan
09:47:29: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:47:29: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:47:29: Current learning rate: 0.1. Time taken for epoch: 22.66 seconds.

09:47:29: Current learning rate: 0.1. Time taken for epoch: 22.66 seconds.

09:47:30: Train Epoch: 14 [0/45000 (0%)]	Loss: nan
09:47:30: Train Epoch: 14 [0/45000 (0%)]	Loss: nan
09:47:34: Train Epoch: 14 [10000/45000 (22%)]	Loss: nan
09:47:34: Train Epoch: 14 [10000/45000 (22%)]	Loss: nan
09:47:38: Train Epoch: 14 [20000/45000 (44%)]	Loss: nan
09:47:38: Train Epoch: 14 [20000/45000 (44%)]	Loss: nan
09:47:43: Train Epoch: 14 [30000/45000 (67%)]	Loss: nan
09:47:43: Train Epoch: 14 [30000/45000 (67%)]	Loss: nan
09:47:47: Train Epoch: 14 [40000/45000 (89%)]	Loss: nan
09:47:47: Train Epoch: 14 [40000/45000 (89%)]	Loss: nan
09:47:51: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:47:51: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:47:51: Current learning rate: 0.1. Time taken for epoch: 22.10 seconds.

09:47:51: Current learning rate: 0.1. Time taken for epoch: 22.10 seconds.

09:47:52: Train Epoch: 15 [0/45000 (0%)]	Loss: nan
09:47:52: Train Epoch: 15 [0/45000 (0%)]	Loss: nan
09:47:57: Train Epoch: 15 [10000/45000 (22%)]	Loss: nan
09:47:57: Train Epoch: 15 [10000/45000 (22%)]	Loss: nan
09:48:01: Train Epoch: 15 [20000/45000 (44%)]	Loss: nan
09:48:01: Train Epoch: 15 [20000/45000 (44%)]	Loss: nan
09:48:06: Train Epoch: 15 [30000/45000 (67%)]	Loss: nan
09:48:06: Train Epoch: 15 [30000/45000 (67%)]	Loss: nan
09:48:10: Train Epoch: 15 [40000/45000 (89%)]	Loss: nan
09:48:10: Train Epoch: 15 [40000/45000 (89%)]	Loss: nan
09:48:14: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:48:14: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:48:14: Current learning rate: 0.1. Time taken for epoch: 23.19 seconds.

09:48:14: Current learning rate: 0.1. Time taken for epoch: 23.19 seconds.

09:48:15: Train Epoch: 16 [0/45000 (0%)]	Loss: nan
09:48:15: Train Epoch: 16 [0/45000 (0%)]	Loss: nan
09:48:19: Train Epoch: 16 [10000/45000 (22%)]	Loss: nan
09:48:19: Train Epoch: 16 [10000/45000 (22%)]	Loss: nan
09:48:24: Train Epoch: 16 [20000/45000 (44%)]	Loss: nan
09:48:24: Train Epoch: 16 [20000/45000 (44%)]	Loss: nan
09:48:28: Train Epoch: 16 [30000/45000 (67%)]	Loss: nan
09:48:28: Train Epoch: 16 [30000/45000 (67%)]	Loss: nan
09:48:32: Train Epoch: 16 [40000/45000 (89%)]	Loss: nan
09:48:32: Train Epoch: 16 [40000/45000 (89%)]	Loss: nan
09:48:36: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:48:36: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:48:36: Current learning rate: 0.1. Time taken for epoch: 21.99 seconds.

09:48:36: Current learning rate: 0.1. Time taken for epoch: 21.99 seconds.

09:48:37: Train Epoch: 17 [0/45000 (0%)]	Loss: nan
09:48:37: Train Epoch: 17 [0/45000 (0%)]	Loss: nan
09:48:42: Train Epoch: 17 [10000/45000 (22%)]	Loss: nan
09:48:42: Train Epoch: 17 [10000/45000 (22%)]	Loss: nan
09:48:46: Train Epoch: 17 [20000/45000 (44%)]	Loss: nan
09:48:46: Train Epoch: 17 [20000/45000 (44%)]	Loss: nan
09:48:50: Train Epoch: 17 [30000/45000 (67%)]	Loss: nan
09:48:50: Train Epoch: 17 [30000/45000 (67%)]	Loss: nan
09:48:55: Train Epoch: 17 [40000/45000 (89%)]	Loss: nan
09:48:55: Train Epoch: 17 [40000/45000 (89%)]	Loss: nan
09:48:59: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:48:59: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:48:59: Current learning rate: 0.1. Time taken for epoch: 22.83 seconds.

09:48:59: Current learning rate: 0.1. Time taken for epoch: 22.83 seconds.

09:49:00: Train Epoch: 18 [0/45000 (0%)]	Loss: nan
09:49:00: Train Epoch: 18 [0/45000 (0%)]	Loss: nan
09:49:04: Train Epoch: 18 [10000/45000 (22%)]	Loss: nan
09:49:04: Train Epoch: 18 [10000/45000 (22%)]	Loss: nan
09:49:09: Train Epoch: 18 [20000/45000 (44%)]	Loss: nan
09:49:09: Train Epoch: 18 [20000/45000 (44%)]	Loss: nan
09:49:14: Train Epoch: 18 [30000/45000 (67%)]	Loss: nan
09:49:14: Train Epoch: 18 [30000/45000 (67%)]	Loss: nan
09:49:18: Train Epoch: 18 [40000/45000 (89%)]	Loss: nan
09:49:18: Train Epoch: 18 [40000/45000 (89%)]	Loss: nan
09:49:22: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:49:22: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:49:22: Current learning rate: 0.1. Time taken for epoch: 22.70 seconds.

09:49:22: Current learning rate: 0.1. Time taken for epoch: 22.70 seconds.

09:49:22: Train Epoch: 19 [0/45000 (0%)]	Loss: nan
09:49:22: Train Epoch: 19 [0/45000 (0%)]	Loss: nan
09:49:27: Train Epoch: 19 [10000/45000 (22%)]	Loss: nan
09:49:27: Train Epoch: 19 [10000/45000 (22%)]	Loss: nan
09:49:31: Train Epoch: 19 [20000/45000 (44%)]	Loss: nan
09:49:31: Train Epoch: 19 [20000/45000 (44%)]	Loss: nan
09:49:36: Train Epoch: 19 [30000/45000 (67%)]	Loss: nan
09:49:36: Train Epoch: 19 [30000/45000 (67%)]	Loss: nan
09:49:40: Train Epoch: 19 [40000/45000 (89%)]	Loss: nan
09:49:40: Train Epoch: 19 [40000/45000 (89%)]	Loss: nan
09:49:44: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:49:44: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:49:44: Current learning rate: 0.1. Time taken for epoch: 21.99 seconds.

09:49:44: Current learning rate: 0.1. Time taken for epoch: 21.99 seconds.

09:49:44: Train Epoch: 20 [0/45000 (0%)]	Loss: nan
09:49:44: Train Epoch: 20 [0/45000 (0%)]	Loss: nan
09:49:49: Train Epoch: 20 [10000/45000 (22%)]	Loss: nan
09:49:49: Train Epoch: 20 [10000/45000 (22%)]	Loss: nan
09:49:53: Train Epoch: 20 [20000/45000 (44%)]	Loss: nan
09:49:53: Train Epoch: 20 [20000/45000 (44%)]	Loss: nan
09:49:58: Train Epoch: 20 [30000/45000 (67%)]	Loss: nan
09:49:58: Train Epoch: 20 [30000/45000 (67%)]	Loss: nan
09:50:03: Train Epoch: 20 [40000/45000 (89%)]	Loss: nan
09:50:03: Train Epoch: 20 [40000/45000 (89%)]	Loss: nan
09:50:07: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:50:07: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:50:07: Current learning rate: 0.1. Time taken for epoch: 22.94 seconds.

09:50:07: Current learning rate: 0.1. Time taken for epoch: 22.94 seconds.

09:50:07: Train Epoch: 21 [0/45000 (0%)]	Loss: nan
09:50:07: Train Epoch: 21 [0/45000 (0%)]	Loss: nan
09:50:12: Train Epoch: 21 [10000/45000 (22%)]	Loss: nan
09:50:12: Train Epoch: 21 [10000/45000 (22%)]	Loss: nan
09:50:16: Train Epoch: 21 [20000/45000 (44%)]	Loss: nan
09:50:16: Train Epoch: 21 [20000/45000 (44%)]	Loss: nan
09:50:21: Train Epoch: 21 [30000/45000 (67%)]	Loss: nan
09:50:21: Train Epoch: 21 [30000/45000 (67%)]	Loss: nan
09:50:25: Train Epoch: 21 [40000/45000 (89%)]	Loss: nan
09:50:25: Train Epoch: 21 [40000/45000 (89%)]	Loss: nan
09:50:29: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:50:29: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:50:29: Current learning rate: 0.1. Time taken for epoch: 22.39 seconds.

09:50:29: Current learning rate: 0.1. Time taken for epoch: 22.39 seconds.

09:50:29: Train Epoch: 22 [0/45000 (0%)]	Loss: nan
09:50:29: Train Epoch: 22 [0/45000 (0%)]	Loss: nan
09:50:34: Train Epoch: 22 [10000/45000 (22%)]	Loss: nan
09:50:34: Train Epoch: 22 [10000/45000 (22%)]	Loss: nan
09:50:38: Train Epoch: 22 [20000/45000 (44%)]	Loss: nan
09:50:38: Train Epoch: 22 [20000/45000 (44%)]	Loss: nan
09:50:43: Train Epoch: 22 [30000/45000 (67%)]	Loss: nan
09:50:43: Train Epoch: 22 [30000/45000 (67%)]	Loss: nan
09:50:47: Train Epoch: 22 [40000/45000 (89%)]	Loss: nan
09:50:47: Train Epoch: 22 [40000/45000 (89%)]	Loss: nan
09:50:51: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:50:51: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:50:51: Current learning rate: 0.1. Time taken for epoch: 22.07 seconds.

09:50:51: Current learning rate: 0.1. Time taken for epoch: 22.07 seconds.

09:50:52: Train Epoch: 23 [0/45000 (0%)]	Loss: nan
09:50:52: Train Epoch: 23 [0/45000 (0%)]	Loss: nan
09:50:57: Train Epoch: 23 [10000/45000 (22%)]	Loss: nan
09:50:57: Train Epoch: 23 [10000/45000 (22%)]	Loss: nan
09:51:01: Train Epoch: 23 [20000/45000 (44%)]	Loss: nan
09:51:01: Train Epoch: 23 [20000/45000 (44%)]	Loss: nan
09:51:06: Train Epoch: 23 [30000/45000 (67%)]	Loss: nan
09:51:06: Train Epoch: 23 [30000/45000 (67%)]	Loss: nan
09:51:10: Train Epoch: 23 [40000/45000 (89%)]	Loss: nan
09:51:10: Train Epoch: 23 [40000/45000 (89%)]	Loss: nan
09:51:14: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:51:14: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:51:14: Current learning rate: 0.1. Time taken for epoch: 23.08 seconds.

09:51:14: Current learning rate: 0.1. Time taken for epoch: 23.08 seconds.

09:51:15: Train Epoch: 24 [0/45000 (0%)]	Loss: nan
09:51:15: Train Epoch: 24 [0/45000 (0%)]	Loss: nan
09:51:19: Train Epoch: 24 [10000/45000 (22%)]	Loss: nan
09:51:19: Train Epoch: 24 [10000/45000 (22%)]	Loss: nan
09:51:24: Train Epoch: 24 [20000/45000 (44%)]	Loss: nan
09:51:24: Train Epoch: 24 [20000/45000 (44%)]	Loss: nan
09:51:28: Train Epoch: 24 [30000/45000 (67%)]	Loss: nan
09:51:28: Train Epoch: 24 [30000/45000 (67%)]	Loss: nan
09:51:33: Train Epoch: 24 [40000/45000 (89%)]	Loss: nan
09:51:33: Train Epoch: 24 [40000/45000 (89%)]	Loss: nan
09:51:36: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:51:36: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:51:36: Current learning rate: 0.1. Time taken for epoch: 22.01 seconds.

09:51:36: Current learning rate: 0.1. Time taken for epoch: 22.01 seconds.

09:51:37: Train Epoch: 25 [0/45000 (0%)]	Loss: nan
09:51:37: Train Epoch: 25 [0/45000 (0%)]	Loss: nan
09:51:41: Train Epoch: 25 [10000/45000 (22%)]	Loss: nan
09:51:41: Train Epoch: 25 [10000/45000 (22%)]	Loss: nan
09:51:46: Train Epoch: 25 [20000/45000 (44%)]	Loss: nan
09:51:46: Train Epoch: 25 [20000/45000 (44%)]	Loss: nan
09:51:50: Train Epoch: 25 [30000/45000 (67%)]	Loss: nan
09:51:50: Train Epoch: 25 [30000/45000 (67%)]	Loss: nan
09:51:55: Train Epoch: 25 [40000/45000 (89%)]	Loss: nan
09:51:55: Train Epoch: 25 [40000/45000 (89%)]	Loss: nan
09:51:59: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:51:59: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:51:59: Current learning rate: 0.1. Time taken for epoch: 22.76 seconds.

09:51:59: Current learning rate: 0.1. Time taken for epoch: 22.76 seconds.

09:51:59: Train Epoch: 26 [0/45000 (0%)]	Loss: nan
09:51:59: Train Epoch: 26 [0/45000 (0%)]	Loss: nan
09:52:04: Train Epoch: 26 [10000/45000 (22%)]	Loss: nan
09:52:04: Train Epoch: 26 [10000/45000 (22%)]	Loss: nan
09:52:09: Train Epoch: 26 [20000/45000 (44%)]	Loss: nan
09:52:09: Train Epoch: 26 [20000/45000 (44%)]	Loss: nan
09:52:14: Train Epoch: 26 [30000/45000 (67%)]	Loss: nan
09:52:14: Train Epoch: 26 [30000/45000 (67%)]	Loss: nan
09:52:18: Train Epoch: 26 [40000/45000 (89%)]	Loss: nan
09:52:18: Train Epoch: 26 [40000/45000 (89%)]	Loss: nan
09:52:22: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:52:22: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:52:22: Current learning rate: 0.1. Time taken for epoch: 22.93 seconds.

09:52:22: Current learning rate: 0.1. Time taken for epoch: 22.93 seconds.

09:52:23: Train Epoch: 27 [0/45000 (0%)]	Loss: nan
09:52:23: Train Epoch: 27 [0/45000 (0%)]	Loss: nan
09:52:27: Train Epoch: 27 [10000/45000 (22%)]	Loss: nan
09:52:27: Train Epoch: 27 [10000/45000 (22%)]	Loss: nan
09:52:31: Train Epoch: 27 [20000/45000 (44%)]	Loss: nan
09:52:31: Train Epoch: 27 [20000/45000 (44%)]	Loss: nan
09:52:36: Train Epoch: 27 [30000/45000 (67%)]	Loss: nan
09:52:36: Train Epoch: 27 [30000/45000 (67%)]	Loss: nan
09:52:40: Train Epoch: 27 [40000/45000 (89%)]	Loss: nan
09:52:40: Train Epoch: 27 [40000/45000 (89%)]	Loss: nan
09:52:44: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:52:44: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:52:44: Current learning rate: 0.1. Time taken for epoch: 22.14 seconds.

09:52:44: Current learning rate: 0.1. Time taken for epoch: 22.14 seconds.

09:52:45: Train Epoch: 28 [0/45000 (0%)]	Loss: nan
09:52:45: Train Epoch: 28 [0/45000 (0%)]	Loss: nan
09:52:49: Train Epoch: 28 [10000/45000 (22%)]	Loss: nan
09:52:49: Train Epoch: 28 [10000/45000 (22%)]	Loss: nan
09:52:54: Train Epoch: 28 [20000/45000 (44%)]	Loss: nan
09:52:54: Train Epoch: 28 [20000/45000 (44%)]	Loss: nan
09:52:58: Train Epoch: 28 [30000/45000 (67%)]	Loss: nan
09:52:58: Train Epoch: 28 [30000/45000 (67%)]	Loss: nan
09:53:03: Train Epoch: 28 [40000/45000 (89%)]	Loss: nan
09:53:03: Train Epoch: 28 [40000/45000 (89%)]	Loss: nan
09:53:07: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:53:07: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:53:07: Current learning rate: 0.1. Time taken for epoch: 22.78 seconds.

09:53:07: Current learning rate: 0.1. Time taken for epoch: 22.78 seconds.

09:53:07: Train Epoch: 29 [0/45000 (0%)]	Loss: nan
09:53:07: Train Epoch: 29 [0/45000 (0%)]	Loss: nan
09:53:12: Train Epoch: 29 [10000/45000 (22%)]	Loss: nan
09:53:12: Train Epoch: 29 [10000/45000 (22%)]	Loss: nan
09:53:17: Train Epoch: 29 [20000/45000 (44%)]	Loss: nan
09:53:17: Train Epoch: 29 [20000/45000 (44%)]	Loss: nan
09:53:21: Train Epoch: 29 [30000/45000 (67%)]	Loss: nan
09:53:21: Train Epoch: 29 [30000/45000 (67%)]	Loss: nan
09:53:25: Train Epoch: 29 [40000/45000 (89%)]	Loss: nan
09:53:25: Train Epoch: 29 [40000/45000 (89%)]	Loss: nan
09:53:29: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:53:29: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:53:29: Current learning rate: 0.1. Time taken for epoch: 22.32 seconds.

09:53:29: Current learning rate: 0.1. Time taken for epoch: 22.32 seconds.

09:53:30: Train Epoch: 30 [0/45000 (0%)]	Loss: nan
09:53:30: Train Epoch: 30 [0/45000 (0%)]	Loss: nan
09:53:34: Train Epoch: 30 [10000/45000 (22%)]	Loss: nan
09:53:34: Train Epoch: 30 [10000/45000 (22%)]	Loss: nan
09:53:39: Train Epoch: 30 [20000/45000 (44%)]	Loss: nan
09:53:39: Train Epoch: 30 [20000/45000 (44%)]	Loss: nan
09:53:43: Train Epoch: 30 [30000/45000 (67%)]	Loss: nan
09:53:43: Train Epoch: 30 [30000/45000 (67%)]	Loss: nan
09:53:47: Train Epoch: 30 [40000/45000 (89%)]	Loss: nan
09:53:47: Train Epoch: 30 [40000/45000 (89%)]	Loss: nan
09:53:51: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:53:51: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:53:51: Current learning rate: 0.1. Time taken for epoch: 22.06 seconds.

09:53:51: Current learning rate: 0.1. Time taken for epoch: 22.06 seconds.

09:53:52: Train Epoch: 31 [0/45000 (0%)]	Loss: nan
09:53:52: Train Epoch: 31 [0/45000 (0%)]	Loss: nan
09:53:57: Train Epoch: 31 [10000/45000 (22%)]	Loss: nan
09:53:57: Train Epoch: 31 [10000/45000 (22%)]	Loss: nan
09:54:02: Train Epoch: 31 [20000/45000 (44%)]	Loss: nan
09:54:02: Train Epoch: 31 [20000/45000 (44%)]	Loss: nan
09:54:06: Train Epoch: 31 [30000/45000 (67%)]	Loss: nan
09:54:06: Train Epoch: 31 [30000/45000 (67%)]	Loss: nan
09:54:10: Train Epoch: 31 [40000/45000 (89%)]	Loss: nan
09:54:10: Train Epoch: 31 [40000/45000 (89%)]	Loss: nan
09:54:14: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:54:14: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:54:14: Current learning rate: 0.1. Time taken for epoch: 23.15 seconds.

09:54:14: Current learning rate: 0.1. Time taken for epoch: 23.15 seconds.

09:54:15: Train Epoch: 32 [0/45000 (0%)]	Loss: nan
09:54:15: Train Epoch: 32 [0/45000 (0%)]	Loss: nan
09:54:19: Train Epoch: 32 [10000/45000 (22%)]	Loss: nan
09:54:19: Train Epoch: 32 [10000/45000 (22%)]	Loss: nan
09:54:24: Train Epoch: 32 [20000/45000 (44%)]	Loss: nan
09:54:24: Train Epoch: 32 [20000/45000 (44%)]	Loss: nan
09:54:28: Train Epoch: 32 [30000/45000 (67%)]	Loss: nan
09:54:28: Train Epoch: 32 [30000/45000 (67%)]	Loss: nan
09:54:32: Train Epoch: 32 [40000/45000 (89%)]	Loss: nan
09:54:32: Train Epoch: 32 [40000/45000 (89%)]	Loss: nan
09:54:36: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:54:36: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:54:36: Current learning rate: 0.1. Time taken for epoch: 21.76 seconds.

09:54:36: Current learning rate: 0.1. Time taken for epoch: 21.76 seconds.

09:54:37: Train Epoch: 33 [0/45000 (0%)]	Loss: nan
09:54:37: Train Epoch: 33 [0/45000 (0%)]	Loss: nan
09:54:41: Train Epoch: 33 [10000/45000 (22%)]	Loss: nan
09:54:41: Train Epoch: 33 [10000/45000 (22%)]	Loss: nan
09:54:46: Train Epoch: 33 [20000/45000 (44%)]	Loss: nan
09:54:46: Train Epoch: 33 [20000/45000 (44%)]	Loss: nan
09:54:50: Train Epoch: 33 [30000/45000 (67%)]	Loss: nan
09:54:50: Train Epoch: 33 [30000/45000 (67%)]	Loss: nan
09:54:55: Train Epoch: 33 [40000/45000 (89%)]	Loss: nan
09:54:55: Train Epoch: 33 [40000/45000 (89%)]	Loss: nan
09:54:59: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:54:59: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:54:59: Current learning rate: 0.1. Time taken for epoch: 22.64 seconds.

09:54:59: Current learning rate: 0.1. Time taken for epoch: 22.64 seconds.

09:54:59: Train Epoch: 34 [0/45000 (0%)]	Loss: nan
09:54:59: Train Epoch: 34 [0/45000 (0%)]	Loss: nan
09:55:04: Train Epoch: 34 [10000/45000 (22%)]	Loss: nan
09:55:04: Train Epoch: 34 [10000/45000 (22%)]	Loss: nan
09:55:08: Train Epoch: 34 [20000/45000 (44%)]	Loss: nan
09:55:08: Train Epoch: 34 [20000/45000 (44%)]	Loss: nan
09:55:13: Train Epoch: 34 [30000/45000 (67%)]	Loss: nan
09:55:13: Train Epoch: 34 [30000/45000 (67%)]	Loss: nan
09:55:18: Train Epoch: 34 [40000/45000 (89%)]	Loss: nan
09:55:18: Train Epoch: 34 [40000/45000 (89%)]	Loss: nan
09:55:21: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:55:21: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:55:21: Current learning rate: 0.1. Time taken for epoch: 22.64 seconds.

09:55:21: Current learning rate: 0.1. Time taken for epoch: 22.64 seconds.

09:55:22: Train Epoch: 35 [0/45000 (0%)]	Loss: nan
09:55:22: Train Epoch: 35 [0/45000 (0%)]	Loss: nan
09:55:27: Train Epoch: 35 [10000/45000 (22%)]	Loss: nan
09:55:27: Train Epoch: 35 [10000/45000 (22%)]	Loss: nan
09:55:31: Train Epoch: 35 [20000/45000 (44%)]	Loss: nan
09:55:31: Train Epoch: 35 [20000/45000 (44%)]	Loss: nan
09:55:35: Train Epoch: 35 [30000/45000 (67%)]	Loss: nan
09:55:35: Train Epoch: 35 [30000/45000 (67%)]	Loss: nan
09:55:40: Train Epoch: 35 [40000/45000 (89%)]	Loss: nan
09:55:40: Train Epoch: 35 [40000/45000 (89%)]	Loss: nan
09:55:43: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:55:43: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:55:44: Current learning rate: 0.1. Time taken for epoch: 22.19 seconds.

09:55:44: Current learning rate: 0.1. Time taken for epoch: 22.19 seconds.

09:55:44: Train Epoch: 36 [0/45000 (0%)]	Loss: nan
09:55:44: Train Epoch: 36 [0/45000 (0%)]	Loss: nan
09:55:49: Train Epoch: 36 [10000/45000 (22%)]	Loss: nan
09:55:49: Train Epoch: 36 [10000/45000 (22%)]	Loss: nan
09:55:53: Train Epoch: 36 [20000/45000 (44%)]	Loss: nan
09:55:53: Train Epoch: 36 [20000/45000 (44%)]	Loss: nan
09:55:58: Train Epoch: 36 [30000/45000 (67%)]	Loss: nan
09:55:58: Train Epoch: 36 [30000/45000 (67%)]	Loss: nan
09:56:03: Train Epoch: 36 [40000/45000 (89%)]	Loss: nan
09:56:03: Train Epoch: 36 [40000/45000 (89%)]	Loss: nan
09:56:06: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:56:06: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:56:06: Current learning rate: 0.1. Time taken for epoch: 22.74 seconds.

09:56:06: Current learning rate: 0.1. Time taken for epoch: 22.74 seconds.

09:56:07: Train Epoch: 37 [0/45000 (0%)]	Loss: nan
09:56:07: Train Epoch: 37 [0/45000 (0%)]	Loss: nan
09:56:11: Train Epoch: 37 [10000/45000 (22%)]	Loss: nan
09:56:11: Train Epoch: 37 [10000/45000 (22%)]	Loss: nan
09:56:16: Train Epoch: 37 [20000/45000 (44%)]	Loss: nan
09:56:16: Train Epoch: 37 [20000/45000 (44%)]	Loss: nan
09:56:20: Train Epoch: 37 [30000/45000 (67%)]	Loss: nan
09:56:20: Train Epoch: 37 [30000/45000 (67%)]	Loss: nan
09:56:25: Train Epoch: 37 [40000/45000 (89%)]	Loss: nan
09:56:25: Train Epoch: 37 [40000/45000 (89%)]	Loss: nan
09:56:28: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:56:28: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:56:29: Current learning rate: 0.1. Time taken for epoch: 22.27 seconds.

09:56:29: Current learning rate: 0.1. Time taken for epoch: 22.27 seconds.

09:56:29: Train Epoch: 38 [0/45000 (0%)]	Loss: nan
09:56:29: Train Epoch: 38 [0/45000 (0%)]	Loss: nan
09:56:34: Train Epoch: 38 [10000/45000 (22%)]	Loss: nan
09:56:34: Train Epoch: 38 [10000/45000 (22%)]	Loss: nan
09:56:38: Train Epoch: 38 [20000/45000 (44%)]	Loss: nan
09:56:38: Train Epoch: 38 [20000/45000 (44%)]	Loss: nan
09:56:43: Train Epoch: 38 [30000/45000 (67%)]	Loss: nan
09:56:43: Train Epoch: 38 [30000/45000 (67%)]	Loss: nan
09:56:47: Train Epoch: 38 [40000/45000 (89%)]	Loss: nan
09:56:47: Train Epoch: 38 [40000/45000 (89%)]	Loss: nan
09:56:51: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:56:51: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:56:51: Current learning rate: 0.1. Time taken for epoch: 22.20 seconds.

09:56:51: Current learning rate: 0.1. Time taken for epoch: 22.20 seconds.

09:56:51: Train Epoch: 39 [0/45000 (0%)]	Loss: nan
09:56:51: Train Epoch: 39 [0/45000 (0%)]	Loss: nan
09:56:57: Train Epoch: 39 [10000/45000 (22%)]	Loss: nan
09:56:57: Train Epoch: 39 [10000/45000 (22%)]	Loss: nan
09:57:01: Train Epoch: 39 [20000/45000 (44%)]	Loss: nan
09:57:01: Train Epoch: 39 [20000/45000 (44%)]	Loss: nan
09:57:05: Train Epoch: 39 [30000/45000 (67%)]	Loss: nan
09:57:05: Train Epoch: 39 [30000/45000 (67%)]	Loss: nan
09:57:10: Train Epoch: 39 [40000/45000 (89%)]	Loss: nan
09:57:10: Train Epoch: 39 [40000/45000 (89%)]	Loss: nan
09:57:14: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:57:14: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:57:14: Current learning rate: 0.1. Time taken for epoch: 23.19 seconds.

09:57:14: Current learning rate: 0.1. Time taken for epoch: 23.19 seconds.

09:57:14: Train Epoch: 40 [0/45000 (0%)]	Loss: nan
09:57:14: Train Epoch: 40 [0/45000 (0%)]	Loss: nan
09:57:19: Train Epoch: 40 [10000/45000 (22%)]	Loss: nan
09:57:19: Train Epoch: 40 [10000/45000 (22%)]	Loss: nan
09:57:24: Train Epoch: 40 [20000/45000 (44%)]	Loss: nan
09:57:24: Train Epoch: 40 [20000/45000 (44%)]	Loss: nan
09:57:28: Train Epoch: 40 [30000/45000 (67%)]	Loss: nan
09:57:28: Train Epoch: 40 [30000/45000 (67%)]	Loss: nan
09:57:32: Train Epoch: 40 [40000/45000 (89%)]	Loss: nan
09:57:32: Train Epoch: 40 [40000/45000 (89%)]	Loss: nan
09:57:36: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:57:36: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:57:36: Current learning rate: 0.1. Time taken for epoch: 22.13 seconds.

09:57:36: Current learning rate: 0.1. Time taken for epoch: 22.13 seconds.

09:57:37: Train Epoch: 41 [0/45000 (0%)]	Loss: nan
09:57:37: Train Epoch: 41 [0/45000 (0%)]	Loss: nan
09:57:41: Train Epoch: 41 [10000/45000 (22%)]	Loss: nan
09:57:41: Train Epoch: 41 [10000/45000 (22%)]	Loss: nan
09:57:46: Train Epoch: 41 [20000/45000 (44%)]	Loss: nan
09:57:46: Train Epoch: 41 [20000/45000 (44%)]	Loss: nan
09:57:50: Train Epoch: 41 [30000/45000 (67%)]	Loss: nan
09:57:50: Train Epoch: 41 [30000/45000 (67%)]	Loss: nan
09:57:55: Train Epoch: 41 [40000/45000 (89%)]	Loss: nan
09:57:55: Train Epoch: 41 [40000/45000 (89%)]	Loss: nan
09:57:59: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:57:59: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:57:59: Current learning rate: 0.1. Time taken for epoch: 22.74 seconds.

09:57:59: Current learning rate: 0.1. Time taken for epoch: 22.74 seconds.

09:57:59: Train Epoch: 42 [0/45000 (0%)]	Loss: nan
09:57:59: Train Epoch: 42 [0/45000 (0%)]	Loss: nan
09:58:04: Train Epoch: 42 [10000/45000 (22%)]	Loss: nan
09:58:04: Train Epoch: 42 [10000/45000 (22%)]	Loss: nan
09:58:09: Train Epoch: 42 [20000/45000 (44%)]	Loss: nan
09:58:09: Train Epoch: 42 [20000/45000 (44%)]	Loss: nan
09:59:03: Namespace(batch_size=100, bench=True, data='cifar', decay_frequency=25000, decay_schedule='cosine', dense=True, density=1.0, epochs=100, fp16=False, growth='momentum', iters=1, l1=0.0, l2=0.0005, log_interval=100, lr=0.1, max_threads=10, model='MobileNet', momentum=0.9, no_cuda=False, optimizer='sgd', prune='magnitude', prune_rate=0.7, redistribution='momentum', resume=None, save_features=False, save_model='./models/model.pt', seed=17, start_epoch=1, test_batch_size=100, valid_split=0.1, verbose=True)
09:59:03: Namespace(batch_size=100, bench=True, data='cifar', decay_frequency=25000, decay_schedule='cosine', dense=True, density=1.0, epochs=100, fp16=False, growth='momentum', iters=1, l1=0.0, l2=0.0005, log_interval=100, lr=0.1, max_threads=10, model='MobileNet', momentum=0.9, no_cuda=False, optimizer='sgd', prune='magnitude', prune_rate=0.7, redistribution='momentum', resume=None, save_features=False, save_model='./models/model.pt', seed=17, start_epoch=1, test_batch_size=100, valid_split=0.1, verbose=True)
09:59:03: Namespace(batch_size=100, bench=True, data='cifar', decay_frequency=25000, decay_schedule='cosine', dense=True, density=1.0, epochs=100, fp16=False, growth='momentum', iters=1, l1=0.0, l2=0.0005, log_interval=100, lr=0.1, max_threads=10, model='MobileNet', momentum=0.9, no_cuda=False, optimizer='sgd', prune='magnitude', prune_rate=0.7, redistribution='momentum', resume=None, save_features=False, save_model='./models/model.pt', seed=17, start_epoch=1, test_batch_size=100, valid_split=0.1, verbose=True)
09:59:03: 


09:59:03: 


09:59:03: 


09:59:03: ================================================================================
09:59:03: ================================================================================
09:59:03: ================================================================================
09:59:03: 
Iteration start: 1/1

09:59:03: 
Iteration start: 1/1

09:59:03: 
Iteration start: 1/1

09:59:04: MobileNet(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (dw2_1): DepthWiseBlock(
    (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
    (bn_dw): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw2_2): DepthWiseBlock(
    (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
    (bn_dw): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw3_1): DepthWiseBlock(
    (conv_dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
    (bn_dw): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw3_2): DepthWiseBlock(
    (conv_dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
    (bn_dw): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw4_1): DepthWiseBlock(
    (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
    (bn_dw): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw4_2): DepthWiseBlock(
    (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
    (bn_dw): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_1): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_2): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_3): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_4): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_5): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_6): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw6): DepthWiseBlock(
    (conv_dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)
    (bn_dw): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (fc): Linear(in_features=1024, out_features=100, bias=True)
)
09:59:04: MobileNet(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (dw2_1): DepthWiseBlock(
    (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
    (bn_dw): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw2_2): DepthWiseBlock(
    (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
    (bn_dw): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw3_1): DepthWiseBlock(
    (conv_dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
    (bn_dw): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw3_2): DepthWiseBlock(
    (conv_dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
    (bn_dw): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw4_1): DepthWiseBlock(
    (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
    (bn_dw): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw4_2): DepthWiseBlock(
    (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
    (bn_dw): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_1): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_2): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_3): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_4): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_5): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_6): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw6): DepthWiseBlock(
    (conv_dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)
    (bn_dw): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (fc): Linear(in_features=1024, out_features=100, bias=True)
)
09:59:04: MobileNet(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (dw2_1): DepthWiseBlock(
    (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
    (bn_dw): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw2_2): DepthWiseBlock(
    (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
    (bn_dw): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw3_1): DepthWiseBlock(
    (conv_dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
    (bn_dw): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw3_2): DepthWiseBlock(
    (conv_dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
    (bn_dw): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw4_1): DepthWiseBlock(
    (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
    (bn_dw): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw4_2): DepthWiseBlock(
    (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
    (bn_dw): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_1): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_2): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_3): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_4): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_5): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_6): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw6): DepthWiseBlock(
    (conv_dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)
    (bn_dw): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (fc): Linear(in_features=1024, out_features=100, bias=True)
)
09:59:04: ============================================================
09:59:04: ============================================================
09:59:04: ============================================================
09:59:04: MobileNet
09:59:04: MobileNet
09:59:04: MobileNet
09:59:04: ============================================================
09:59:04: ============================================================
09:59:04: ============================================================
09:59:04: ============================================================
09:59:04: ============================================================
09:59:04: ============================================================
09:59:04: Prune mode: magnitude
09:59:04: Prune mode: magnitude
09:59:04: Prune mode: magnitude
09:59:04: Growth mode: momentum
09:59:04: Growth mode: momentum
09:59:04: Growth mode: momentum
09:59:04: Redistribution mode: momentum
09:59:04: Redistribution mode: momentum
09:59:04: Redistribution mode: momentum
09:59:04: ============================================================
09:59:04: ============================================================
09:59:04: ============================================================
09:59:04: Train Epoch: 1 [0/45000 (0%)]	Loss: -0.048067
09:59:04: Train Epoch: 1 [0/45000 (0%)]	Loss: -0.048067
09:59:04: Train Epoch: 1 [0/45000 (0%)]	Loss: -0.048067
09:59:09: Train Epoch: 1 [10000/45000 (22%)]	Loss: -14982227951616.000000
09:59:09: Train Epoch: 1 [10000/45000 (22%)]	Loss: -14982227951616.000000
09:59:09: Train Epoch: 1 [10000/45000 (22%)]	Loss: -14982227951616.000000
09:59:14: Train Epoch: 1 [20000/45000 (44%)]	Loss: nan
09:59:14: Train Epoch: 1 [20000/45000 (44%)]	Loss: nan
09:59:14: Train Epoch: 1 [20000/45000 (44%)]	Loss: nan
09:59:18: Train Epoch: 1 [30000/45000 (67%)]	Loss: nan
09:59:18: Train Epoch: 1 [30000/45000 (67%)]	Loss: nan
09:59:18: Train Epoch: 1 [30000/45000 (67%)]	Loss: nan
09:59:23: Train Epoch: 1 [40000/45000 (89%)]	Loss: nan
09:59:23: Train Epoch: 1 [40000/45000 (89%)]	Loss: nan
09:59:23: Train Epoch: 1 [40000/45000 (89%)]	Loss: nan
09:59:27: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:59:27: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:59:27: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:59:27: Current learning rate: 0.1. Time taken for epoch: 22.66 seconds.

09:59:27: Current learning rate: 0.1. Time taken for epoch: 22.66 seconds.

09:59:27: Current learning rate: 0.1. Time taken for epoch: 22.66 seconds.

09:59:27: Train Epoch: 2 [0/45000 (0%)]	Loss: nan
09:59:27: Train Epoch: 2 [0/45000 (0%)]	Loss: nan
09:59:27: Train Epoch: 2 [0/45000 (0%)]	Loss: nan
09:59:32: Train Epoch: 2 [10000/45000 (22%)]	Loss: nan
09:59:32: Train Epoch: 2 [10000/45000 (22%)]	Loss: nan
09:59:32: Train Epoch: 2 [10000/45000 (22%)]	Loss: nan
09:59:36: Train Epoch: 2 [20000/45000 (44%)]	Loss: nan
09:59:36: Train Epoch: 2 [20000/45000 (44%)]	Loss: nan
09:59:36: Train Epoch: 2 [20000/45000 (44%)]	Loss: nan
09:59:41: Train Epoch: 2 [30000/45000 (67%)]	Loss: nan
09:59:41: Train Epoch: 2 [30000/45000 (67%)]	Loss: nan
09:59:41: Train Epoch: 2 [30000/45000 (67%)]	Loss: nan
09:59:45: Train Epoch: 2 [40000/45000 (89%)]	Loss: nan
09:59:45: Train Epoch: 2 [40000/45000 (89%)]	Loss: nan
09:59:45: Train Epoch: 2 [40000/45000 (89%)]	Loss: nan
09:59:49: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:59:49: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:59:49: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

09:59:49: Current learning rate: 0.1. Time taken for epoch: 22.15 seconds.

09:59:49: Current learning rate: 0.1. Time taken for epoch: 22.15 seconds.

09:59:49: Current learning rate: 0.1. Time taken for epoch: 22.15 seconds.

09:59:49: Train Epoch: 3 [0/45000 (0%)]	Loss: nan
09:59:49: Train Epoch: 3 [0/45000 (0%)]	Loss: nan
09:59:49: Train Epoch: 3 [0/45000 (0%)]	Loss: nan
09:59:54: Train Epoch: 3 [10000/45000 (22%)]	Loss: nan
09:59:54: Train Epoch: 3 [10000/45000 (22%)]	Loss: nan
09:59:54: Train Epoch: 3 [10000/45000 (22%)]	Loss: nan
09:59:59: Train Epoch: 3 [20000/45000 (44%)]	Loss: nan
09:59:59: Train Epoch: 3 [20000/45000 (44%)]	Loss: nan
09:59:59: Train Epoch: 3 [20000/45000 (44%)]	Loss: nan
10:00:04: Train Epoch: 3 [30000/45000 (67%)]	Loss: nan
10:00:04: Train Epoch: 3 [30000/45000 (67%)]	Loss: nan
10:00:04: Train Epoch: 3 [30000/45000 (67%)]	Loss: nan
10:00:08: Train Epoch: 3 [40000/45000 (89%)]	Loss: nan
10:00:08: Train Epoch: 3 [40000/45000 (89%)]	Loss: nan
10:00:08: Train Epoch: 3 [40000/45000 (89%)]	Loss: nan
10:00:12: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:00:12: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:00:12: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:00:12: Current learning rate: 0.1. Time taken for epoch: 23.05 seconds.

10:00:12: Current learning rate: 0.1. Time taken for epoch: 23.05 seconds.

10:00:12: Current learning rate: 0.1. Time taken for epoch: 23.05 seconds.

10:00:13: Train Epoch: 4 [0/45000 (0%)]	Loss: nan
10:00:13: Train Epoch: 4 [0/45000 (0%)]	Loss: nan
10:00:13: Train Epoch: 4 [0/45000 (0%)]	Loss: nan
10:00:17: Train Epoch: 4 [10000/45000 (22%)]	Loss: nan
10:00:17: Train Epoch: 4 [10000/45000 (22%)]	Loss: nan
10:00:17: Train Epoch: 4 [10000/45000 (22%)]	Loss: nan
10:00:22: Train Epoch: 4 [20000/45000 (44%)]	Loss: nan
10:00:22: Train Epoch: 4 [20000/45000 (44%)]	Loss: nan
10:00:22: Train Epoch: 4 [20000/45000 (44%)]	Loss: nan
10:00:26: Train Epoch: 4 [30000/45000 (67%)]	Loss: nan
10:00:26: Train Epoch: 4 [30000/45000 (67%)]	Loss: nan
10:00:26: Train Epoch: 4 [30000/45000 (67%)]	Loss: nan
10:00:30: Train Epoch: 4 [40000/45000 (89%)]	Loss: nan
10:00:30: Train Epoch: 4 [40000/45000 (89%)]	Loss: nan
10:00:30: Train Epoch: 4 [40000/45000 (89%)]	Loss: nan
10:00:34: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:00:34: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:00:34: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:00:34: Current learning rate: 0.1. Time taken for epoch: 22.44 seconds.

10:00:34: Current learning rate: 0.1. Time taken for epoch: 22.44 seconds.

10:00:34: Current learning rate: 0.1. Time taken for epoch: 22.44 seconds.

10:00:35: Train Epoch: 5 [0/45000 (0%)]	Loss: nan
10:00:35: Train Epoch: 5 [0/45000 (0%)]	Loss: nan
10:00:35: Train Epoch: 5 [0/45000 (0%)]	Loss: nan
10:00:40: Train Epoch: 5 [10000/45000 (22%)]	Loss: nan
10:00:40: Train Epoch: 5 [10000/45000 (22%)]	Loss: nan
10:00:40: Train Epoch: 5 [10000/45000 (22%)]	Loss: nan
10:00:44: Train Epoch: 5 [20000/45000 (44%)]	Loss: nan
10:00:44: Train Epoch: 5 [20000/45000 (44%)]	Loss: nan
10:00:44: Train Epoch: 5 [20000/45000 (44%)]	Loss: nan
10:00:49: Train Epoch: 5 [30000/45000 (67%)]	Loss: nan
10:00:49: Train Epoch: 5 [30000/45000 (67%)]	Loss: nan
10:00:49: Train Epoch: 5 [30000/45000 (67%)]	Loss: nan
10:00:53: Train Epoch: 5 [40000/45000 (89%)]	Loss: nan
10:00:53: Train Epoch: 5 [40000/45000 (89%)]	Loss: nan
10:00:53: Train Epoch: 5 [40000/45000 (89%)]	Loss: nan
10:00:57: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:00:57: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:00:57: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:00:57: Current learning rate: 0.1. Time taken for epoch: 23.15 seconds.

10:00:57: Current learning rate: 0.1. Time taken for epoch: 23.15 seconds.

10:00:57: Current learning rate: 0.1. Time taken for epoch: 23.15 seconds.

10:00:58: Train Epoch: 6 [0/45000 (0%)]	Loss: nan
10:00:58: Train Epoch: 6 [0/45000 (0%)]	Loss: nan
10:00:58: Train Epoch: 6 [0/45000 (0%)]	Loss: nan
10:01:03: Train Epoch: 6 [10000/45000 (22%)]	Loss: nan
10:01:03: Train Epoch: 6 [10000/45000 (22%)]	Loss: nan
10:01:03: Train Epoch: 6 [10000/45000 (22%)]	Loss: nan
10:01:07: Train Epoch: 6 [20000/45000 (44%)]	Loss: nan
10:01:07: Train Epoch: 6 [20000/45000 (44%)]	Loss: nan
10:01:07: Train Epoch: 6 [20000/45000 (44%)]	Loss: nan
10:01:12: Train Epoch: 6 [30000/45000 (67%)]	Loss: nan
10:01:12: Train Epoch: 6 [30000/45000 (67%)]	Loss: nan
10:01:12: Train Epoch: 6 [30000/45000 (67%)]	Loss: nan
10:01:17: Train Epoch: 6 [40000/45000 (89%)]	Loss: nan
10:01:17: Train Epoch: 6 [40000/45000 (89%)]	Loss: nan
10:01:17: Train Epoch: 6 [40000/45000 (89%)]	Loss: nan
10:01:20: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:01:20: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:01:20: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:01:20: Current learning rate: 0.1. Time taken for epoch: 22.85 seconds.

10:01:20: Current learning rate: 0.1. Time taken for epoch: 22.85 seconds.

10:01:20: Current learning rate: 0.1. Time taken for epoch: 22.85 seconds.

10:01:21: Train Epoch: 7 [0/45000 (0%)]	Loss: nan
10:01:21: Train Epoch: 7 [0/45000 (0%)]	Loss: nan
10:01:21: Train Epoch: 7 [0/45000 (0%)]	Loss: nan
10:01:25: Train Epoch: 7 [10000/45000 (22%)]	Loss: nan
10:01:25: Train Epoch: 7 [10000/45000 (22%)]	Loss: nan
10:01:25: Train Epoch: 7 [10000/45000 (22%)]	Loss: nan
10:01:30: Train Epoch: 7 [20000/45000 (44%)]	Loss: nan
10:01:30: Train Epoch: 7 [20000/45000 (44%)]	Loss: nan
10:01:30: Train Epoch: 7 [20000/45000 (44%)]	Loss: nan
10:01:34: Train Epoch: 7 [30000/45000 (67%)]	Loss: nan
10:01:34: Train Epoch: 7 [30000/45000 (67%)]	Loss: nan
10:01:34: Train Epoch: 7 [30000/45000 (67%)]	Loss: nan
10:01:39: Train Epoch: 7 [40000/45000 (89%)]	Loss: nan
10:01:39: Train Epoch: 7 [40000/45000 (89%)]	Loss: nan
10:01:39: Train Epoch: 7 [40000/45000 (89%)]	Loss: nan
10:01:43: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:01:43: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:01:43: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:01:43: Current learning rate: 0.1. Time taken for epoch: 22.28 seconds.

10:01:43: Current learning rate: 0.1. Time taken for epoch: 22.28 seconds.

10:01:43: Current learning rate: 0.1. Time taken for epoch: 22.28 seconds.

10:01:43: Train Epoch: 8 [0/45000 (0%)]	Loss: nan
10:01:43: Train Epoch: 8 [0/45000 (0%)]	Loss: nan
10:01:43: Train Epoch: 8 [0/45000 (0%)]	Loss: nan
10:01:48: Train Epoch: 8 [10000/45000 (22%)]	Loss: nan
10:01:48: Train Epoch: 8 [10000/45000 (22%)]	Loss: nan
10:01:48: Train Epoch: 8 [10000/45000 (22%)]	Loss: nan
10:01:52: Train Epoch: 8 [20000/45000 (44%)]	Loss: nan
10:01:52: Train Epoch: 8 [20000/45000 (44%)]	Loss: nan
10:01:52: Train Epoch: 8 [20000/45000 (44%)]	Loss: nan
10:01:57: Train Epoch: 8 [30000/45000 (67%)]	Loss: nan
10:01:57: Train Epoch: 8 [30000/45000 (67%)]	Loss: nan
10:01:57: Train Epoch: 8 [30000/45000 (67%)]	Loss: nan
10:02:02: Train Epoch: 8 [40000/45000 (89%)]	Loss: nan
10:02:02: Train Epoch: 8 [40000/45000 (89%)]	Loss: nan
10:02:02: Train Epoch: 8 [40000/45000 (89%)]	Loss: nan
10:02:06: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:02:06: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:02:06: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:02:06: Current learning rate: 0.1. Time taken for epoch: 23.34 seconds.

10:02:06: Current learning rate: 0.1. Time taken for epoch: 23.34 seconds.

10:02:06: Current learning rate: 0.1. Time taken for epoch: 23.34 seconds.

10:02:07: Train Epoch: 9 [0/45000 (0%)]	Loss: nan
10:02:07: Train Epoch: 9 [0/45000 (0%)]	Loss: nan
10:02:07: Train Epoch: 9 [0/45000 (0%)]	Loss: nan
10:02:11: Train Epoch: 9 [10000/45000 (22%)]	Loss: nan
10:02:11: Train Epoch: 9 [10000/45000 (22%)]	Loss: nan
10:02:11: Train Epoch: 9 [10000/45000 (22%)]	Loss: nan
10:02:16: Train Epoch: 9 [20000/45000 (44%)]	Loss: nan
10:02:16: Train Epoch: 9 [20000/45000 (44%)]	Loss: nan
10:02:16: Train Epoch: 9 [20000/45000 (44%)]	Loss: nan
10:02:20: Train Epoch: 9 [30000/45000 (67%)]	Loss: nan
10:02:20: Train Epoch: 9 [30000/45000 (67%)]	Loss: nan
10:02:20: Train Epoch: 9 [30000/45000 (67%)]	Loss: nan
10:02:25: Train Epoch: 9 [40000/45000 (89%)]	Loss: nan
10:02:25: Train Epoch: 9 [40000/45000 (89%)]	Loss: nan
10:02:25: Train Epoch: 9 [40000/45000 (89%)]	Loss: nan
10:02:29: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:02:29: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:02:29: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:02:29: Current learning rate: 0.1. Time taken for epoch: 22.87 seconds.

10:02:29: Current learning rate: 0.1. Time taken for epoch: 22.87 seconds.

10:02:29: Current learning rate: 0.1. Time taken for epoch: 22.87 seconds.

10:02:30: Train Epoch: 10 [0/45000 (0%)]	Loss: nan
10:02:30: Train Epoch: 10 [0/45000 (0%)]	Loss: nan
10:02:30: Train Epoch: 10 [0/45000 (0%)]	Loss: nan
10:02:34: Train Epoch: 10 [10000/45000 (22%)]	Loss: nan
10:02:34: Train Epoch: 10 [10000/45000 (22%)]	Loss: nan
10:02:34: Train Epoch: 10 [10000/45000 (22%)]	Loss: nan
10:02:39: Train Epoch: 10 [20000/45000 (44%)]	Loss: nan
10:02:39: Train Epoch: 10 [20000/45000 (44%)]	Loss: nan
10:02:39: Train Epoch: 10 [20000/45000 (44%)]	Loss: nan
10:02:43: Train Epoch: 10 [30000/45000 (67%)]	Loss: nan
10:02:43: Train Epoch: 10 [30000/45000 (67%)]	Loss: nan
10:02:43: Train Epoch: 10 [30000/45000 (67%)]	Loss: nan
10:02:47: Train Epoch: 10 [40000/45000 (89%)]	Loss: nan
10:02:47: Train Epoch: 10 [40000/45000 (89%)]	Loss: nan
10:02:47: Train Epoch: 10 [40000/45000 (89%)]	Loss: nan
10:02:51: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:02:51: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:02:51: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:02:51: Current learning rate: 0.1. Time taken for epoch: 22.48 seconds.

10:02:51: Current learning rate: 0.1. Time taken for epoch: 22.48 seconds.

10:02:51: Current learning rate: 0.1. Time taken for epoch: 22.48 seconds.

10:02:52: Train Epoch: 11 [0/45000 (0%)]	Loss: nan
10:02:52: Train Epoch: 11 [0/45000 (0%)]	Loss: nan
10:02:52: Train Epoch: 11 [0/45000 (0%)]	Loss: nan
10:02:57: Train Epoch: 11 [10000/45000 (22%)]	Loss: nan
10:02:57: Train Epoch: 11 [10000/45000 (22%)]	Loss: nan
10:02:57: Train Epoch: 11 [10000/45000 (22%)]	Loss: nan
10:03:02: Train Epoch: 11 [20000/45000 (44%)]	Loss: nan
10:03:02: Train Epoch: 11 [20000/45000 (44%)]	Loss: nan
10:03:02: Train Epoch: 11 [20000/45000 (44%)]	Loss: nan
10:03:06: Train Epoch: 11 [30000/45000 (67%)]	Loss: nan
10:03:06: Train Epoch: 11 [30000/45000 (67%)]	Loss: nan
10:03:06: Train Epoch: 11 [30000/45000 (67%)]	Loss: nan
10:03:10: Train Epoch: 11 [40000/45000 (89%)]	Loss: nan
10:03:10: Train Epoch: 11 [40000/45000 (89%)]	Loss: nan
10:03:10: Train Epoch: 11 [40000/45000 (89%)]	Loss: nan
10:03:14: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:03:14: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:03:14: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:03:14: Current learning rate: 0.1. Time taken for epoch: 23.20 seconds.

10:03:14: Current learning rate: 0.1. Time taken for epoch: 23.20 seconds.

10:03:14: Current learning rate: 0.1. Time taken for epoch: 23.20 seconds.

10:03:15: Train Epoch: 12 [0/45000 (0%)]	Loss: nan
10:03:15: Train Epoch: 12 [0/45000 (0%)]	Loss: nan
10:03:15: Train Epoch: 12 [0/45000 (0%)]	Loss: nan
10:03:20: Train Epoch: 12 [10000/45000 (22%)]	Loss: nan
10:03:20: Train Epoch: 12 [10000/45000 (22%)]	Loss: nan
10:03:20: Train Epoch: 12 [10000/45000 (22%)]	Loss: nan
10:03:24: Train Epoch: 12 [20000/45000 (44%)]	Loss: nan
10:03:24: Train Epoch: 12 [20000/45000 (44%)]	Loss: nan
10:03:24: Train Epoch: 12 [20000/45000 (44%)]	Loss: nan
10:03:29: Train Epoch: 12 [30000/45000 (67%)]	Loss: nan
10:03:29: Train Epoch: 12 [30000/45000 (67%)]	Loss: nan
10:03:29: Train Epoch: 12 [30000/45000 (67%)]	Loss: nan
10:03:33: Train Epoch: 12 [40000/45000 (89%)]	Loss: nan
10:03:33: Train Epoch: 12 [40000/45000 (89%)]	Loss: nan
10:03:33: Train Epoch: 12 [40000/45000 (89%)]	Loss: nan
10:03:37: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:03:37: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:03:37: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:03:37: Current learning rate: 0.1. Time taken for epoch: 22.50 seconds.

10:03:37: Current learning rate: 0.1. Time taken for epoch: 22.50 seconds.

10:03:37: Current learning rate: 0.1. Time taken for epoch: 22.50 seconds.

10:03:38: Train Epoch: 13 [0/45000 (0%)]	Loss: nan
10:03:38: Train Epoch: 13 [0/45000 (0%)]	Loss: nan
10:03:38: Train Epoch: 13 [0/45000 (0%)]	Loss: nan
10:03:42: Train Epoch: 13 [10000/45000 (22%)]	Loss: nan
10:03:42: Train Epoch: 13 [10000/45000 (22%)]	Loss: nan
10:03:42: Train Epoch: 13 [10000/45000 (22%)]	Loss: nan
10:03:47: Train Epoch: 13 [20000/45000 (44%)]	Loss: nan
10:03:47: Train Epoch: 13 [20000/45000 (44%)]	Loss: nan
10:03:47: Train Epoch: 13 [20000/45000 (44%)]	Loss: nan
10:03:51: Train Epoch: 13 [30000/45000 (67%)]	Loss: nan
10:03:51: Train Epoch: 13 [30000/45000 (67%)]	Loss: nan
10:03:51: Train Epoch: 13 [30000/45000 (67%)]	Loss: nan
10:03:56: Train Epoch: 13 [40000/45000 (89%)]	Loss: nan
10:03:56: Train Epoch: 13 [40000/45000 (89%)]	Loss: nan
10:03:56: Train Epoch: 13 [40000/45000 (89%)]	Loss: nan
10:04:00: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:04:00: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:04:00: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:04:00: Current learning rate: 0.1. Time taken for epoch: 23.00 seconds.

10:04:00: Current learning rate: 0.1. Time taken for epoch: 23.00 seconds.

10:04:00: Current learning rate: 0.1. Time taken for epoch: 23.00 seconds.

10:04:01: Train Epoch: 14 [0/45000 (0%)]	Loss: nan
10:04:01: Train Epoch: 14 [0/45000 (0%)]	Loss: nan
10:04:01: Train Epoch: 14 [0/45000 (0%)]	Loss: nan
10:04:05: Train Epoch: 14 [10000/45000 (22%)]	Loss: nan
10:04:05: Train Epoch: 14 [10000/45000 (22%)]	Loss: nan
10:04:05: Train Epoch: 14 [10000/45000 (22%)]	Loss: nan
10:04:10: Train Epoch: 14 [20000/45000 (44%)]	Loss: nan
10:04:10: Train Epoch: 14 [20000/45000 (44%)]	Loss: nan
10:04:10: Train Epoch: 14 [20000/45000 (44%)]	Loss: nan
10:04:15: Train Epoch: 14 [30000/45000 (67%)]	Loss: nan
10:04:15: Train Epoch: 14 [30000/45000 (67%)]	Loss: nan
10:04:15: Train Epoch: 14 [30000/45000 (67%)]	Loss: nan
10:04:19: Train Epoch: 14 [40000/45000 (89%)]	Loss: nan
10:04:19: Train Epoch: 14 [40000/45000 (89%)]	Loss: nan
10:04:19: Train Epoch: 14 [40000/45000 (89%)]	Loss: nan
10:04:23: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:04:23: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:04:23: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:04:23: Current learning rate: 0.1. Time taken for epoch: 23.11 seconds.

10:04:23: Current learning rate: 0.1. Time taken for epoch: 23.11 seconds.

10:04:23: Current learning rate: 0.1. Time taken for epoch: 23.11 seconds.

10:04:24: Train Epoch: 15 [0/45000 (0%)]	Loss: nan
10:04:24: Train Epoch: 15 [0/45000 (0%)]	Loss: nan
10:04:24: Train Epoch: 15 [0/45000 (0%)]	Loss: nan
10:04:28: Train Epoch: 15 [10000/45000 (22%)]	Loss: nan
10:04:28: Train Epoch: 15 [10000/45000 (22%)]	Loss: nan
10:04:28: Train Epoch: 15 [10000/45000 (22%)]	Loss: nan
10:04:33: Train Epoch: 15 [20000/45000 (44%)]	Loss: nan
10:04:33: Train Epoch: 15 [20000/45000 (44%)]	Loss: nan
10:04:33: Train Epoch: 15 [20000/45000 (44%)]	Loss: nan
10:04:37: Train Epoch: 15 [30000/45000 (67%)]	Loss: nan
10:04:37: Train Epoch: 15 [30000/45000 (67%)]	Loss: nan
10:04:37: Train Epoch: 15 [30000/45000 (67%)]	Loss: nan
10:04:42: Train Epoch: 15 [40000/45000 (89%)]	Loss: nan
10:04:42: Train Epoch: 15 [40000/45000 (89%)]	Loss: nan
10:04:42: Train Epoch: 15 [40000/45000 (89%)]	Loss: nan
10:04:46: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:04:46: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:04:46: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:04:46: Current learning rate: 0.1. Time taken for epoch: 22.63 seconds.

10:04:46: Current learning rate: 0.1. Time taken for epoch: 22.63 seconds.

10:04:46: Current learning rate: 0.1. Time taken for epoch: 22.63 seconds.

10:04:46: Train Epoch: 16 [0/45000 (0%)]	Loss: nan
10:04:46: Train Epoch: 16 [0/45000 (0%)]	Loss: nan
10:04:46: Train Epoch: 16 [0/45000 (0%)]	Loss: nan
10:04:51: Train Epoch: 16 [10000/45000 (22%)]	Loss: nan
10:04:51: Train Epoch: 16 [10000/45000 (22%)]	Loss: nan
10:04:51: Train Epoch: 16 [10000/45000 (22%)]	Loss: nan
10:04:56: Train Epoch: 16 [20000/45000 (44%)]	Loss: nan
10:04:56: Train Epoch: 16 [20000/45000 (44%)]	Loss: nan
10:04:56: Train Epoch: 16 [20000/45000 (44%)]	Loss: nan
10:05:00: Train Epoch: 16 [30000/45000 (67%)]	Loss: nan
10:05:00: Train Epoch: 16 [30000/45000 (67%)]	Loss: nan
10:05:00: Train Epoch: 16 [30000/45000 (67%)]	Loss: nan
10:05:05: Train Epoch: 16 [40000/45000 (89%)]	Loss: nan
10:05:05: Train Epoch: 16 [40000/45000 (89%)]	Loss: nan
10:05:05: Train Epoch: 16 [40000/45000 (89%)]	Loss: nan
10:05:09: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:05:09: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:05:09: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:05:09: Current learning rate: 0.1. Time taken for epoch: 23.06 seconds.

10:05:09: Current learning rate: 0.1. Time taken for epoch: 23.06 seconds.

10:05:09: Current learning rate: 0.1. Time taken for epoch: 23.06 seconds.

10:05:09: Train Epoch: 17 [0/45000 (0%)]	Loss: nan
10:05:09: Train Epoch: 17 [0/45000 (0%)]	Loss: nan
10:05:09: Train Epoch: 17 [0/45000 (0%)]	Loss: nan
10:05:14: Train Epoch: 17 [10000/45000 (22%)]	Loss: nan
10:05:14: Train Epoch: 17 [10000/45000 (22%)]	Loss: nan
10:05:14: Train Epoch: 17 [10000/45000 (22%)]	Loss: nan
10:05:19: Train Epoch: 17 [20000/45000 (44%)]	Loss: nan
10:05:19: Train Epoch: 17 [20000/45000 (44%)]	Loss: nan
10:05:19: Train Epoch: 17 [20000/45000 (44%)]	Loss: nan
10:05:23: Train Epoch: 17 [30000/45000 (67%)]	Loss: nan
10:05:23: Train Epoch: 17 [30000/45000 (67%)]	Loss: nan
10:05:23: Train Epoch: 17 [30000/45000 (67%)]	Loss: nan
10:05:28: Train Epoch: 17 [40000/45000 (89%)]	Loss: nan
10:05:28: Train Epoch: 17 [40000/45000 (89%)]	Loss: nan
10:05:28: Train Epoch: 17 [40000/45000 (89%)]	Loss: nan
10:05:31: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:05:31: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:05:31: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:05:32: Current learning rate: 0.1. Time taken for epoch: 22.72 seconds.

10:05:32: Current learning rate: 0.1. Time taken for epoch: 22.72 seconds.

10:05:32: Current learning rate: 0.1. Time taken for epoch: 22.72 seconds.

10:05:32: Train Epoch: 18 [0/45000 (0%)]	Loss: nan
10:05:32: Train Epoch: 18 [0/45000 (0%)]	Loss: nan
10:05:32: Train Epoch: 18 [0/45000 (0%)]	Loss: nan
10:05:37: Train Epoch: 18 [10000/45000 (22%)]	Loss: nan
10:05:37: Train Epoch: 18 [10000/45000 (22%)]	Loss: nan
10:05:37: Train Epoch: 18 [10000/45000 (22%)]	Loss: nan
10:05:41: Train Epoch: 18 [20000/45000 (44%)]	Loss: nan
10:05:41: Train Epoch: 18 [20000/45000 (44%)]	Loss: nan
10:05:41: Train Epoch: 18 [20000/45000 (44%)]	Loss: nan
10:05:46: Train Epoch: 18 [30000/45000 (67%)]	Loss: nan
10:05:46: Train Epoch: 18 [30000/45000 (67%)]	Loss: nan
10:05:46: Train Epoch: 18 [30000/45000 (67%)]	Loss: nan
10:05:50: Train Epoch: 18 [40000/45000 (89%)]	Loss: nan
10:05:50: Train Epoch: 18 [40000/45000 (89%)]	Loss: nan
10:05:50: Train Epoch: 18 [40000/45000 (89%)]	Loss: nan
10:05:54: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:05:54: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:05:54: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:05:54: Current learning rate: 0.1. Time taken for epoch: 22.48 seconds.

10:05:54: Current learning rate: 0.1. Time taken for epoch: 22.48 seconds.

10:05:54: Current learning rate: 0.1. Time taken for epoch: 22.48 seconds.

10:05:55: Train Epoch: 19 [0/45000 (0%)]	Loss: nan
10:05:55: Train Epoch: 19 [0/45000 (0%)]	Loss: nan
10:05:55: Train Epoch: 19 [0/45000 (0%)]	Loss: nan
10:06:00: Train Epoch: 19 [10000/45000 (22%)]	Loss: nan
10:06:00: Train Epoch: 19 [10000/45000 (22%)]	Loss: nan
10:06:00: Train Epoch: 19 [10000/45000 (22%)]	Loss: nan
10:06:04: Train Epoch: 19 [20000/45000 (44%)]	Loss: nan
10:06:04: Train Epoch: 19 [20000/45000 (44%)]	Loss: nan
10:06:04: Train Epoch: 19 [20000/45000 (44%)]	Loss: nan
10:06:09: Train Epoch: 19 [30000/45000 (67%)]	Loss: nan
10:06:09: Train Epoch: 19 [30000/45000 (67%)]	Loss: nan
10:06:09: Train Epoch: 19 [30000/45000 (67%)]	Loss: nan
10:06:14: Train Epoch: 19 [40000/45000 (89%)]	Loss: nan
10:06:14: Train Epoch: 19 [40000/45000 (89%)]	Loss: nan
10:06:14: Train Epoch: 19 [40000/45000 (89%)]	Loss: nan
10:06:18: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:06:18: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:06:18: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:06:18: Current learning rate: 0.1. Time taken for epoch: 23.59 seconds.

10:06:18: Current learning rate: 0.1. Time taken for epoch: 23.59 seconds.

10:06:18: Current learning rate: 0.1. Time taken for epoch: 23.59 seconds.

10:06:18: Train Epoch: 20 [0/45000 (0%)]	Loss: nan
10:06:18: Train Epoch: 20 [0/45000 (0%)]	Loss: nan
10:06:18: Train Epoch: 20 [0/45000 (0%)]	Loss: nan
10:06:23: Train Epoch: 20 [10000/45000 (22%)]	Loss: nan
10:06:23: Train Epoch: 20 [10000/45000 (22%)]	Loss: nan
10:06:23: Train Epoch: 20 [10000/45000 (22%)]	Loss: nan
10:06:27: Train Epoch: 20 [20000/45000 (44%)]	Loss: nan
10:06:27: Train Epoch: 20 [20000/45000 (44%)]	Loss: nan
10:06:27: Train Epoch: 20 [20000/45000 (44%)]	Loss: nan
10:06:32: Train Epoch: 20 [30000/45000 (67%)]	Loss: nan
10:06:32: Train Epoch: 20 [30000/45000 (67%)]	Loss: nan
10:06:32: Train Epoch: 20 [30000/45000 (67%)]	Loss: nan
10:06:36: Train Epoch: 20 [40000/45000 (89%)]	Loss: nan
10:06:36: Train Epoch: 20 [40000/45000 (89%)]	Loss: nan
10:06:36: Train Epoch: 20 [40000/45000 (89%)]	Loss: nan
10:06:40: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:06:40: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:06:40: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:06:40: Current learning rate: 0.1. Time taken for epoch: 22.22 seconds.

10:06:40: Current learning rate: 0.1. Time taken for epoch: 22.22 seconds.

10:06:40: Current learning rate: 0.1. Time taken for epoch: 22.22 seconds.

10:06:41: Train Epoch: 21 [0/45000 (0%)]	Loss: nan
10:06:41: Train Epoch: 21 [0/45000 (0%)]	Loss: nan
10:06:41: Train Epoch: 21 [0/45000 (0%)]	Loss: nan
10:06:45: Train Epoch: 21 [10000/45000 (22%)]	Loss: nan
10:06:45: Train Epoch: 21 [10000/45000 (22%)]	Loss: nan
10:06:45: Train Epoch: 21 [10000/45000 (22%)]	Loss: nan
10:06:50: Train Epoch: 21 [20000/45000 (44%)]	Loss: nan
10:06:50: Train Epoch: 21 [20000/45000 (44%)]	Loss: nan
10:06:50: Train Epoch: 21 [20000/45000 (44%)]	Loss: nan
10:06:54: Train Epoch: 21 [30000/45000 (67%)]	Loss: nan
10:06:54: Train Epoch: 21 [30000/45000 (67%)]	Loss: nan
10:06:54: Train Epoch: 21 [30000/45000 (67%)]	Loss: nan
10:06:59: Train Epoch: 21 [40000/45000 (89%)]	Loss: nan
10:06:59: Train Epoch: 21 [40000/45000 (89%)]	Loss: nan
10:06:59: Train Epoch: 21 [40000/45000 (89%)]	Loss: nan
10:07:03: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:07:03: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:07:03: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:07:03: Current learning rate: 0.1. Time taken for epoch: 23.01 seconds.

10:07:03: Current learning rate: 0.1. Time taken for epoch: 23.01 seconds.

10:07:03: Current learning rate: 0.1. Time taken for epoch: 23.01 seconds.

10:07:04: Train Epoch: 22 [0/45000 (0%)]	Loss: nan
10:07:04: Train Epoch: 22 [0/45000 (0%)]	Loss: nan
10:07:04: Train Epoch: 22 [0/45000 (0%)]	Loss: nan
10:07:08: Train Epoch: 22 [10000/45000 (22%)]	Loss: nan
10:07:08: Train Epoch: 22 [10000/45000 (22%)]	Loss: nan
10:07:08: Train Epoch: 22 [10000/45000 (22%)]	Loss: nan
10:07:13: Train Epoch: 22 [20000/45000 (44%)]	Loss: nan
10:07:13: Train Epoch: 22 [20000/45000 (44%)]	Loss: nan
10:07:13: Train Epoch: 22 [20000/45000 (44%)]	Loss: nan
10:07:17: Train Epoch: 22 [30000/45000 (67%)]	Loss: nan
10:07:17: Train Epoch: 22 [30000/45000 (67%)]	Loss: nan
10:07:17: Train Epoch: 22 [30000/45000 (67%)]	Loss: nan
10:07:22: Train Epoch: 22 [40000/45000 (89%)]	Loss: nan
10:07:22: Train Epoch: 22 [40000/45000 (89%)]	Loss: nan
10:07:22: Train Epoch: 22 [40000/45000 (89%)]	Loss: nan
10:07:26: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:07:26: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:07:26: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:07:26: Current learning rate: 0.1. Time taken for epoch: 22.75 seconds.

10:07:26: Current learning rate: 0.1. Time taken for epoch: 22.75 seconds.

10:07:26: Current learning rate: 0.1. Time taken for epoch: 22.75 seconds.

10:07:26: Train Epoch: 23 [0/45000 (0%)]	Loss: nan
10:07:26: Train Epoch: 23 [0/45000 (0%)]	Loss: nan
10:07:26: Train Epoch: 23 [0/45000 (0%)]	Loss: nan
10:07:31: Train Epoch: 23 [10000/45000 (22%)]	Loss: nan
10:07:31: Train Epoch: 23 [10000/45000 (22%)]	Loss: nan
10:07:31: Train Epoch: 23 [10000/45000 (22%)]	Loss: nan
10:07:35: Train Epoch: 23 [20000/45000 (44%)]	Loss: nan
10:07:35: Train Epoch: 23 [20000/45000 (44%)]	Loss: nan
10:07:35: Train Epoch: 23 [20000/45000 (44%)]	Loss: nan
10:07:40: Train Epoch: 23 [30000/45000 (67%)]	Loss: nan
10:07:40: Train Epoch: 23 [30000/45000 (67%)]	Loss: nan
10:07:40: Train Epoch: 23 [30000/45000 (67%)]	Loss: nan
10:07:45: Train Epoch: 23 [40000/45000 (89%)]	Loss: nan
10:07:45: Train Epoch: 23 [40000/45000 (89%)]	Loss: nan
10:07:45: Train Epoch: 23 [40000/45000 (89%)]	Loss: nan
10:07:48: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:07:48: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:07:48: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:07:48: Current learning rate: 0.1. Time taken for epoch: 22.76 seconds.

10:07:48: Current learning rate: 0.1. Time taken for epoch: 22.76 seconds.

10:07:48: Current learning rate: 0.1. Time taken for epoch: 22.76 seconds.

10:07:49: Train Epoch: 24 [0/45000 (0%)]	Loss: nan
10:07:49: Train Epoch: 24 [0/45000 (0%)]	Loss: nan
10:07:49: Train Epoch: 24 [0/45000 (0%)]	Loss: nan
10:07:54: Train Epoch: 24 [10000/45000 (22%)]	Loss: nan
10:07:54: Train Epoch: 24 [10000/45000 (22%)]	Loss: nan
10:07:54: Train Epoch: 24 [10000/45000 (22%)]	Loss: nan
10:07:58: Train Epoch: 24 [20000/45000 (44%)]	Loss: nan
10:07:58: Train Epoch: 24 [20000/45000 (44%)]	Loss: nan
10:07:58: Train Epoch: 24 [20000/45000 (44%)]	Loss: nan
10:08:03: Train Epoch: 24 [30000/45000 (67%)]	Loss: nan
10:08:03: Train Epoch: 24 [30000/45000 (67%)]	Loss: nan
10:08:03: Train Epoch: 24 [30000/45000 (67%)]	Loss: nan
10:08:07: Train Epoch: 24 [40000/45000 (89%)]	Loss: nan
10:08:07: Train Epoch: 24 [40000/45000 (89%)]	Loss: nan
10:08:07: Train Epoch: 24 [40000/45000 (89%)]	Loss: nan
10:08:11: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:08:11: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:08:11: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:08:11: Current learning rate: 0.1. Time taken for epoch: 22.94 seconds.

10:08:11: Current learning rate: 0.1. Time taken for epoch: 22.94 seconds.

10:08:11: Current learning rate: 0.1. Time taken for epoch: 22.94 seconds.

10:08:12: Train Epoch: 25 [0/45000 (0%)]	Loss: nan
10:08:12: Train Epoch: 25 [0/45000 (0%)]	Loss: nan
10:08:12: Train Epoch: 25 [0/45000 (0%)]	Loss: nan
10:08:17: Train Epoch: 25 [10000/45000 (22%)]	Loss: nan
10:08:17: Train Epoch: 25 [10000/45000 (22%)]	Loss: nan
10:08:17: Train Epoch: 25 [10000/45000 (22%)]	Loss: nan
10:08:21: Train Epoch: 25 [20000/45000 (44%)]	Loss: nan
10:08:21: Train Epoch: 25 [20000/45000 (44%)]	Loss: nan
10:08:21: Train Epoch: 25 [20000/45000 (44%)]	Loss: nan
10:08:26: Train Epoch: 25 [30000/45000 (67%)]	Loss: nan
10:08:26: Train Epoch: 25 [30000/45000 (67%)]	Loss: nan
10:08:26: Train Epoch: 25 [30000/45000 (67%)]	Loss: nan
10:08:30: Train Epoch: 25 [40000/45000 (89%)]	Loss: nan
10:08:30: Train Epoch: 25 [40000/45000 (89%)]	Loss: nan
10:08:30: Train Epoch: 25 [40000/45000 (89%)]	Loss: nan
10:08:34: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:08:34: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:08:34: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:08:34: Current learning rate: 0.1. Time taken for epoch: 22.34 seconds.

10:08:34: Current learning rate: 0.1. Time taken for epoch: 22.34 seconds.

10:08:34: Current learning rate: 0.1. Time taken for epoch: 22.34 seconds.

10:08:34: Train Epoch: 26 [0/45000 (0%)]	Loss: nan
10:08:34: Train Epoch: 26 [0/45000 (0%)]	Loss: nan
10:08:34: Train Epoch: 26 [0/45000 (0%)]	Loss: nan
10:08:39: Train Epoch: 26 [10000/45000 (22%)]	Loss: nan
10:08:39: Train Epoch: 26 [10000/45000 (22%)]	Loss: nan
10:08:39: Train Epoch: 26 [10000/45000 (22%)]	Loss: nan
10:08:43: Train Epoch: 26 [20000/45000 (44%)]	Loss: nan
10:08:43: Train Epoch: 26 [20000/45000 (44%)]	Loss: nan
10:08:43: Train Epoch: 26 [20000/45000 (44%)]	Loss: nan
10:08:48: Train Epoch: 26 [30000/45000 (67%)]	Loss: nan
10:08:48: Train Epoch: 26 [30000/45000 (67%)]	Loss: nan
10:08:48: Train Epoch: 26 [30000/45000 (67%)]	Loss: nan
10:08:52: Train Epoch: 26 [40000/45000 (89%)]	Loss: nan
10:08:52: Train Epoch: 26 [40000/45000 (89%)]	Loss: nan
10:08:52: Train Epoch: 26 [40000/45000 (89%)]	Loss: nan
10:08:56: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:08:56: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:08:56: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:08:56: Current learning rate: 0.1. Time taken for epoch: 22.65 seconds.

10:08:56: Current learning rate: 0.1. Time taken for epoch: 22.65 seconds.

10:08:56: Current learning rate: 0.1. Time taken for epoch: 22.65 seconds.

10:08:57: Train Epoch: 27 [0/45000 (0%)]	Loss: nan
10:08:57: Train Epoch: 27 [0/45000 (0%)]	Loss: nan
10:08:57: Train Epoch: 27 [0/45000 (0%)]	Loss: nan
10:09:02: Train Epoch: 27 [10000/45000 (22%)]	Loss: nan
10:09:02: Train Epoch: 27 [10000/45000 (22%)]	Loss: nan
10:09:02: Train Epoch: 27 [10000/45000 (22%)]	Loss: nan
10:09:06: Train Epoch: 27 [20000/45000 (44%)]	Loss: nan
10:09:06: Train Epoch: 27 [20000/45000 (44%)]	Loss: nan
10:09:06: Train Epoch: 27 [20000/45000 (44%)]	Loss: nan
10:09:10: Train Epoch: 27 [30000/45000 (67%)]	Loss: nan
10:09:10: Train Epoch: 27 [30000/45000 (67%)]	Loss: nan
10:09:10: Train Epoch: 27 [30000/45000 (67%)]	Loss: nan
10:09:15: Train Epoch: 27 [40000/45000 (89%)]	Loss: nan
10:09:15: Train Epoch: 27 [40000/45000 (89%)]	Loss: nan
10:09:15: Train Epoch: 27 [40000/45000 (89%)]	Loss: nan
10:09:19: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:09:19: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:09:19: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:09:19: Current learning rate: 0.1. Time taken for epoch: 22.76 seconds.

10:09:19: Current learning rate: 0.1. Time taken for epoch: 22.76 seconds.

10:09:19: Current learning rate: 0.1. Time taken for epoch: 22.76 seconds.

10:09:19: Train Epoch: 28 [0/45000 (0%)]	Loss: nan
10:09:19: Train Epoch: 28 [0/45000 (0%)]	Loss: nan
10:09:19: Train Epoch: 28 [0/45000 (0%)]	Loss: nan
10:09:24: Train Epoch: 28 [10000/45000 (22%)]	Loss: nan
10:09:24: Train Epoch: 28 [10000/45000 (22%)]	Loss: nan
10:09:24: Train Epoch: 28 [10000/45000 (22%)]	Loss: nan
10:09:29: Train Epoch: 28 [20000/45000 (44%)]	Loss: nan
10:09:29: Train Epoch: 28 [20000/45000 (44%)]	Loss: nan
10:09:29: Train Epoch: 28 [20000/45000 (44%)]	Loss: nan
10:09:33: Train Epoch: 28 [30000/45000 (67%)]	Loss: nan
10:09:33: Train Epoch: 28 [30000/45000 (67%)]	Loss: nan
10:09:33: Train Epoch: 28 [30000/45000 (67%)]	Loss: nan
10:09:37: Train Epoch: 28 [40000/45000 (89%)]	Loss: nan
10:09:37: Train Epoch: 28 [40000/45000 (89%)]	Loss: nan
10:09:37: Train Epoch: 28 [40000/45000 (89%)]	Loss: nan
10:09:41: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:09:41: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:09:41: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:09:41: Current learning rate: 0.1. Time taken for epoch: 22.01 seconds.

10:09:41: Current learning rate: 0.1. Time taken for epoch: 22.01 seconds.

10:09:41: Current learning rate: 0.1. Time taken for epoch: 22.01 seconds.

10:09:42: Train Epoch: 29 [0/45000 (0%)]	Loss: nan
10:09:42: Train Epoch: 29 [0/45000 (0%)]	Loss: nan
10:09:42: Train Epoch: 29 [0/45000 (0%)]	Loss: nan
10:09:46: Train Epoch: 29 [10000/45000 (22%)]	Loss: nan
10:09:46: Train Epoch: 29 [10000/45000 (22%)]	Loss: nan
10:09:46: Train Epoch: 29 [10000/45000 (22%)]	Loss: nan
10:09:51: Train Epoch: 29 [20000/45000 (44%)]	Loss: nan
10:09:51: Train Epoch: 29 [20000/45000 (44%)]	Loss: nan
10:09:51: Train Epoch: 29 [20000/45000 (44%)]	Loss: nan
10:09:55: Train Epoch: 29 [30000/45000 (67%)]	Loss: nan
10:09:55: Train Epoch: 29 [30000/45000 (67%)]	Loss: nan
10:09:55: Train Epoch: 29 [30000/45000 (67%)]	Loss: nan
10:10:00: Train Epoch: 29 [40000/45000 (89%)]	Loss: nan
10:10:00: Train Epoch: 29 [40000/45000 (89%)]	Loss: nan
10:10:00: Train Epoch: 29 [40000/45000 (89%)]	Loss: nan
10:10:04: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:10:04: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:10:04: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:10:04: Current learning rate: 0.1. Time taken for epoch: 22.85 seconds.

10:10:04: Current learning rate: 0.1. Time taken for epoch: 22.85 seconds.

10:10:04: Current learning rate: 0.1. Time taken for epoch: 22.85 seconds.

10:10:04: Train Epoch: 30 [0/45000 (0%)]	Loss: nan
10:10:04: Train Epoch: 30 [0/45000 (0%)]	Loss: nan
10:10:04: Train Epoch: 30 [0/45000 (0%)]	Loss: nan
10:10:09: Train Epoch: 30 [10000/45000 (22%)]	Loss: nan
10:10:09: Train Epoch: 30 [10000/45000 (22%)]	Loss: nan
10:10:09: Train Epoch: 30 [10000/45000 (22%)]	Loss: nan
10:10:14: Train Epoch: 30 [20000/45000 (44%)]	Loss: nan
10:10:14: Train Epoch: 30 [20000/45000 (44%)]	Loss: nan
10:10:14: Train Epoch: 30 [20000/45000 (44%)]	Loss: nan
10:10:18: Train Epoch: 30 [30000/45000 (67%)]	Loss: nan
10:10:18: Train Epoch: 30 [30000/45000 (67%)]	Loss: nan
10:10:18: Train Epoch: 30 [30000/45000 (67%)]	Loss: nan
10:10:23: Train Epoch: 30 [40000/45000 (89%)]	Loss: nan
10:10:23: Train Epoch: 30 [40000/45000 (89%)]	Loss: nan
10:10:23: Train Epoch: 30 [40000/45000 (89%)]	Loss: nan
10:10:26: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:10:26: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:10:26: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:10:26: Current learning rate: 0.1. Time taken for epoch: 22.60 seconds.

10:10:26: Current learning rate: 0.1. Time taken for epoch: 22.60 seconds.

10:10:26: Current learning rate: 0.1. Time taken for epoch: 22.60 seconds.

10:10:27: Train Epoch: 31 [0/45000 (0%)]	Loss: nan
10:10:27: Train Epoch: 31 [0/45000 (0%)]	Loss: nan
10:10:27: Train Epoch: 31 [0/45000 (0%)]	Loss: nan
10:10:32: Train Epoch: 31 [10000/45000 (22%)]	Loss: nan
10:10:32: Train Epoch: 31 [10000/45000 (22%)]	Loss: nan
10:10:32: Train Epoch: 31 [10000/45000 (22%)]	Loss: nan
10:10:36: Train Epoch: 31 [20000/45000 (44%)]	Loss: nan
10:10:36: Train Epoch: 31 [20000/45000 (44%)]	Loss: nan
10:10:36: Train Epoch: 31 [20000/45000 (44%)]	Loss: nan
10:10:40: Train Epoch: 31 [30000/45000 (67%)]	Loss: nan
10:10:40: Train Epoch: 31 [30000/45000 (67%)]	Loss: nan
10:10:40: Train Epoch: 31 [30000/45000 (67%)]	Loss: nan
10:10:45: Train Epoch: 31 [40000/45000 (89%)]	Loss: nan
10:10:45: Train Epoch: 31 [40000/45000 (89%)]	Loss: nan
10:10:45: Train Epoch: 31 [40000/45000 (89%)]	Loss: nan
10:10:49: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:10:49: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:10:49: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:10:49: Current learning rate: 0.1. Time taken for epoch: 22.11 seconds.

10:10:49: Current learning rate: 0.1. Time taken for epoch: 22.11 seconds.

10:10:49: Current learning rate: 0.1. Time taken for epoch: 22.11 seconds.

10:10:49: Train Epoch: 32 [0/45000 (0%)]	Loss: nan
10:10:49: Train Epoch: 32 [0/45000 (0%)]	Loss: nan
10:10:49: Train Epoch: 32 [0/45000 (0%)]	Loss: nan
10:10:54: Train Epoch: 32 [10000/45000 (22%)]	Loss: nan
10:10:54: Train Epoch: 32 [10000/45000 (22%)]	Loss: nan
10:10:54: Train Epoch: 32 [10000/45000 (22%)]	Loss: nan
10:10:59: Train Epoch: 32 [20000/45000 (44%)]	Loss: nan
10:10:59: Train Epoch: 32 [20000/45000 (44%)]	Loss: nan
10:10:59: Train Epoch: 32 [20000/45000 (44%)]	Loss: nan
10:11:03: Train Epoch: 32 [30000/45000 (67%)]	Loss: nan
10:11:03: Train Epoch: 32 [30000/45000 (67%)]	Loss: nan
10:11:03: Train Epoch: 32 [30000/45000 (67%)]	Loss: nan
10:11:08: Train Epoch: 32 [40000/45000 (89%)]	Loss: nan
10:11:08: Train Epoch: 32 [40000/45000 (89%)]	Loss: nan
10:11:08: Train Epoch: 32 [40000/45000 (89%)]	Loss: nan
10:11:11: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:11:11: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:11:11: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:11:11: Current learning rate: 0.1. Time taken for epoch: 22.83 seconds.

10:11:11: Current learning rate: 0.1. Time taken for epoch: 22.83 seconds.

10:11:11: Current learning rate: 0.1. Time taken for epoch: 22.83 seconds.

10:11:12: Train Epoch: 33 [0/45000 (0%)]	Loss: nan
10:11:12: Train Epoch: 33 [0/45000 (0%)]	Loss: nan
10:11:12: Train Epoch: 33 [0/45000 (0%)]	Loss: nan
10:11:17: Train Epoch: 33 [10000/45000 (22%)]	Loss: nan
10:11:17: Train Epoch: 33 [10000/45000 (22%)]	Loss: nan
10:11:17: Train Epoch: 33 [10000/45000 (22%)]	Loss: nan
10:11:21: Train Epoch: 33 [20000/45000 (44%)]	Loss: nan
10:11:21: Train Epoch: 33 [20000/45000 (44%)]	Loss: nan
10:11:21: Train Epoch: 33 [20000/45000 (44%)]	Loss: nan
10:11:26: Train Epoch: 33 [30000/45000 (67%)]	Loss: nan
10:11:26: Train Epoch: 33 [30000/45000 (67%)]	Loss: nan
10:11:26: Train Epoch: 33 [30000/45000 (67%)]	Loss: nan
10:11:30: Train Epoch: 33 [40000/45000 (89%)]	Loss: nan
10:11:30: Train Epoch: 33 [40000/45000 (89%)]	Loss: nan
10:11:30: Train Epoch: 33 [40000/45000 (89%)]	Loss: nan
10:11:34: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:11:34: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:11:34: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:11:34: Current learning rate: 0.1. Time taken for epoch: 22.54 seconds.

10:11:34: Current learning rate: 0.1. Time taken for epoch: 22.54 seconds.

10:11:34: Current learning rate: 0.1. Time taken for epoch: 22.54 seconds.

10:11:35: Train Epoch: 34 [0/45000 (0%)]	Loss: nan
10:11:35: Train Epoch: 34 [0/45000 (0%)]	Loss: nan
10:11:35: Train Epoch: 34 [0/45000 (0%)]	Loss: nan
10:11:39: Train Epoch: 34 [10000/45000 (22%)]	Loss: nan
10:11:39: Train Epoch: 34 [10000/45000 (22%)]	Loss: nan
10:11:39: Train Epoch: 34 [10000/45000 (22%)]	Loss: nan
10:11:44: Train Epoch: 34 [20000/45000 (44%)]	Loss: nan
10:11:44: Train Epoch: 34 [20000/45000 (44%)]	Loss: nan
10:11:44: Train Epoch: 34 [20000/45000 (44%)]	Loss: nan
10:11:48: Train Epoch: 34 [30000/45000 (67%)]	Loss: nan
10:11:48: Train Epoch: 34 [30000/45000 (67%)]	Loss: nan
10:11:48: Train Epoch: 34 [30000/45000 (67%)]	Loss: nan
10:11:52: Train Epoch: 34 [40000/45000 (89%)]	Loss: nan
10:11:52: Train Epoch: 34 [40000/45000 (89%)]	Loss: nan
10:11:52: Train Epoch: 34 [40000/45000 (89%)]	Loss: nan
10:11:56: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:11:56: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:11:56: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:11:56: Current learning rate: 0.1. Time taken for epoch: 22.49 seconds.

10:11:56: Current learning rate: 0.1. Time taken for epoch: 22.49 seconds.

10:11:56: Current learning rate: 0.1. Time taken for epoch: 22.49 seconds.

10:11:57: Train Epoch: 35 [0/45000 (0%)]	Loss: nan
10:11:57: Train Epoch: 35 [0/45000 (0%)]	Loss: nan
10:11:57: Train Epoch: 35 [0/45000 (0%)]	Loss: nan
10:12:02: Train Epoch: 35 [10000/45000 (22%)]	Loss: nan
10:12:02: Train Epoch: 35 [10000/45000 (22%)]	Loss: nan
10:12:02: Train Epoch: 35 [10000/45000 (22%)]	Loss: nan
10:12:06: Train Epoch: 35 [20000/45000 (44%)]	Loss: nan
10:12:06: Train Epoch: 35 [20000/45000 (44%)]	Loss: nan
10:12:06: Train Epoch: 35 [20000/45000 (44%)]	Loss: nan
10:12:11: Train Epoch: 35 [30000/45000 (67%)]	Loss: nan
10:12:11: Train Epoch: 35 [30000/45000 (67%)]	Loss: nan
10:12:11: Train Epoch: 35 [30000/45000 (67%)]	Loss: nan
10:12:15: Train Epoch: 35 [40000/45000 (89%)]	Loss: nan
10:12:15: Train Epoch: 35 [40000/45000 (89%)]	Loss: nan
10:12:15: Train Epoch: 35 [40000/45000 (89%)]	Loss: nan
10:12:19: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:12:19: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:12:19: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:12:19: Current learning rate: 0.1. Time taken for epoch: 22.74 seconds.

10:12:19: Current learning rate: 0.1. Time taken for epoch: 22.74 seconds.

10:12:19: Current learning rate: 0.1. Time taken for epoch: 22.74 seconds.

10:12:20: Train Epoch: 36 [0/45000 (0%)]	Loss: nan
10:12:20: Train Epoch: 36 [0/45000 (0%)]	Loss: nan
10:12:20: Train Epoch: 36 [0/45000 (0%)]	Loss: nan
10:12:24: Train Epoch: 36 [10000/45000 (22%)]	Loss: nan
10:12:24: Train Epoch: 36 [10000/45000 (22%)]	Loss: nan
10:12:24: Train Epoch: 36 [10000/45000 (22%)]	Loss: nan
10:12:29: Train Epoch: 36 [20000/45000 (44%)]	Loss: nan
10:12:29: Train Epoch: 36 [20000/45000 (44%)]	Loss: nan
10:12:29: Train Epoch: 36 [20000/45000 (44%)]	Loss: nan
10:12:33: Train Epoch: 36 [30000/45000 (67%)]	Loss: nan
10:12:33: Train Epoch: 36 [30000/45000 (67%)]	Loss: nan
10:12:33: Train Epoch: 36 [30000/45000 (67%)]	Loss: nan
10:12:37: Train Epoch: 36 [40000/45000 (89%)]	Loss: nan
10:12:37: Train Epoch: 36 [40000/45000 (89%)]	Loss: nan
10:12:37: Train Epoch: 36 [40000/45000 (89%)]	Loss: nan
10:12:41: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:12:41: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:12:41: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:12:41: Current learning rate: 0.1. Time taken for epoch: 22.09 seconds.

10:12:41: Current learning rate: 0.1. Time taken for epoch: 22.09 seconds.

10:12:41: Current learning rate: 0.1. Time taken for epoch: 22.09 seconds.

10:12:42: Train Epoch: 37 [0/45000 (0%)]	Loss: nan
10:12:42: Train Epoch: 37 [0/45000 (0%)]	Loss: nan
10:12:42: Train Epoch: 37 [0/45000 (0%)]	Loss: nan
10:12:47: Train Epoch: 37 [10000/45000 (22%)]	Loss: nan
10:12:47: Train Epoch: 37 [10000/45000 (22%)]	Loss: nan
10:12:47: Train Epoch: 37 [10000/45000 (22%)]	Loss: nan
10:12:51: Train Epoch: 37 [20000/45000 (44%)]	Loss: nan
10:12:51: Train Epoch: 37 [20000/45000 (44%)]	Loss: nan
10:12:51: Train Epoch: 37 [20000/45000 (44%)]	Loss: nan
10:12:56: Train Epoch: 37 [30000/45000 (67%)]	Loss: nan
10:12:56: Train Epoch: 37 [30000/45000 (67%)]	Loss: nan
10:12:56: Train Epoch: 37 [30000/45000 (67%)]	Loss: nan
10:13:01: Train Epoch: 37 [40000/45000 (89%)]	Loss: nan
10:13:01: Train Epoch: 37 [40000/45000 (89%)]	Loss: nan
10:13:01: Train Epoch: 37 [40000/45000 (89%)]	Loss: nan
10:13:05: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:13:05: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:13:05: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:13:05: Current learning rate: 0.1. Time taken for epoch: 23.43 seconds.

10:13:05: Current learning rate: 0.1. Time taken for epoch: 23.43 seconds.

10:13:05: Current learning rate: 0.1. Time taken for epoch: 23.43 seconds.

10:13:05: Train Epoch: 38 [0/45000 (0%)]	Loss: nan
10:13:05: Train Epoch: 38 [0/45000 (0%)]	Loss: nan
10:13:05: Train Epoch: 38 [0/45000 (0%)]	Loss: nan
10:13:10: Train Epoch: 38 [10000/45000 (22%)]	Loss: nan
10:13:10: Train Epoch: 38 [10000/45000 (22%)]	Loss: nan
10:13:10: Train Epoch: 38 [10000/45000 (22%)]	Loss: nan
10:13:15: Train Epoch: 38 [20000/45000 (44%)]	Loss: nan
10:13:15: Train Epoch: 38 [20000/45000 (44%)]	Loss: nan
10:13:15: Train Epoch: 38 [20000/45000 (44%)]	Loss: nan
10:13:19: Train Epoch: 38 [30000/45000 (67%)]	Loss: nan
10:13:19: Train Epoch: 38 [30000/45000 (67%)]	Loss: nan
10:13:19: Train Epoch: 38 [30000/45000 (67%)]	Loss: nan
10:13:23: Train Epoch: 38 [40000/45000 (89%)]	Loss: nan
10:13:23: Train Epoch: 38 [40000/45000 (89%)]	Loss: nan
10:13:23: Train Epoch: 38 [40000/45000 (89%)]	Loss: nan
10:13:27: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:13:27: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:13:27: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:13:27: Current learning rate: 0.1. Time taken for epoch: 22.44 seconds.

10:13:27: Current learning rate: 0.1. Time taken for epoch: 22.44 seconds.

10:13:27: Current learning rate: 0.1. Time taken for epoch: 22.44 seconds.

10:13:28: Train Epoch: 39 [0/45000 (0%)]	Loss: nan
10:13:28: Train Epoch: 39 [0/45000 (0%)]	Loss: nan
10:13:28: Train Epoch: 39 [0/45000 (0%)]	Loss: nan
10:13:32: Train Epoch: 39 [10000/45000 (22%)]	Loss: nan
10:13:32: Train Epoch: 39 [10000/45000 (22%)]	Loss: nan
10:13:32: Train Epoch: 39 [10000/45000 (22%)]	Loss: nan
10:13:37: Train Epoch: 39 [20000/45000 (44%)]	Loss: nan
10:13:37: Train Epoch: 39 [20000/45000 (44%)]	Loss: nan
10:13:37: Train Epoch: 39 [20000/45000 (44%)]	Loss: nan
10:13:41: Train Epoch: 39 [30000/45000 (67%)]	Loss: nan
10:13:41: Train Epoch: 39 [30000/45000 (67%)]	Loss: nan
10:13:41: Train Epoch: 39 [30000/45000 (67%)]	Loss: nan
10:13:45: Train Epoch: 39 [40000/45000 (89%)]	Loss: nan
10:13:45: Train Epoch: 39 [40000/45000 (89%)]	Loss: nan
10:13:45: Train Epoch: 39 [40000/45000 (89%)]	Loss: nan
10:13:49: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:13:49: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:13:49: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:13:49: Current learning rate: 0.1. Time taken for epoch: 22.11 seconds.

10:13:49: Current learning rate: 0.1. Time taken for epoch: 22.11 seconds.

10:13:49: Current learning rate: 0.1. Time taken for epoch: 22.11 seconds.

10:13:50: Train Epoch: 40 [0/45000 (0%)]	Loss: nan
10:13:50: Train Epoch: 40 [0/45000 (0%)]	Loss: nan
10:13:50: Train Epoch: 40 [0/45000 (0%)]	Loss: nan
10:13:55: Train Epoch: 40 [10000/45000 (22%)]	Loss: nan
10:13:55: Train Epoch: 40 [10000/45000 (22%)]	Loss: nan
10:13:55: Train Epoch: 40 [10000/45000 (22%)]	Loss: nan
10:13:59: Train Epoch: 40 [20000/45000 (44%)]	Loss: nan
10:13:59: Train Epoch: 40 [20000/45000 (44%)]	Loss: nan
10:13:59: Train Epoch: 40 [20000/45000 (44%)]	Loss: nan
10:14:04: Train Epoch: 40 [30000/45000 (67%)]	Loss: nan
10:14:04: Train Epoch: 40 [30000/45000 (67%)]	Loss: nan
10:14:04: Train Epoch: 40 [30000/45000 (67%)]	Loss: nan
10:14:08: Train Epoch: 40 [40000/45000 (89%)]	Loss: nan
10:14:08: Train Epoch: 40 [40000/45000 (89%)]	Loss: nan
10:14:08: Train Epoch: 40 [40000/45000 (89%)]	Loss: nan
10:14:13: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:14:13: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:14:13: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:14:13: Current learning rate: 0.1. Time taken for epoch: 23.34 seconds.

10:14:13: Current learning rate: 0.1. Time taken for epoch: 23.34 seconds.

10:14:13: Current learning rate: 0.1. Time taken for epoch: 23.34 seconds.

10:14:13: Train Epoch: 41 [0/45000 (0%)]	Loss: nan
10:14:13: Train Epoch: 41 [0/45000 (0%)]	Loss: nan
10:14:13: Train Epoch: 41 [0/45000 (0%)]	Loss: nan
10:14:18: Train Epoch: 41 [10000/45000 (22%)]	Loss: nan
10:14:18: Train Epoch: 41 [10000/45000 (22%)]	Loss: nan
10:14:18: Train Epoch: 41 [10000/45000 (22%)]	Loss: nan
10:14:22: Train Epoch: 41 [20000/45000 (44%)]	Loss: nan
10:14:22: Train Epoch: 41 [20000/45000 (44%)]	Loss: nan
10:14:22: Train Epoch: 41 [20000/45000 (44%)]	Loss: nan
10:14:27: Train Epoch: 41 [30000/45000 (67%)]	Loss: nan
10:14:27: Train Epoch: 41 [30000/45000 (67%)]	Loss: nan
10:14:27: Train Epoch: 41 [30000/45000 (67%)]	Loss: nan
10:14:31: Train Epoch: 41 [40000/45000 (89%)]	Loss: nan
10:14:31: Train Epoch: 41 [40000/45000 (89%)]	Loss: nan
10:14:31: Train Epoch: 41 [40000/45000 (89%)]	Loss: nan
10:14:35: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:14:35: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:14:35: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:14:35: Current learning rate: 0.1. Time taken for epoch: 22.15 seconds.

10:14:35: Current learning rate: 0.1. Time taken for epoch: 22.15 seconds.

10:14:35: Current learning rate: 0.1. Time taken for epoch: 22.15 seconds.

10:14:35: Train Epoch: 42 [0/45000 (0%)]	Loss: nan
10:14:35: Train Epoch: 42 [0/45000 (0%)]	Loss: nan
10:14:35: Train Epoch: 42 [0/45000 (0%)]	Loss: nan
10:14:40: Train Epoch: 42 [10000/45000 (22%)]	Loss: nan
10:14:40: Train Epoch: 42 [10000/45000 (22%)]	Loss: nan
10:14:40: Train Epoch: 42 [10000/45000 (22%)]	Loss: nan
10:14:44: Train Epoch: 42 [20000/45000 (44%)]	Loss: nan
10:14:44: Train Epoch: 42 [20000/45000 (44%)]	Loss: nan
10:14:44: Train Epoch: 42 [20000/45000 (44%)]	Loss: nan
10:14:49: Train Epoch: 42 [30000/45000 (67%)]	Loss: nan
10:14:49: Train Epoch: 42 [30000/45000 (67%)]	Loss: nan
10:14:49: Train Epoch: 42 [30000/45000 (67%)]	Loss: nan
10:14:53: Train Epoch: 42 [40000/45000 (89%)]	Loss: nan
10:14:53: Train Epoch: 42 [40000/45000 (89%)]	Loss: nan
10:14:53: Train Epoch: 42 [40000/45000 (89%)]	Loss: nan
10:14:57: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:14:57: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:14:57: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:14:57: Current learning rate: 0.1. Time taken for epoch: 22.54 seconds.

10:14:57: Current learning rate: 0.1. Time taken for epoch: 22.54 seconds.

10:14:57: Current learning rate: 0.1. Time taken for epoch: 22.54 seconds.

10:14:58: Train Epoch: 43 [0/45000 (0%)]	Loss: nan
10:14:58: Train Epoch: 43 [0/45000 (0%)]	Loss: nan
10:14:58: Train Epoch: 43 [0/45000 (0%)]	Loss: nan
10:15:03: Train Epoch: 43 [10000/45000 (22%)]	Loss: nan
10:15:03: Train Epoch: 43 [10000/45000 (22%)]	Loss: nan
10:15:03: Train Epoch: 43 [10000/45000 (22%)]	Loss: nan
10:15:07: Train Epoch: 43 [20000/45000 (44%)]	Loss: nan
10:15:07: Train Epoch: 43 [20000/45000 (44%)]	Loss: nan
10:15:07: Train Epoch: 43 [20000/45000 (44%)]	Loss: nan
10:15:12: Train Epoch: 43 [30000/45000 (67%)]	Loss: nan
10:15:12: Train Epoch: 43 [30000/45000 (67%)]	Loss: nan
10:15:12: Train Epoch: 43 [30000/45000 (67%)]	Loss: nan
10:15:16: Train Epoch: 43 [40000/45000 (89%)]	Loss: nan
10:15:16: Train Epoch: 43 [40000/45000 (89%)]	Loss: nan
10:15:16: Train Epoch: 43 [40000/45000 (89%)]	Loss: nan
10:15:20: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:15:20: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:15:20: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:15:20: Current learning rate: 0.1. Time taken for epoch: 23.01 seconds.

10:15:20: Current learning rate: 0.1. Time taken for epoch: 23.01 seconds.

10:15:20: Current learning rate: 0.1. Time taken for epoch: 23.01 seconds.

10:15:21: Train Epoch: 44 [0/45000 (0%)]	Loss: nan
10:15:21: Train Epoch: 44 [0/45000 (0%)]	Loss: nan
10:15:21: Train Epoch: 44 [0/45000 (0%)]	Loss: nan
10:15:26: Train Epoch: 44 [10000/45000 (22%)]	Loss: nan
10:15:26: Train Epoch: 44 [10000/45000 (22%)]	Loss: nan
10:15:26: Train Epoch: 44 [10000/45000 (22%)]	Loss: nan
10:15:30: Train Epoch: 44 [20000/45000 (44%)]	Loss: nan
10:15:30: Train Epoch: 44 [20000/45000 (44%)]	Loss: nan
10:15:30: Train Epoch: 44 [20000/45000 (44%)]	Loss: nan
10:15:35: Train Epoch: 44 [30000/45000 (67%)]	Loss: nan
10:15:35: Train Epoch: 44 [30000/45000 (67%)]	Loss: nan
10:15:35: Train Epoch: 44 [30000/45000 (67%)]	Loss: nan
10:15:39: Train Epoch: 44 [40000/45000 (89%)]	Loss: nan
10:15:39: Train Epoch: 44 [40000/45000 (89%)]	Loss: nan
10:15:39: Train Epoch: 44 [40000/45000 (89%)]	Loss: nan
10:15:43: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:15:43: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:15:43: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:15:43: Current learning rate: 0.1. Time taken for epoch: 22.83 seconds.

10:15:43: Current learning rate: 0.1. Time taken for epoch: 22.83 seconds.

10:15:43: Current learning rate: 0.1. Time taken for epoch: 22.83 seconds.

10:15:44: Train Epoch: 45 [0/45000 (0%)]	Loss: nan
10:15:44: Train Epoch: 45 [0/45000 (0%)]	Loss: nan
10:15:44: Train Epoch: 45 [0/45000 (0%)]	Loss: nan
10:15:48: Train Epoch: 45 [10000/45000 (22%)]	Loss: nan
10:15:48: Train Epoch: 45 [10000/45000 (22%)]	Loss: nan
10:15:48: Train Epoch: 45 [10000/45000 (22%)]	Loss: nan
10:15:53: Train Epoch: 45 [20000/45000 (44%)]	Loss: nan
10:15:53: Train Epoch: 45 [20000/45000 (44%)]	Loss: nan
10:15:53: Train Epoch: 45 [20000/45000 (44%)]	Loss: nan
10:15:58: Train Epoch: 45 [30000/45000 (67%)]	Loss: nan
10:15:58: Train Epoch: 45 [30000/45000 (67%)]	Loss: nan
10:15:58: Train Epoch: 45 [30000/45000 (67%)]	Loss: nan
10:16:03: Train Epoch: 45 [40000/45000 (89%)]	Loss: nan
10:16:03: Train Epoch: 45 [40000/45000 (89%)]	Loss: nan
10:16:03: Train Epoch: 45 [40000/45000 (89%)]	Loss: nan
10:16:07: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:16:07: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:16:07: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:16:07: Current learning rate: 0.1. Time taken for epoch: 23.66 seconds.

10:16:07: Current learning rate: 0.1. Time taken for epoch: 23.66 seconds.

10:16:07: Current learning rate: 0.1. Time taken for epoch: 23.66 seconds.

10:16:07: Train Epoch: 46 [0/45000 (0%)]	Loss: nan
10:16:07: Train Epoch: 46 [0/45000 (0%)]	Loss: nan
10:16:07: Train Epoch: 46 [0/45000 (0%)]	Loss: nan
10:16:12: Train Epoch: 46 [10000/45000 (22%)]	Loss: nan
10:16:12: Train Epoch: 46 [10000/45000 (22%)]	Loss: nan
10:16:12: Train Epoch: 46 [10000/45000 (22%)]	Loss: nan
10:16:17: Train Epoch: 46 [20000/45000 (44%)]	Loss: nan
10:16:17: Train Epoch: 46 [20000/45000 (44%)]	Loss: nan
10:16:17: Train Epoch: 46 [20000/45000 (44%)]	Loss: nan
10:16:21: Train Epoch: 46 [30000/45000 (67%)]	Loss: nan
10:16:21: Train Epoch: 46 [30000/45000 (67%)]	Loss: nan
10:16:21: Train Epoch: 46 [30000/45000 (67%)]	Loss: nan
10:16:26: Train Epoch: 46 [40000/45000 (89%)]	Loss: nan
10:16:26: Train Epoch: 46 [40000/45000 (89%)]	Loss: nan
10:16:26: Train Epoch: 46 [40000/45000 (89%)]	Loss: nan
10:16:29: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:16:29: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:16:29: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:16:30: Current learning rate: 0.1. Time taken for epoch: 22.76 seconds.

10:16:30: Current learning rate: 0.1. Time taken for epoch: 22.76 seconds.

10:16:30: Current learning rate: 0.1. Time taken for epoch: 22.76 seconds.

10:16:30: Train Epoch: 47 [0/45000 (0%)]	Loss: nan
10:16:30: Train Epoch: 47 [0/45000 (0%)]	Loss: nan
10:16:30: Train Epoch: 47 [0/45000 (0%)]	Loss: nan
10:16:35: Train Epoch: 47 [10000/45000 (22%)]	Loss: nan
10:16:35: Train Epoch: 47 [10000/45000 (22%)]	Loss: nan
10:16:35: Train Epoch: 47 [10000/45000 (22%)]	Loss: nan
10:16:39: Train Epoch: 47 [20000/45000 (44%)]	Loss: nan
10:16:39: Train Epoch: 47 [20000/45000 (44%)]	Loss: nan
10:16:39: Train Epoch: 47 [20000/45000 (44%)]	Loss: nan
10:16:44: Train Epoch: 47 [30000/45000 (67%)]	Loss: nan
10:16:44: Train Epoch: 47 [30000/45000 (67%)]	Loss: nan
10:16:44: Train Epoch: 47 [30000/45000 (67%)]	Loss: nan
10:16:48: Train Epoch: 47 [40000/45000 (89%)]	Loss: nan
10:16:48: Train Epoch: 47 [40000/45000 (89%)]	Loss: nan
10:16:48: Train Epoch: 47 [40000/45000 (89%)]	Loss: nan
10:16:52: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:16:52: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:16:52: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:16:52: Current learning rate: 0.1. Time taken for epoch: 22.73 seconds.

10:16:52: Current learning rate: 0.1. Time taken for epoch: 22.73 seconds.

10:16:52: Current learning rate: 0.1. Time taken for epoch: 22.73 seconds.

10:16:53: Train Epoch: 48 [0/45000 (0%)]	Loss: nan
10:16:53: Train Epoch: 48 [0/45000 (0%)]	Loss: nan
10:16:53: Train Epoch: 48 [0/45000 (0%)]	Loss: nan
10:16:58: Train Epoch: 48 [10000/45000 (22%)]	Loss: nan
10:16:58: Train Epoch: 48 [10000/45000 (22%)]	Loss: nan
10:16:58: Train Epoch: 48 [10000/45000 (22%)]	Loss: nan
10:17:03: Train Epoch: 48 [20000/45000 (44%)]	Loss: nan
10:17:03: Train Epoch: 48 [20000/45000 (44%)]	Loss: nan
10:17:03: Train Epoch: 48 [20000/45000 (44%)]	Loss: nan
10:17:07: Train Epoch: 48 [30000/45000 (67%)]	Loss: nan
10:17:07: Train Epoch: 48 [30000/45000 (67%)]	Loss: nan
10:17:07: Train Epoch: 48 [30000/45000 (67%)]	Loss: nan
10:17:12: Train Epoch: 48 [40000/45000 (89%)]	Loss: nan
10:17:12: Train Epoch: 48 [40000/45000 (89%)]	Loss: nan
10:17:12: Train Epoch: 48 [40000/45000 (89%)]	Loss: nan
10:17:16: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:17:16: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:17:16: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:17:16: Current learning rate: 0.1. Time taken for epoch: 23.72 seconds.

10:17:16: Current learning rate: 0.1. Time taken for epoch: 23.72 seconds.

10:17:16: Current learning rate: 0.1. Time taken for epoch: 23.72 seconds.

10:17:17: Train Epoch: 49 [0/45000 (0%)]	Loss: nan
10:17:17: Train Epoch: 49 [0/45000 (0%)]	Loss: nan
10:17:17: Train Epoch: 49 [0/45000 (0%)]	Loss: nan
10:17:21: Train Epoch: 49 [10000/45000 (22%)]	Loss: nan
10:17:21: Train Epoch: 49 [10000/45000 (22%)]	Loss: nan
10:17:21: Train Epoch: 49 [10000/45000 (22%)]	Loss: nan
10:17:26: Train Epoch: 49 [20000/45000 (44%)]	Loss: nan
10:17:26: Train Epoch: 49 [20000/45000 (44%)]	Loss: nan
10:17:26: Train Epoch: 49 [20000/45000 (44%)]	Loss: nan
10:17:30: Train Epoch: 49 [30000/45000 (67%)]	Loss: nan
10:17:30: Train Epoch: 49 [30000/45000 (67%)]	Loss: nan
10:17:30: Train Epoch: 49 [30000/45000 (67%)]	Loss: nan
10:17:35: Train Epoch: 49 [40000/45000 (89%)]	Loss: nan
10:17:35: Train Epoch: 49 [40000/45000 (89%)]	Loss: nan
10:17:35: Train Epoch: 49 [40000/45000 (89%)]	Loss: nan
10:17:38: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:17:38: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:17:38: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:17:38: Current learning rate: 0.1. Time taken for epoch: 22.32 seconds.

10:17:38: Current learning rate: 0.1. Time taken for epoch: 22.32 seconds.

10:17:38: Current learning rate: 0.1. Time taken for epoch: 22.32 seconds.

10:17:39: Train Epoch: 50 [0/45000 (0%)]	Loss: nan
10:17:39: Train Epoch: 50 [0/45000 (0%)]	Loss: nan
10:17:39: Train Epoch: 50 [0/45000 (0%)]	Loss: nan
10:17:44: Train Epoch: 50 [10000/45000 (22%)]	Loss: nan
10:17:44: Train Epoch: 50 [10000/45000 (22%)]	Loss: nan
10:17:44: Train Epoch: 50 [10000/45000 (22%)]	Loss: nan
10:17:48: Train Epoch: 50 [20000/45000 (44%)]	Loss: nan
10:17:48: Train Epoch: 50 [20000/45000 (44%)]	Loss: nan
10:17:48: Train Epoch: 50 [20000/45000 (44%)]	Loss: nan
10:17:53: Train Epoch: 50 [30000/45000 (67%)]	Loss: nan
10:17:53: Train Epoch: 50 [30000/45000 (67%)]	Loss: nan
10:17:53: Train Epoch: 50 [30000/45000 (67%)]	Loss: nan
10:17:58: Train Epoch: 50 [40000/45000 (89%)]	Loss: nan
10:17:58: Train Epoch: 50 [40000/45000 (89%)]	Loss: nan
10:17:58: Train Epoch: 50 [40000/45000 (89%)]	Loss: nan
10:18:02: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:18:02: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:18:02: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:18:02: Current learning rate: 0.1. Time taken for epoch: 23.78 seconds.

10:18:02: Current learning rate: 0.1. Time taken for epoch: 23.78 seconds.

10:18:02: Current learning rate: 0.1. Time taken for epoch: 23.78 seconds.

10:18:03: Train Epoch: 51 [0/45000 (0%)]	Loss: nan
10:18:03: Train Epoch: 51 [0/45000 (0%)]	Loss: nan
10:18:03: Train Epoch: 51 [0/45000 (0%)]	Loss: nan
10:18:07: Train Epoch: 51 [10000/45000 (22%)]	Loss: nan
10:18:07: Train Epoch: 51 [10000/45000 (22%)]	Loss: nan
10:18:07: Train Epoch: 51 [10000/45000 (22%)]	Loss: nan
10:18:12: Train Epoch: 51 [20000/45000 (44%)]	Loss: nan
10:18:12: Train Epoch: 51 [20000/45000 (44%)]	Loss: nan
10:18:12: Train Epoch: 51 [20000/45000 (44%)]	Loss: nan
10:18:17: Train Epoch: 51 [30000/45000 (67%)]	Loss: nan
10:18:17: Train Epoch: 51 [30000/45000 (67%)]	Loss: nan
10:18:17: Train Epoch: 51 [30000/45000 (67%)]	Loss: nan
10:18:21: Train Epoch: 51 [40000/45000 (89%)]	Loss: nan
10:18:21: Train Epoch: 51 [40000/45000 (89%)]	Loss: nan
10:18:21: Train Epoch: 51 [40000/45000 (89%)]	Loss: nan
10:18:25: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:18:25: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:18:25: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:18:25: Current learning rate: 0.1. Time taken for epoch: 22.77 seconds.

10:18:25: Current learning rate: 0.1. Time taken for epoch: 22.77 seconds.

10:18:25: Current learning rate: 0.1. Time taken for epoch: 22.77 seconds.

10:18:25: Train Epoch: 52 [0/45000 (0%)]	Loss: nan
10:18:25: Train Epoch: 52 [0/45000 (0%)]	Loss: nan
10:18:25: Train Epoch: 52 [0/45000 (0%)]	Loss: nan
10:18:30: Train Epoch: 52 [10000/45000 (22%)]	Loss: nan
10:18:30: Train Epoch: 52 [10000/45000 (22%)]	Loss: nan
10:18:30: Train Epoch: 52 [10000/45000 (22%)]	Loss: nan
10:18:34: Train Epoch: 52 [20000/45000 (44%)]	Loss: nan
10:18:34: Train Epoch: 52 [20000/45000 (44%)]	Loss: nan
10:18:34: Train Epoch: 52 [20000/45000 (44%)]	Loss: nan
10:18:39: Train Epoch: 52 [30000/45000 (67%)]	Loss: nan
10:18:39: Train Epoch: 52 [30000/45000 (67%)]	Loss: nan
10:18:39: Train Epoch: 52 [30000/45000 (67%)]	Loss: nan
10:18:43: Train Epoch: 52 [40000/45000 (89%)]	Loss: nan
10:18:43: Train Epoch: 52 [40000/45000 (89%)]	Loss: nan
10:18:43: Train Epoch: 52 [40000/45000 (89%)]	Loss: nan
10:18:47: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:18:47: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:18:47: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:18:47: Current learning rate: 0.1. Time taken for epoch: 22.36 seconds.

10:18:47: Current learning rate: 0.1. Time taken for epoch: 22.36 seconds.

10:18:47: Current learning rate: 0.1. Time taken for epoch: 22.36 seconds.

10:18:48: Train Epoch: 53 [0/45000 (0%)]	Loss: nan
10:18:48: Train Epoch: 53 [0/45000 (0%)]	Loss: nan
10:18:48: Train Epoch: 53 [0/45000 (0%)]	Loss: nan
10:18:52: Train Epoch: 53 [10000/45000 (22%)]	Loss: nan
10:18:52: Train Epoch: 53 [10000/45000 (22%)]	Loss: nan
10:18:52: Train Epoch: 53 [10000/45000 (22%)]	Loss: nan
10:18:57: Train Epoch: 53 [20000/45000 (44%)]	Loss: nan
10:18:57: Train Epoch: 53 [20000/45000 (44%)]	Loss: nan
10:18:57: Train Epoch: 53 [20000/45000 (44%)]	Loss: nan
10:19:02: Train Epoch: 53 [30000/45000 (67%)]	Loss: nan
10:19:02: Train Epoch: 53 [30000/45000 (67%)]	Loss: nan
10:19:02: Train Epoch: 53 [30000/45000 (67%)]	Loss: nan
10:19:06: Train Epoch: 53 [40000/45000 (89%)]	Loss: nan
10:19:06: Train Epoch: 53 [40000/45000 (89%)]	Loss: nan
10:19:06: Train Epoch: 53 [40000/45000 (89%)]	Loss: nan
10:19:10: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:19:10: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:19:10: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:19:10: Current learning rate: 0.1. Time taken for epoch: 22.96 seconds.

10:19:10: Current learning rate: 0.1. Time taken for epoch: 22.96 seconds.

10:19:10: Current learning rate: 0.1. Time taken for epoch: 22.96 seconds.

10:19:11: Train Epoch: 54 [0/45000 (0%)]	Loss: nan
10:19:11: Train Epoch: 54 [0/45000 (0%)]	Loss: nan
10:19:11: Train Epoch: 54 [0/45000 (0%)]	Loss: nan
10:19:16: Train Epoch: 54 [10000/45000 (22%)]	Loss: nan
10:19:16: Train Epoch: 54 [10000/45000 (22%)]	Loss: nan
10:19:16: Train Epoch: 54 [10000/45000 (22%)]	Loss: nan
10:19:20: Train Epoch: 54 [20000/45000 (44%)]	Loss: nan
10:19:20: Train Epoch: 54 [20000/45000 (44%)]	Loss: nan
10:19:20: Train Epoch: 54 [20000/45000 (44%)]	Loss: nan
10:19:25: Train Epoch: 54 [30000/45000 (67%)]	Loss: nan
10:19:25: Train Epoch: 54 [30000/45000 (67%)]	Loss: nan
10:19:25: Train Epoch: 54 [30000/45000 (67%)]	Loss: nan
10:19:29: Train Epoch: 54 [40000/45000 (89%)]	Loss: nan
10:19:29: Train Epoch: 54 [40000/45000 (89%)]	Loss: nan
10:19:29: Train Epoch: 54 [40000/45000 (89%)]	Loss: nan
10:19:33: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:19:33: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:19:33: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:19:33: Current learning rate: 0.1. Time taken for epoch: 22.81 seconds.

10:19:33: Current learning rate: 0.1. Time taken for epoch: 22.81 seconds.

10:19:33: Current learning rate: 0.1. Time taken for epoch: 22.81 seconds.

10:19:34: Train Epoch: 55 [0/45000 (0%)]	Loss: nan
10:19:34: Train Epoch: 55 [0/45000 (0%)]	Loss: nan
10:19:34: Train Epoch: 55 [0/45000 (0%)]	Loss: nan
10:19:38: Train Epoch: 55 [10000/45000 (22%)]	Loss: nan
10:19:38: Train Epoch: 55 [10000/45000 (22%)]	Loss: nan
10:19:38: Train Epoch: 55 [10000/45000 (22%)]	Loss: nan
10:19:43: Train Epoch: 55 [20000/45000 (44%)]	Loss: nan
10:19:43: Train Epoch: 55 [20000/45000 (44%)]	Loss: nan
10:19:43: Train Epoch: 55 [20000/45000 (44%)]	Loss: nan
10:19:47: Train Epoch: 55 [30000/45000 (67%)]	Loss: nan
10:19:47: Train Epoch: 55 [30000/45000 (67%)]	Loss: nan
10:19:47: Train Epoch: 55 [30000/45000 (67%)]	Loss: nan
10:19:52: Train Epoch: 55 [40000/45000 (89%)]	Loss: nan
10:19:52: Train Epoch: 55 [40000/45000 (89%)]	Loss: nan
10:19:52: Train Epoch: 55 [40000/45000 (89%)]	Loss: nan
10:19:56: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:19:56: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:19:56: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:19:56: Current learning rate: 0.1. Time taken for epoch: 22.78 seconds.

10:19:56: Current learning rate: 0.1. Time taken for epoch: 22.78 seconds.

10:19:56: Current learning rate: 0.1. Time taken for epoch: 22.78 seconds.

10:19:57: Train Epoch: 56 [0/45000 (0%)]	Loss: nan
10:19:57: Train Epoch: 56 [0/45000 (0%)]	Loss: nan
10:19:57: Train Epoch: 56 [0/45000 (0%)]	Loss: nan
10:20:01: Train Epoch: 56 [10000/45000 (22%)]	Loss: nan
10:20:01: Train Epoch: 56 [10000/45000 (22%)]	Loss: nan
10:20:01: Train Epoch: 56 [10000/45000 (22%)]	Loss: nan
10:20:06: Train Epoch: 56 [20000/45000 (44%)]	Loss: nan
10:20:06: Train Epoch: 56 [20000/45000 (44%)]	Loss: nan
10:20:06: Train Epoch: 56 [20000/45000 (44%)]	Loss: nan
10:20:10: Train Epoch: 56 [30000/45000 (67%)]	Loss: nan
10:20:10: Train Epoch: 56 [30000/45000 (67%)]	Loss: nan
10:20:10: Train Epoch: 56 [30000/45000 (67%)]	Loss: nan
10:20:15: Train Epoch: 56 [40000/45000 (89%)]	Loss: nan
10:20:15: Train Epoch: 56 [40000/45000 (89%)]	Loss: nan
10:20:15: Train Epoch: 56 [40000/45000 (89%)]	Loss: nan
10:20:19: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:20:19: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:20:19: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:20:19: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.16 seconds.

10:20:19: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.16 seconds.

10:20:19: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.16 seconds.

10:20:19: Train Epoch: 57 [0/45000 (0%)]	Loss: nan
10:20:19: Train Epoch: 57 [0/45000 (0%)]	Loss: nan
10:20:19: Train Epoch: 57 [0/45000 (0%)]	Loss: nan
10:20:24: Train Epoch: 57 [10000/45000 (22%)]	Loss: nan
10:20:24: Train Epoch: 57 [10000/45000 (22%)]	Loss: nan
10:20:24: Train Epoch: 57 [10000/45000 (22%)]	Loss: nan
10:20:29: Train Epoch: 57 [20000/45000 (44%)]	Loss: nan
10:20:29: Train Epoch: 57 [20000/45000 (44%)]	Loss: nan
10:20:29: Train Epoch: 57 [20000/45000 (44%)]	Loss: nan
10:20:33: Train Epoch: 57 [30000/45000 (67%)]	Loss: nan
10:20:33: Train Epoch: 57 [30000/45000 (67%)]	Loss: nan
10:20:33: Train Epoch: 57 [30000/45000 (67%)]	Loss: nan
10:20:37: Train Epoch: 57 [40000/45000 (89%)]	Loss: nan
10:20:37: Train Epoch: 57 [40000/45000 (89%)]	Loss: nan
10:20:37: Train Epoch: 57 [40000/45000 (89%)]	Loss: nan
10:20:41: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:20:41: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:20:41: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:20:41: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.29 seconds.

10:20:41: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.29 seconds.

10:20:41: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.29 seconds.

10:20:42: Train Epoch: 58 [0/45000 (0%)]	Loss: nan
10:20:42: Train Epoch: 58 [0/45000 (0%)]	Loss: nan
10:20:42: Train Epoch: 58 [0/45000 (0%)]	Loss: nan
10:20:46: Train Epoch: 58 [10000/45000 (22%)]	Loss: nan
10:20:46: Train Epoch: 58 [10000/45000 (22%)]	Loss: nan
10:20:46: Train Epoch: 58 [10000/45000 (22%)]	Loss: nan
10:20:51: Train Epoch: 58 [20000/45000 (44%)]	Loss: nan
10:20:51: Train Epoch: 58 [20000/45000 (44%)]	Loss: nan
10:20:51: Train Epoch: 58 [20000/45000 (44%)]	Loss: nan
10:20:56: Train Epoch: 58 [30000/45000 (67%)]	Loss: nan
10:20:56: Train Epoch: 58 [30000/45000 (67%)]	Loss: nan
10:20:56: Train Epoch: 58 [30000/45000 (67%)]	Loss: nan
10:21:01: Train Epoch: 58 [40000/45000 (89%)]	Loss: nan
10:21:01: Train Epoch: 58 [40000/45000 (89%)]	Loss: nan
10:21:01: Train Epoch: 58 [40000/45000 (89%)]	Loss: nan
10:21:05: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:21:05: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:21:05: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:21:05: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.58 seconds.

10:21:05: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.58 seconds.

10:21:05: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.58 seconds.

10:21:05: Train Epoch: 59 [0/45000 (0%)]	Loss: nan
10:21:05: Train Epoch: 59 [0/45000 (0%)]	Loss: nan
10:21:05: Train Epoch: 59 [0/45000 (0%)]	Loss: nan
10:21:10: Train Epoch: 59 [10000/45000 (22%)]	Loss: nan
10:21:10: Train Epoch: 59 [10000/45000 (22%)]	Loss: nan
10:21:10: Train Epoch: 59 [10000/45000 (22%)]	Loss: nan
10:21:15: Train Epoch: 59 [20000/45000 (44%)]	Loss: nan
10:21:15: Train Epoch: 59 [20000/45000 (44%)]	Loss: nan
10:21:15: Train Epoch: 59 [20000/45000 (44%)]	Loss: nan
10:21:19: Train Epoch: 59 [30000/45000 (67%)]	Loss: nan
10:21:19: Train Epoch: 59 [30000/45000 (67%)]	Loss: nan
10:21:19: Train Epoch: 59 [30000/45000 (67%)]	Loss: nan
10:21:24: Train Epoch: 59 [40000/45000 (89%)]	Loss: nan
10:21:24: Train Epoch: 59 [40000/45000 (89%)]	Loss: nan
10:21:24: Train Epoch: 59 [40000/45000 (89%)]	Loss: nan
10:21:28: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:21:28: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:21:28: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:21:28: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.98 seconds.

10:21:28: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.98 seconds.

10:21:28: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.98 seconds.

10:21:28: Train Epoch: 60 [0/45000 (0%)]	Loss: nan
10:21:28: Train Epoch: 60 [0/45000 (0%)]	Loss: nan
10:21:28: Train Epoch: 60 [0/45000 (0%)]	Loss: nan
10:21:33: Train Epoch: 60 [10000/45000 (22%)]	Loss: nan
10:21:33: Train Epoch: 60 [10000/45000 (22%)]	Loss: nan
10:21:33: Train Epoch: 60 [10000/45000 (22%)]	Loss: nan
10:21:37: Train Epoch: 60 [20000/45000 (44%)]	Loss: nan
10:21:37: Train Epoch: 60 [20000/45000 (44%)]	Loss: nan
10:21:37: Train Epoch: 60 [20000/45000 (44%)]	Loss: nan
10:21:42: Train Epoch: 60 [30000/45000 (67%)]	Loss: nan
10:21:42: Train Epoch: 60 [30000/45000 (67%)]	Loss: nan
10:21:42: Train Epoch: 60 [30000/45000 (67%)]	Loss: nan
10:21:46: Train Epoch: 60 [40000/45000 (89%)]	Loss: nan
10:21:46: Train Epoch: 60 [40000/45000 (89%)]	Loss: nan
10:21:46: Train Epoch: 60 [40000/45000 (89%)]	Loss: nan
10:21:50: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:21:50: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:21:50: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:21:50: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.50 seconds.

10:21:50: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.50 seconds.

10:21:50: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.50 seconds.

10:21:51: Train Epoch: 61 [0/45000 (0%)]	Loss: nan
10:21:51: Train Epoch: 61 [0/45000 (0%)]	Loss: nan
10:21:51: Train Epoch: 61 [0/45000 (0%)]	Loss: nan
10:21:56: Train Epoch: 61 [10000/45000 (22%)]	Loss: nan
10:21:56: Train Epoch: 61 [10000/45000 (22%)]	Loss: nan
10:21:56: Train Epoch: 61 [10000/45000 (22%)]	Loss: nan
10:22:01: Train Epoch: 61 [20000/45000 (44%)]	Loss: nan
10:22:01: Train Epoch: 61 [20000/45000 (44%)]	Loss: nan
10:22:01: Train Epoch: 61 [20000/45000 (44%)]	Loss: nan
10:22:05: Train Epoch: 61 [30000/45000 (67%)]	Loss: nan
10:22:05: Train Epoch: 61 [30000/45000 (67%)]	Loss: nan
10:22:05: Train Epoch: 61 [30000/45000 (67%)]	Loss: nan
10:22:10: Train Epoch: 61 [40000/45000 (89%)]	Loss: nan
10:22:10: Train Epoch: 61 [40000/45000 (89%)]	Loss: nan
10:22:10: Train Epoch: 61 [40000/45000 (89%)]	Loss: nan
10:22:14: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:22:14: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:22:14: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:22:14: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.65 seconds.

10:22:14: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.65 seconds.

10:22:14: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.65 seconds.

10:22:15: Train Epoch: 62 [0/45000 (0%)]	Loss: nan
10:22:15: Train Epoch: 62 [0/45000 (0%)]	Loss: nan
10:22:15: Train Epoch: 62 [0/45000 (0%)]	Loss: nan
10:22:19: Train Epoch: 62 [10000/45000 (22%)]	Loss: nan
10:22:19: Train Epoch: 62 [10000/45000 (22%)]	Loss: nan
10:22:19: Train Epoch: 62 [10000/45000 (22%)]	Loss: nan
10:22:24: Train Epoch: 62 [20000/45000 (44%)]	Loss: nan
10:22:24: Train Epoch: 62 [20000/45000 (44%)]	Loss: nan
10:22:24: Train Epoch: 62 [20000/45000 (44%)]	Loss: nan
10:22:28: Train Epoch: 62 [30000/45000 (67%)]	Loss: nan
10:22:28: Train Epoch: 62 [30000/45000 (67%)]	Loss: nan
10:22:28: Train Epoch: 62 [30000/45000 (67%)]	Loss: nan
10:22:32: Train Epoch: 62 [40000/45000 (89%)]	Loss: nan
10:22:32: Train Epoch: 62 [40000/45000 (89%)]	Loss: nan
10:22:32: Train Epoch: 62 [40000/45000 (89%)]	Loss: nan
10:22:36: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:22:36: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:22:36: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:22:36: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.24 seconds.

10:22:36: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.24 seconds.

10:22:36: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.24 seconds.

10:22:37: Train Epoch: 63 [0/45000 (0%)]	Loss: nan
10:22:37: Train Epoch: 63 [0/45000 (0%)]	Loss: nan
10:22:37: Train Epoch: 63 [0/45000 (0%)]	Loss: nan
10:22:41: Train Epoch: 63 [10000/45000 (22%)]	Loss: nan
10:22:41: Train Epoch: 63 [10000/45000 (22%)]	Loss: nan
10:22:41: Train Epoch: 63 [10000/45000 (22%)]	Loss: nan
10:22:46: Train Epoch: 63 [20000/45000 (44%)]	Loss: nan
10:22:46: Train Epoch: 63 [20000/45000 (44%)]	Loss: nan
10:22:46: Train Epoch: 63 [20000/45000 (44%)]	Loss: nan
10:22:50: Train Epoch: 63 [30000/45000 (67%)]	Loss: nan
10:22:50: Train Epoch: 63 [30000/45000 (67%)]	Loss: nan
10:22:50: Train Epoch: 63 [30000/45000 (67%)]	Loss: nan
10:22:55: Train Epoch: 63 [40000/45000 (89%)]	Loss: nan
10:22:55: Train Epoch: 63 [40000/45000 (89%)]	Loss: nan
10:22:55: Train Epoch: 63 [40000/45000 (89%)]	Loss: nan
10:22:59: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:22:59: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:22:59: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:22:59: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.74 seconds.

10:22:59: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.74 seconds.

10:22:59: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.74 seconds.

10:22:59: Train Epoch: 64 [0/45000 (0%)]	Loss: nan
10:22:59: Train Epoch: 64 [0/45000 (0%)]	Loss: nan
10:22:59: Train Epoch: 64 [0/45000 (0%)]	Loss: nan
10:23:04: Train Epoch: 64 [10000/45000 (22%)]	Loss: nan
10:23:04: Train Epoch: 64 [10000/45000 (22%)]	Loss: nan
10:23:04: Train Epoch: 64 [10000/45000 (22%)]	Loss: nan
10:23:09: Train Epoch: 64 [20000/45000 (44%)]	Loss: nan
10:23:09: Train Epoch: 64 [20000/45000 (44%)]	Loss: nan
10:23:09: Train Epoch: 64 [20000/45000 (44%)]	Loss: nan
10:23:14: Train Epoch: 64 [30000/45000 (67%)]	Loss: nan
10:23:14: Train Epoch: 64 [30000/45000 (67%)]	Loss: nan
10:23:14: Train Epoch: 64 [30000/45000 (67%)]	Loss: nan
10:23:18: Train Epoch: 64 [40000/45000 (89%)]	Loss: nan
10:23:18: Train Epoch: 64 [40000/45000 (89%)]	Loss: nan
10:23:18: Train Epoch: 64 [40000/45000 (89%)]	Loss: nan
10:23:22: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:23:22: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:23:22: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:23:22: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.29 seconds.

10:23:22: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.29 seconds.

10:23:22: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.29 seconds.

10:23:23: Train Epoch: 65 [0/45000 (0%)]	Loss: nan
10:23:23: Train Epoch: 65 [0/45000 (0%)]	Loss: nan
10:23:23: Train Epoch: 65 [0/45000 (0%)]	Loss: nan
10:23:27: Train Epoch: 65 [10000/45000 (22%)]	Loss: nan
10:23:27: Train Epoch: 65 [10000/45000 (22%)]	Loss: nan
10:23:27: Train Epoch: 65 [10000/45000 (22%)]	Loss: nan
10:23:32: Train Epoch: 65 [20000/45000 (44%)]	Loss: nan
10:23:32: Train Epoch: 65 [20000/45000 (44%)]	Loss: nan
10:23:32: Train Epoch: 65 [20000/45000 (44%)]	Loss: nan
10:23:36: Train Epoch: 65 [30000/45000 (67%)]	Loss: nan
10:23:36: Train Epoch: 65 [30000/45000 (67%)]	Loss: nan
10:23:36: Train Epoch: 65 [30000/45000 (67%)]	Loss: nan
10:23:41: Train Epoch: 65 [40000/45000 (89%)]	Loss: nan
10:23:41: Train Epoch: 65 [40000/45000 (89%)]	Loss: nan
10:23:41: Train Epoch: 65 [40000/45000 (89%)]	Loss: nan
10:23:45: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:23:45: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:23:45: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:23:45: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.40 seconds.

10:23:45: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.40 seconds.

10:23:45: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.40 seconds.

10:23:45: Train Epoch: 66 [0/45000 (0%)]	Loss: nan
10:23:45: Train Epoch: 66 [0/45000 (0%)]	Loss: nan
10:23:45: Train Epoch: 66 [0/45000 (0%)]	Loss: nan
10:23:50: Train Epoch: 66 [10000/45000 (22%)]	Loss: nan
10:23:50: Train Epoch: 66 [10000/45000 (22%)]	Loss: nan
10:23:50: Train Epoch: 66 [10000/45000 (22%)]	Loss: nan
10:23:54: Train Epoch: 66 [20000/45000 (44%)]	Loss: nan
10:23:54: Train Epoch: 66 [20000/45000 (44%)]	Loss: nan
10:23:54: Train Epoch: 66 [20000/45000 (44%)]	Loss: nan
10:23:59: Train Epoch: 66 [30000/45000 (67%)]	Loss: nan
10:23:59: Train Epoch: 66 [30000/45000 (67%)]	Loss: nan
10:23:59: Train Epoch: 66 [30000/45000 (67%)]	Loss: nan
10:24:04: Train Epoch: 66 [40000/45000 (89%)]	Loss: nan
10:24:04: Train Epoch: 66 [40000/45000 (89%)]	Loss: nan
10:24:04: Train Epoch: 66 [40000/45000 (89%)]	Loss: nan
10:24:07: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:24:07: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:24:07: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:24:07: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.80 seconds.

10:24:07: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.80 seconds.

10:24:07: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.80 seconds.

10:24:08: Train Epoch: 67 [0/45000 (0%)]	Loss: nan
10:24:08: Train Epoch: 67 [0/45000 (0%)]	Loss: nan
10:24:08: Train Epoch: 67 [0/45000 (0%)]	Loss: nan
10:24:13: Train Epoch: 67 [10000/45000 (22%)]	Loss: nan
10:24:13: Train Epoch: 67 [10000/45000 (22%)]	Loss: nan
10:24:13: Train Epoch: 67 [10000/45000 (22%)]	Loss: nan
10:24:17: Train Epoch: 67 [20000/45000 (44%)]	Loss: nan
10:24:17: Train Epoch: 67 [20000/45000 (44%)]	Loss: nan
10:24:17: Train Epoch: 67 [20000/45000 (44%)]	Loss: nan
10:24:22: Train Epoch: 67 [30000/45000 (67%)]	Loss: nan
10:24:22: Train Epoch: 67 [30000/45000 (67%)]	Loss: nan
10:24:22: Train Epoch: 67 [30000/45000 (67%)]	Loss: nan
10:24:26: Train Epoch: 67 [40000/45000 (89%)]	Loss: nan
10:24:26: Train Epoch: 67 [40000/45000 (89%)]	Loss: nan
10:24:26: Train Epoch: 67 [40000/45000 (89%)]	Loss: nan
10:24:30: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:24:30: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:24:30: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:24:30: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.37 seconds.

10:24:30: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.37 seconds.

10:24:30: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.37 seconds.

10:24:30: Train Epoch: 68 [0/45000 (0%)]	Loss: nan
10:24:30: Train Epoch: 68 [0/45000 (0%)]	Loss: nan
10:24:30: Train Epoch: 68 [0/45000 (0%)]	Loss: nan
10:24:35: Train Epoch: 68 [10000/45000 (22%)]	Loss: nan
10:24:35: Train Epoch: 68 [10000/45000 (22%)]	Loss: nan
10:24:35: Train Epoch: 68 [10000/45000 (22%)]	Loss: nan
10:24:39: Train Epoch: 68 [20000/45000 (44%)]	Loss: nan
10:24:39: Train Epoch: 68 [20000/45000 (44%)]	Loss: nan
10:24:39: Train Epoch: 68 [20000/45000 (44%)]	Loss: nan
10:24:44: Train Epoch: 68 [30000/45000 (67%)]	Loss: nan
10:24:44: Train Epoch: 68 [30000/45000 (67%)]	Loss: nan
10:24:44: Train Epoch: 68 [30000/45000 (67%)]	Loss: nan
10:24:48: Train Epoch: 68 [40000/45000 (89%)]	Loss: nan
10:24:48: Train Epoch: 68 [40000/45000 (89%)]	Loss: nan
10:24:48: Train Epoch: 68 [40000/45000 (89%)]	Loss: nan
10:24:52: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:24:52: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:24:52: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:24:52: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.35 seconds.

10:24:52: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.35 seconds.

10:24:52: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.35 seconds.

10:24:53: Train Epoch: 69 [0/45000 (0%)]	Loss: nan
10:24:53: Train Epoch: 69 [0/45000 (0%)]	Loss: nan
10:24:53: Train Epoch: 69 [0/45000 (0%)]	Loss: nan
10:24:58: Train Epoch: 69 [10000/45000 (22%)]	Loss: nan
10:24:58: Train Epoch: 69 [10000/45000 (22%)]	Loss: nan
10:24:58: Train Epoch: 69 [10000/45000 (22%)]	Loss: nan
10:25:03: Train Epoch: 69 [20000/45000 (44%)]	Loss: nan
10:25:03: Train Epoch: 69 [20000/45000 (44%)]	Loss: nan
10:25:03: Train Epoch: 69 [20000/45000 (44%)]	Loss: nan
10:25:07: Train Epoch: 69 [30000/45000 (67%)]	Loss: nan
10:25:07: Train Epoch: 69 [30000/45000 (67%)]	Loss: nan
10:25:07: Train Epoch: 69 [30000/45000 (67%)]	Loss: nan
10:25:11: Train Epoch: 69 [40000/45000 (89%)]	Loss: nan
10:25:11: Train Epoch: 69 [40000/45000 (89%)]	Loss: nan
10:25:11: Train Epoch: 69 [40000/45000 (89%)]	Loss: nan
10:25:15: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:25:15: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:25:15: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:25:15: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.37 seconds.

10:25:15: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.37 seconds.

10:25:15: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.37 seconds.

10:25:16: Train Epoch: 70 [0/45000 (0%)]	Loss: nan
10:25:16: Train Epoch: 70 [0/45000 (0%)]	Loss: nan
10:25:16: Train Epoch: 70 [0/45000 (0%)]	Loss: nan
10:25:21: Train Epoch: 70 [10000/45000 (22%)]	Loss: nan
10:25:21: Train Epoch: 70 [10000/45000 (22%)]	Loss: nan
10:25:21: Train Epoch: 70 [10000/45000 (22%)]	Loss: nan
10:25:25: Train Epoch: 70 [20000/45000 (44%)]	Loss: nan
10:25:25: Train Epoch: 70 [20000/45000 (44%)]	Loss: nan
10:25:25: Train Epoch: 70 [20000/45000 (44%)]	Loss: nan
10:25:29: Train Epoch: 70 [30000/45000 (67%)]	Loss: nan
10:25:29: Train Epoch: 70 [30000/45000 (67%)]	Loss: nan
10:25:29: Train Epoch: 70 [30000/45000 (67%)]	Loss: nan
10:25:34: Train Epoch: 70 [40000/45000 (89%)]	Loss: nan
10:25:34: Train Epoch: 70 [40000/45000 (89%)]	Loss: nan
10:25:34: Train Epoch: 70 [40000/45000 (89%)]	Loss: nan
10:25:37: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:25:37: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:25:37: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:25:37: Current learning rate: 0.010000000000000002. Time taken for epoch: 21.97 seconds.

10:25:37: Current learning rate: 0.010000000000000002. Time taken for epoch: 21.97 seconds.

10:25:37: Current learning rate: 0.010000000000000002. Time taken for epoch: 21.97 seconds.

10:25:38: Train Epoch: 71 [0/45000 (0%)]	Loss: nan
10:25:38: Train Epoch: 71 [0/45000 (0%)]	Loss: nan
10:25:38: Train Epoch: 71 [0/45000 (0%)]	Loss: nan
10:25:43: Train Epoch: 71 [10000/45000 (22%)]	Loss: nan
10:25:43: Train Epoch: 71 [10000/45000 (22%)]	Loss: nan
10:25:43: Train Epoch: 71 [10000/45000 (22%)]	Loss: nan
10:25:47: Train Epoch: 71 [20000/45000 (44%)]	Loss: nan
10:25:47: Train Epoch: 71 [20000/45000 (44%)]	Loss: nan
10:25:47: Train Epoch: 71 [20000/45000 (44%)]	Loss: nan
10:25:52: Train Epoch: 71 [30000/45000 (67%)]	Loss: nan
10:25:52: Train Epoch: 71 [30000/45000 (67%)]	Loss: nan
10:25:52: Train Epoch: 71 [30000/45000 (67%)]	Loss: nan
10:25:56: Train Epoch: 71 [40000/45000 (89%)]	Loss: nan
10:25:56: Train Epoch: 71 [40000/45000 (89%)]	Loss: nan
10:25:56: Train Epoch: 71 [40000/45000 (89%)]	Loss: nan
10:26:00: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:26:00: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:26:00: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:26:00: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.76 seconds.

10:26:00: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.76 seconds.

10:26:00: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.76 seconds.

10:26:01: Train Epoch: 72 [0/45000 (0%)]	Loss: nan
10:26:01: Train Epoch: 72 [0/45000 (0%)]	Loss: nan
10:26:01: Train Epoch: 72 [0/45000 (0%)]	Loss: nan
10:26:06: Train Epoch: 72 [10000/45000 (22%)]	Loss: nan
10:26:06: Train Epoch: 72 [10000/45000 (22%)]	Loss: nan
10:26:06: Train Epoch: 72 [10000/45000 (22%)]	Loss: nan
10:26:10: Train Epoch: 72 [20000/45000 (44%)]	Loss: nan
10:26:10: Train Epoch: 72 [20000/45000 (44%)]	Loss: nan
10:26:10: Train Epoch: 72 [20000/45000 (44%)]	Loss: nan
10:26:15: Train Epoch: 72 [30000/45000 (67%)]	Loss: nan
10:26:15: Train Epoch: 72 [30000/45000 (67%)]	Loss: nan
10:26:15: Train Epoch: 72 [30000/45000 (67%)]	Loss: nan
10:26:19: Train Epoch: 72 [40000/45000 (89%)]	Loss: nan
10:26:19: Train Epoch: 72 [40000/45000 (89%)]	Loss: nan
10:26:19: Train Epoch: 72 [40000/45000 (89%)]	Loss: nan
10:26:23: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:26:23: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:26:23: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:26:23: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.93 seconds.

10:26:23: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.93 seconds.

10:26:23: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.93 seconds.

10:26:24: Train Epoch: 73 [0/45000 (0%)]	Loss: nan
10:26:24: Train Epoch: 73 [0/45000 (0%)]	Loss: nan
10:26:24: Train Epoch: 73 [0/45000 (0%)]	Loss: nan
10:26:28: Train Epoch: 73 [10000/45000 (22%)]	Loss: nan
10:26:28: Train Epoch: 73 [10000/45000 (22%)]	Loss: nan
10:26:28: Train Epoch: 73 [10000/45000 (22%)]	Loss: nan
10:26:33: Train Epoch: 73 [20000/45000 (44%)]	Loss: nan
10:26:33: Train Epoch: 73 [20000/45000 (44%)]	Loss: nan
10:26:33: Train Epoch: 73 [20000/45000 (44%)]	Loss: nan
10:26:37: Train Epoch: 73 [30000/45000 (67%)]	Loss: nan
10:26:37: Train Epoch: 73 [30000/45000 (67%)]	Loss: nan
10:26:37: Train Epoch: 73 [30000/45000 (67%)]	Loss: nan
10:26:41: Train Epoch: 73 [40000/45000 (89%)]	Loss: nan
10:26:41: Train Epoch: 73 [40000/45000 (89%)]	Loss: nan
10:26:41: Train Epoch: 73 [40000/45000 (89%)]	Loss: nan
10:26:45: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:26:45: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:26:45: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:26:45: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.26 seconds.

10:26:45: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.26 seconds.

10:26:45: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.26 seconds.

10:26:46: Train Epoch: 74 [0/45000 (0%)]	Loss: nan
10:26:46: Train Epoch: 74 [0/45000 (0%)]	Loss: nan
10:26:46: Train Epoch: 74 [0/45000 (0%)]	Loss: nan
10:26:50: Train Epoch: 74 [10000/45000 (22%)]	Loss: nan
10:26:50: Train Epoch: 74 [10000/45000 (22%)]	Loss: nan
10:26:50: Train Epoch: 74 [10000/45000 (22%)]	Loss: nan
10:26:55: Train Epoch: 74 [20000/45000 (44%)]	Loss: nan
10:26:55: Train Epoch: 74 [20000/45000 (44%)]	Loss: nan
10:26:55: Train Epoch: 74 [20000/45000 (44%)]	Loss: nan
10:27:00: Train Epoch: 74 [30000/45000 (67%)]	Loss: nan
10:27:00: Train Epoch: 74 [30000/45000 (67%)]	Loss: nan
10:27:00: Train Epoch: 74 [30000/45000 (67%)]	Loss: nan
10:27:04: Train Epoch: 74 [40000/45000 (89%)]	Loss: nan
10:27:04: Train Epoch: 74 [40000/45000 (89%)]	Loss: nan
10:27:04: Train Epoch: 74 [40000/45000 (89%)]	Loss: nan
10:27:08: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:27:08: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:27:08: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:27:08: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.77 seconds.

10:27:08: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.77 seconds.

10:27:08: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.77 seconds.

10:27:09: Train Epoch: 75 [0/45000 (0%)]	Loss: nan
10:27:09: Train Epoch: 75 [0/45000 (0%)]	Loss: nan
10:27:09: Train Epoch: 75 [0/45000 (0%)]	Loss: nan
10:27:14: Train Epoch: 75 [10000/45000 (22%)]	Loss: nan
10:27:14: Train Epoch: 75 [10000/45000 (22%)]	Loss: nan
10:27:14: Train Epoch: 75 [10000/45000 (22%)]	Loss: nan
10:27:18: Train Epoch: 75 [20000/45000 (44%)]	Loss: nan
10:27:18: Train Epoch: 75 [20000/45000 (44%)]	Loss: nan
10:27:18: Train Epoch: 75 [20000/45000 (44%)]	Loss: nan
10:27:23: Train Epoch: 75 [30000/45000 (67%)]	Loss: nan
10:27:23: Train Epoch: 75 [30000/45000 (67%)]	Loss: nan
10:27:23: Train Epoch: 75 [30000/45000 (67%)]	Loss: nan
10:27:27: Train Epoch: 75 [40000/45000 (89%)]	Loss: nan
10:27:27: Train Epoch: 75 [40000/45000 (89%)]	Loss: nan
10:27:27: Train Epoch: 75 [40000/45000 (89%)]	Loss: nan
10:27:31: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:27:31: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:27:31: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:27:31: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.49 seconds.

10:27:31: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.49 seconds.

10:27:31: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.49 seconds.

10:27:31: Train Epoch: 76 [0/45000 (0%)]	Loss: nan
10:27:31: Train Epoch: 76 [0/45000 (0%)]	Loss: nan
10:27:31: Train Epoch: 76 [0/45000 (0%)]	Loss: nan
10:27:36: Train Epoch: 76 [10000/45000 (22%)]	Loss: nan
10:27:36: Train Epoch: 76 [10000/45000 (22%)]	Loss: nan
10:27:36: Train Epoch: 76 [10000/45000 (22%)]	Loss: nan
10:27:40: Train Epoch: 76 [20000/45000 (44%)]	Loss: nan
10:27:40: Train Epoch: 76 [20000/45000 (44%)]	Loss: nan
10:27:40: Train Epoch: 76 [20000/45000 (44%)]	Loss: nan
10:27:45: Train Epoch: 76 [30000/45000 (67%)]	Loss: nan
10:27:45: Train Epoch: 76 [30000/45000 (67%)]	Loss: nan
10:27:45: Train Epoch: 76 [30000/45000 (67%)]	Loss: nan
10:27:49: Train Epoch: 76 [40000/45000 (89%)]	Loss: nan
10:27:49: Train Epoch: 76 [40000/45000 (89%)]	Loss: nan
10:27:49: Train Epoch: 76 [40000/45000 (89%)]	Loss: nan
10:27:53: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:27:53: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:27:53: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:27:53: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.40 seconds.

10:27:53: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.40 seconds.

10:27:53: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.40 seconds.

10:27:54: Train Epoch: 77 [0/45000 (0%)]	Loss: nan
10:27:54: Train Epoch: 77 [0/45000 (0%)]	Loss: nan
10:27:54: Train Epoch: 77 [0/45000 (0%)]	Loss: nan
10:27:59: Train Epoch: 77 [10000/45000 (22%)]	Loss: nan
10:27:59: Train Epoch: 77 [10000/45000 (22%)]	Loss: nan
10:27:59: Train Epoch: 77 [10000/45000 (22%)]	Loss: nan
10:28:03: Train Epoch: 77 [20000/45000 (44%)]	Loss: nan
10:28:03: Train Epoch: 77 [20000/45000 (44%)]	Loss: nan
10:28:03: Train Epoch: 77 [20000/45000 (44%)]	Loss: nan
10:28:08: Train Epoch: 77 [30000/45000 (67%)]	Loss: nan
10:28:08: Train Epoch: 77 [30000/45000 (67%)]	Loss: nan
10:28:08: Train Epoch: 77 [30000/45000 (67%)]	Loss: nan
10:28:12: Train Epoch: 77 [40000/45000 (89%)]	Loss: nan
10:28:12: Train Epoch: 77 [40000/45000 (89%)]	Loss: nan
10:28:12: Train Epoch: 77 [40000/45000 (89%)]	Loss: nan
10:28:16: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:28:16: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:28:16: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:28:16: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.35 seconds.

10:28:16: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.35 seconds.

10:28:16: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.35 seconds.

10:28:17: Train Epoch: 78 [0/45000 (0%)]	Loss: nan
10:28:17: Train Epoch: 78 [0/45000 (0%)]	Loss: nan
10:28:17: Train Epoch: 78 [0/45000 (0%)]	Loss: nan
10:28:22: Train Epoch: 78 [10000/45000 (22%)]	Loss: nan
10:28:22: Train Epoch: 78 [10000/45000 (22%)]	Loss: nan
10:28:22: Train Epoch: 78 [10000/45000 (22%)]	Loss: nan
10:28:26: Train Epoch: 78 [20000/45000 (44%)]	Loss: nan
10:28:26: Train Epoch: 78 [20000/45000 (44%)]	Loss: nan
10:28:26: Train Epoch: 78 [20000/45000 (44%)]	Loss: nan
10:28:30: Train Epoch: 78 [30000/45000 (67%)]	Loss: nan
10:28:30: Train Epoch: 78 [30000/45000 (67%)]	Loss: nan
10:28:30: Train Epoch: 78 [30000/45000 (67%)]	Loss: nan
10:28:35: Train Epoch: 78 [40000/45000 (89%)]	Loss: nan
10:28:35: Train Epoch: 78 [40000/45000 (89%)]	Loss: nan
10:28:35: Train Epoch: 78 [40000/45000 (89%)]	Loss: nan
10:28:38: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:28:38: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:28:38: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:28:38: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.03 seconds.

10:28:38: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.03 seconds.

10:28:38: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.03 seconds.

10:28:39: Train Epoch: 79 [0/45000 (0%)]	Loss: nan
10:28:39: Train Epoch: 79 [0/45000 (0%)]	Loss: nan
10:28:39: Train Epoch: 79 [0/45000 (0%)]	Loss: nan
10:28:44: Train Epoch: 79 [10000/45000 (22%)]	Loss: nan
10:28:44: Train Epoch: 79 [10000/45000 (22%)]	Loss: nan
10:28:44: Train Epoch: 79 [10000/45000 (22%)]	Loss: nan
10:28:48: Train Epoch: 79 [20000/45000 (44%)]	Loss: nan
10:28:48: Train Epoch: 79 [20000/45000 (44%)]	Loss: nan
10:28:48: Train Epoch: 79 [20000/45000 (44%)]	Loss: nan
10:28:53: Train Epoch: 79 [30000/45000 (67%)]	Loss: nan
10:28:53: Train Epoch: 79 [30000/45000 (67%)]	Loss: nan
10:28:53: Train Epoch: 79 [30000/45000 (67%)]	Loss: nan
10:28:57: Train Epoch: 79 [40000/45000 (89%)]	Loss: nan
10:28:57: Train Epoch: 79 [40000/45000 (89%)]	Loss: nan
10:28:57: Train Epoch: 79 [40000/45000 (89%)]	Loss: nan
10:29:02: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:29:02: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:29:02: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:29:02: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.16 seconds.

10:29:02: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.16 seconds.

10:29:02: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.16 seconds.

10:29:02: Train Epoch: 80 [0/45000 (0%)]	Loss: nan
10:29:02: Train Epoch: 80 [0/45000 (0%)]	Loss: nan
10:29:02: Train Epoch: 80 [0/45000 (0%)]	Loss: nan
10:29:07: Train Epoch: 80 [10000/45000 (22%)]	Loss: nan
10:29:07: Train Epoch: 80 [10000/45000 (22%)]	Loss: nan
10:29:07: Train Epoch: 80 [10000/45000 (22%)]	Loss: nan
10:29:11: Train Epoch: 80 [20000/45000 (44%)]	Loss: nan
10:29:11: Train Epoch: 80 [20000/45000 (44%)]	Loss: nan
10:29:11: Train Epoch: 80 [20000/45000 (44%)]	Loss: nan
10:29:16: Train Epoch: 80 [30000/45000 (67%)]	Loss: nan
10:29:16: Train Epoch: 80 [30000/45000 (67%)]	Loss: nan
10:29:16: Train Epoch: 80 [30000/45000 (67%)]	Loss: nan
10:29:20: Train Epoch: 80 [40000/45000 (89%)]	Loss: nan
10:29:20: Train Epoch: 80 [40000/45000 (89%)]	Loss: nan
10:29:20: Train Epoch: 80 [40000/45000 (89%)]	Loss: nan
10:29:24: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:29:24: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:29:24: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:29:24: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.60 seconds.

10:29:24: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.60 seconds.

10:29:24: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.60 seconds.

10:29:25: Train Epoch: 81 [0/45000 (0%)]	Loss: nan
10:29:25: Train Epoch: 81 [0/45000 (0%)]	Loss: nan
10:29:25: Train Epoch: 81 [0/45000 (0%)]	Loss: nan
10:29:29: Train Epoch: 81 [10000/45000 (22%)]	Loss: nan
10:29:29: Train Epoch: 81 [10000/45000 (22%)]	Loss: nan
10:29:29: Train Epoch: 81 [10000/45000 (22%)]	Loss: nan
10:29:34: Train Epoch: 81 [20000/45000 (44%)]	Loss: nan
10:29:34: Train Epoch: 81 [20000/45000 (44%)]	Loss: nan
10:29:34: Train Epoch: 81 [20000/45000 (44%)]	Loss: nan
10:29:38: Train Epoch: 81 [30000/45000 (67%)]	Loss: nan
10:29:38: Train Epoch: 81 [30000/45000 (67%)]	Loss: nan
10:29:38: Train Epoch: 81 [30000/45000 (67%)]	Loss: nan
10:29:43: Train Epoch: 81 [40000/45000 (89%)]	Loss: nan
10:29:43: Train Epoch: 81 [40000/45000 (89%)]	Loss: nan
10:29:43: Train Epoch: 81 [40000/45000 (89%)]	Loss: nan
10:29:46: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:29:46: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:29:46: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:29:46: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.22 seconds.

10:29:46: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.22 seconds.

10:29:46: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.22 seconds.

10:29:47: Train Epoch: 82 [0/45000 (0%)]	Loss: nan
10:29:47: Train Epoch: 82 [0/45000 (0%)]	Loss: nan
10:29:47: Train Epoch: 82 [0/45000 (0%)]	Loss: nan
10:29:52: Train Epoch: 82 [10000/45000 (22%)]	Loss: nan
10:29:52: Train Epoch: 82 [10000/45000 (22%)]	Loss: nan
10:29:52: Train Epoch: 82 [10000/45000 (22%)]	Loss: nan
10:29:56: Train Epoch: 82 [20000/45000 (44%)]	Loss: nan
10:29:56: Train Epoch: 82 [20000/45000 (44%)]	Loss: nan
10:29:56: Train Epoch: 82 [20000/45000 (44%)]	Loss: nan
10:30:01: Train Epoch: 82 [30000/45000 (67%)]	Loss: nan
10:30:01: Train Epoch: 82 [30000/45000 (67%)]	Loss: nan
10:30:01: Train Epoch: 82 [30000/45000 (67%)]	Loss: nan
10:30:06: Train Epoch: 82 [40000/45000 (89%)]	Loss: nan
10:30:06: Train Epoch: 82 [40000/45000 (89%)]	Loss: nan
10:30:06: Train Epoch: 82 [40000/45000 (89%)]	Loss: nan
10:30:09: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:30:09: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:30:09: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:30:09: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.88 seconds.

10:30:09: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.88 seconds.

10:30:09: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.88 seconds.

10:30:10: Train Epoch: 83 [0/45000 (0%)]	Loss: nan
10:30:10: Train Epoch: 83 [0/45000 (0%)]	Loss: nan
10:30:10: Train Epoch: 83 [0/45000 (0%)]	Loss: nan
10:30:15: Train Epoch: 83 [10000/45000 (22%)]	Loss: nan
10:30:15: Train Epoch: 83 [10000/45000 (22%)]	Loss: nan
10:30:15: Train Epoch: 83 [10000/45000 (22%)]	Loss: nan
10:30:19: Train Epoch: 83 [20000/45000 (44%)]	Loss: nan
10:30:19: Train Epoch: 83 [20000/45000 (44%)]	Loss: nan
10:30:19: Train Epoch: 83 [20000/45000 (44%)]	Loss: nan
10:30:24: Train Epoch: 83 [30000/45000 (67%)]	Loss: nan
10:30:24: Train Epoch: 83 [30000/45000 (67%)]	Loss: nan
10:30:24: Train Epoch: 83 [30000/45000 (67%)]	Loss: nan
10:30:28: Train Epoch: 83 [40000/45000 (89%)]	Loss: nan
10:30:28: Train Epoch: 83 [40000/45000 (89%)]	Loss: nan
10:30:28: Train Epoch: 83 [40000/45000 (89%)]	Loss: nan
10:30:32: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:30:32: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:30:32: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:30:32: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.50 seconds.

10:30:32: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.50 seconds.

10:30:32: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.50 seconds.

10:30:32: Train Epoch: 84 [0/45000 (0%)]	Loss: nan
10:30:32: Train Epoch: 84 [0/45000 (0%)]	Loss: nan
10:30:32: Train Epoch: 84 [0/45000 (0%)]	Loss: nan
10:30:37: Train Epoch: 84 [10000/45000 (22%)]	Loss: nan
10:30:37: Train Epoch: 84 [10000/45000 (22%)]	Loss: nan
10:30:37: Train Epoch: 84 [10000/45000 (22%)]	Loss: nan
10:30:41: Train Epoch: 84 [20000/45000 (44%)]	Loss: nan
10:30:41: Train Epoch: 84 [20000/45000 (44%)]	Loss: nan
10:30:41: Train Epoch: 84 [20000/45000 (44%)]	Loss: nan
10:30:46: Train Epoch: 84 [30000/45000 (67%)]	Loss: nan
10:30:46: Train Epoch: 84 [30000/45000 (67%)]	Loss: nan
10:30:46: Train Epoch: 84 [30000/45000 (67%)]	Loss: nan
10:30:50: Train Epoch: 84 [40000/45000 (89%)]	Loss: nan
10:30:50: Train Epoch: 84 [40000/45000 (89%)]	Loss: nan
10:30:50: Train Epoch: 84 [40000/45000 (89%)]	Loss: nan
10:30:54: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:30:54: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:30:54: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:30:54: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.18 seconds.

10:30:54: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.18 seconds.

10:30:54: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.18 seconds.

10:30:55: Train Epoch: 85 [0/45000 (0%)]	Loss: nan
10:30:55: Train Epoch: 85 [0/45000 (0%)]	Loss: nan
10:30:55: Train Epoch: 85 [0/45000 (0%)]	Loss: nan
10:31:00: Train Epoch: 85 [10000/45000 (22%)]	Loss: nan
10:31:00: Train Epoch: 85 [10000/45000 (22%)]	Loss: nan
10:31:00: Train Epoch: 85 [10000/45000 (22%)]	Loss: nan
10:31:04: Train Epoch: 85 [20000/45000 (44%)]	Loss: nan
10:31:04: Train Epoch: 85 [20000/45000 (44%)]	Loss: nan
10:31:04: Train Epoch: 85 [20000/45000 (44%)]	Loss: nan
10:31:09: Train Epoch: 85 [30000/45000 (67%)]	Loss: nan
10:31:09: Train Epoch: 85 [30000/45000 (67%)]	Loss: nan
10:31:09: Train Epoch: 85 [30000/45000 (67%)]	Loss: nan
10:31:14: Train Epoch: 85 [40000/45000 (89%)]	Loss: nan
10:31:14: Train Epoch: 85 [40000/45000 (89%)]	Loss: nan
10:31:14: Train Epoch: 85 [40000/45000 (89%)]	Loss: nan
10:31:17: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:31:17: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:31:17: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:31:18: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.55 seconds.

10:31:18: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.55 seconds.

10:31:18: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.55 seconds.

10:31:18: Train Epoch: 86 [0/45000 (0%)]	Loss: nan
10:31:18: Train Epoch: 86 [0/45000 (0%)]	Loss: nan
10:31:18: Train Epoch: 86 [0/45000 (0%)]	Loss: nan
10:31:23: Train Epoch: 86 [10000/45000 (22%)]	Loss: nan
10:31:23: Train Epoch: 86 [10000/45000 (22%)]	Loss: nan
10:31:23: Train Epoch: 86 [10000/45000 (22%)]	Loss: nan
10:31:27: Train Epoch: 86 [20000/45000 (44%)]	Loss: nan
10:31:27: Train Epoch: 86 [20000/45000 (44%)]	Loss: nan
10:31:27: Train Epoch: 86 [20000/45000 (44%)]	Loss: nan
10:31:31: Train Epoch: 86 [30000/45000 (67%)]	Loss: nan
10:31:31: Train Epoch: 86 [30000/45000 (67%)]	Loss: nan
10:31:31: Train Epoch: 86 [30000/45000 (67%)]	Loss: nan
10:31:36: Train Epoch: 86 [40000/45000 (89%)]	Loss: nan
10:31:36: Train Epoch: 86 [40000/45000 (89%)]	Loss: nan
10:31:36: Train Epoch: 86 [40000/45000 (89%)]	Loss: nan
10:31:40: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:31:40: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:31:40: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:31:40: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.14 seconds.

10:31:40: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.14 seconds.

10:31:40: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.14 seconds.

10:31:40: Train Epoch: 87 [0/45000 (0%)]	Loss: nan
10:31:40: Train Epoch: 87 [0/45000 (0%)]	Loss: nan
10:31:40: Train Epoch: 87 [0/45000 (0%)]	Loss: nan
10:31:45: Train Epoch: 87 [10000/45000 (22%)]	Loss: nan
10:31:45: Train Epoch: 87 [10000/45000 (22%)]	Loss: nan
10:31:45: Train Epoch: 87 [10000/45000 (22%)]	Loss: nan
10:31:49: Train Epoch: 87 [20000/45000 (44%)]	Loss: nan
10:31:49: Train Epoch: 87 [20000/45000 (44%)]	Loss: nan
10:31:49: Train Epoch: 87 [20000/45000 (44%)]	Loss: nan
10:31:54: Train Epoch: 87 [30000/45000 (67%)]	Loss: nan
10:31:54: Train Epoch: 87 [30000/45000 (67%)]	Loss: nan
10:31:54: Train Epoch: 87 [30000/45000 (67%)]	Loss: nan
10:31:59: Train Epoch: 87 [40000/45000 (89%)]	Loss: nan
10:31:59: Train Epoch: 87 [40000/45000 (89%)]	Loss: nan
10:31:59: Train Epoch: 87 [40000/45000 (89%)]	Loss: nan
10:32:03: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:32:03: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:32:03: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:32:03: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.98 seconds.

10:32:03: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.98 seconds.

10:32:03: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.98 seconds.

10:32:03: Train Epoch: 88 [0/45000 (0%)]	Loss: nan
10:32:03: Train Epoch: 88 [0/45000 (0%)]	Loss: nan
10:32:03: Train Epoch: 88 [0/45000 (0%)]	Loss: nan
10:32:08: Train Epoch: 88 [10000/45000 (22%)]	Loss: nan
10:32:08: Train Epoch: 88 [10000/45000 (22%)]	Loss: nan
10:32:08: Train Epoch: 88 [10000/45000 (22%)]	Loss: nan
10:32:12: Train Epoch: 88 [20000/45000 (44%)]	Loss: nan
10:32:12: Train Epoch: 88 [20000/45000 (44%)]	Loss: nan
10:32:12: Train Epoch: 88 [20000/45000 (44%)]	Loss: nan
10:32:17: Train Epoch: 88 [30000/45000 (67%)]	Loss: nan
10:32:17: Train Epoch: 88 [30000/45000 (67%)]	Loss: nan
10:32:17: Train Epoch: 88 [30000/45000 (67%)]	Loss: nan
10:32:21: Train Epoch: 88 [40000/45000 (89%)]	Loss: nan
10:32:21: Train Epoch: 88 [40000/45000 (89%)]	Loss: nan
10:32:21: Train Epoch: 88 [40000/45000 (89%)]	Loss: nan
10:32:25: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:32:25: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:32:25: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:32:25: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.39 seconds.

10:32:25: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.39 seconds.

10:32:25: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.39 seconds.

10:32:26: Train Epoch: 89 [0/45000 (0%)]	Loss: nan
10:32:26: Train Epoch: 89 [0/45000 (0%)]	Loss: nan
10:32:26: Train Epoch: 89 [0/45000 (0%)]	Loss: nan
10:32:30: Train Epoch: 89 [10000/45000 (22%)]	Loss: nan
10:32:30: Train Epoch: 89 [10000/45000 (22%)]	Loss: nan
10:32:30: Train Epoch: 89 [10000/45000 (22%)]	Loss: nan
10:32:35: Train Epoch: 89 [20000/45000 (44%)]	Loss: nan
10:32:35: Train Epoch: 89 [20000/45000 (44%)]	Loss: nan
10:32:35: Train Epoch: 89 [20000/45000 (44%)]	Loss: nan
10:32:39: Train Epoch: 89 [30000/45000 (67%)]	Loss: nan
10:32:39: Train Epoch: 89 [30000/45000 (67%)]	Loss: nan
10:32:39: Train Epoch: 89 [30000/45000 (67%)]	Loss: nan
10:32:43: Train Epoch: 89 [40000/45000 (89%)]	Loss: nan
10:32:43: Train Epoch: 89 [40000/45000 (89%)]	Loss: nan
10:32:43: Train Epoch: 89 [40000/45000 (89%)]	Loss: nan
10:32:47: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:32:47: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:32:47: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:32:47: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.18 seconds.

10:32:47: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.18 seconds.

10:32:47: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.18 seconds.

10:32:48: Train Epoch: 90 [0/45000 (0%)]	Loss: nan
10:32:48: Train Epoch: 90 [0/45000 (0%)]	Loss: nan
10:32:48: Train Epoch: 90 [0/45000 (0%)]	Loss: nan
10:32:52: Train Epoch: 90 [10000/45000 (22%)]	Loss: nan
10:32:52: Train Epoch: 90 [10000/45000 (22%)]	Loss: nan
10:32:52: Train Epoch: 90 [10000/45000 (22%)]	Loss: nan
10:32:57: Train Epoch: 90 [20000/45000 (44%)]	Loss: nan
10:32:57: Train Epoch: 90 [20000/45000 (44%)]	Loss: nan
10:32:57: Train Epoch: 90 [20000/45000 (44%)]	Loss: nan
10:33:02: Train Epoch: 90 [30000/45000 (67%)]	Loss: nan
10:33:02: Train Epoch: 90 [30000/45000 (67%)]	Loss: nan
10:33:02: Train Epoch: 90 [30000/45000 (67%)]	Loss: nan
10:33:06: Train Epoch: 90 [40000/45000 (89%)]	Loss: nan
10:33:06: Train Epoch: 90 [40000/45000 (89%)]	Loss: nan
10:33:06: Train Epoch: 90 [40000/45000 (89%)]	Loss: nan
10:33:10: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:33:10: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:33:10: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:33:10: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.99 seconds.

10:33:10: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.99 seconds.

10:33:10: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.99 seconds.

10:33:11: Train Epoch: 91 [0/45000 (0%)]	Loss: nan
10:33:11: Train Epoch: 91 [0/45000 (0%)]	Loss: nan
10:33:11: Train Epoch: 91 [0/45000 (0%)]	Loss: nan
10:33:16: Train Epoch: 91 [10000/45000 (22%)]	Loss: nan
10:33:16: Train Epoch: 91 [10000/45000 (22%)]	Loss: nan
10:33:16: Train Epoch: 91 [10000/45000 (22%)]	Loss: nan
10:33:20: Train Epoch: 91 [20000/45000 (44%)]	Loss: nan
10:33:20: Train Epoch: 91 [20000/45000 (44%)]	Loss: nan
10:33:20: Train Epoch: 91 [20000/45000 (44%)]	Loss: nan
10:33:25: Train Epoch: 91 [30000/45000 (67%)]	Loss: nan
10:33:25: Train Epoch: 91 [30000/45000 (67%)]	Loss: nan
10:33:25: Train Epoch: 91 [30000/45000 (67%)]	Loss: nan
10:33:29: Train Epoch: 91 [40000/45000 (89%)]	Loss: nan
10:33:29: Train Epoch: 91 [40000/45000 (89%)]	Loss: nan
10:33:29: Train Epoch: 91 [40000/45000 (89%)]	Loss: nan
10:33:33: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:33:33: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:33:33: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:33:33: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.59 seconds.

10:33:33: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.59 seconds.

10:33:33: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.59 seconds.

10:33:33: Train Epoch: 92 [0/45000 (0%)]	Loss: nan
10:33:33: Train Epoch: 92 [0/45000 (0%)]	Loss: nan
10:33:33: Train Epoch: 92 [0/45000 (0%)]	Loss: nan
10:33:38: Train Epoch: 92 [10000/45000 (22%)]	Loss: nan
10:33:38: Train Epoch: 92 [10000/45000 (22%)]	Loss: nan
10:33:38: Train Epoch: 92 [10000/45000 (22%)]	Loss: nan
10:33:43: Train Epoch: 92 [20000/45000 (44%)]	Loss: nan
10:33:43: Train Epoch: 92 [20000/45000 (44%)]	Loss: nan
10:33:43: Train Epoch: 92 [20000/45000 (44%)]	Loss: nan
10:33:47: Train Epoch: 92 [30000/45000 (67%)]	Loss: nan
10:33:47: Train Epoch: 92 [30000/45000 (67%)]	Loss: nan
10:33:47: Train Epoch: 92 [30000/45000 (67%)]	Loss: nan
10:33:51: Train Epoch: 92 [40000/45000 (89%)]	Loss: nan
10:33:51: Train Epoch: 92 [40000/45000 (89%)]	Loss: nan
10:33:51: Train Epoch: 92 [40000/45000 (89%)]	Loss: nan
10:33:55: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:33:55: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:33:55: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:33:55: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.63 seconds.

10:33:55: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.63 seconds.

10:33:55: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.63 seconds.

10:33:56: Train Epoch: 93 [0/45000 (0%)]	Loss: nan
10:33:56: Train Epoch: 93 [0/45000 (0%)]	Loss: nan
10:33:56: Train Epoch: 93 [0/45000 (0%)]	Loss: nan
10:34:01: Train Epoch: 93 [10000/45000 (22%)]	Loss: nan
10:34:01: Train Epoch: 93 [10000/45000 (22%)]	Loss: nan
10:34:01: Train Epoch: 93 [10000/45000 (22%)]	Loss: nan
10:34:05: Train Epoch: 93 [20000/45000 (44%)]	Loss: nan
10:34:05: Train Epoch: 93 [20000/45000 (44%)]	Loss: nan
10:34:05: Train Epoch: 93 [20000/45000 (44%)]	Loss: nan
10:34:10: Train Epoch: 93 [30000/45000 (67%)]	Loss: nan
10:34:10: Train Epoch: 93 [30000/45000 (67%)]	Loss: nan
10:34:10: Train Epoch: 93 [30000/45000 (67%)]	Loss: nan
10:34:15: Train Epoch: 93 [40000/45000 (89%)]	Loss: nan
10:34:15: Train Epoch: 93 [40000/45000 (89%)]	Loss: nan
10:34:15: Train Epoch: 93 [40000/45000 (89%)]	Loss: nan
10:34:18: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:34:18: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:34:18: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:34:18: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.92 seconds.

10:34:18: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.92 seconds.

10:34:18: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.92 seconds.

10:34:19: Train Epoch: 94 [0/45000 (0%)]	Loss: nan
10:34:19: Train Epoch: 94 [0/45000 (0%)]	Loss: nan
10:34:19: Train Epoch: 94 [0/45000 (0%)]	Loss: nan
10:34:23: Train Epoch: 94 [10000/45000 (22%)]	Loss: nan
10:34:23: Train Epoch: 94 [10000/45000 (22%)]	Loss: nan
10:34:23: Train Epoch: 94 [10000/45000 (22%)]	Loss: nan
10:34:28: Train Epoch: 94 [20000/45000 (44%)]	Loss: nan
10:34:28: Train Epoch: 94 [20000/45000 (44%)]	Loss: nan
10:34:28: Train Epoch: 94 [20000/45000 (44%)]	Loss: nan
10:34:32: Train Epoch: 94 [30000/45000 (67%)]	Loss: nan
10:34:32: Train Epoch: 94 [30000/45000 (67%)]	Loss: nan
10:34:32: Train Epoch: 94 [30000/45000 (67%)]	Loss: nan
10:34:37: Train Epoch: 94 [40000/45000 (89%)]	Loss: nan
10:34:37: Train Epoch: 94 [40000/45000 (89%)]	Loss: nan
10:34:37: Train Epoch: 94 [40000/45000 (89%)]	Loss: nan
10:34:40: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:34:40: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:34:40: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:34:40: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.01 seconds.

10:34:40: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.01 seconds.

10:34:40: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.01 seconds.

10:34:41: Train Epoch: 95 [0/45000 (0%)]	Loss: nan
10:34:41: Train Epoch: 95 [0/45000 (0%)]	Loss: nan
10:34:41: Train Epoch: 95 [0/45000 (0%)]	Loss: nan
10:34:46: Train Epoch: 95 [10000/45000 (22%)]	Loss: nan
10:34:46: Train Epoch: 95 [10000/45000 (22%)]	Loss: nan
10:34:46: Train Epoch: 95 [10000/45000 (22%)]	Loss: nan
10:34:50: Train Epoch: 95 [20000/45000 (44%)]	Loss: nan
10:34:50: Train Epoch: 95 [20000/45000 (44%)]	Loss: nan
10:34:50: Train Epoch: 95 [20000/45000 (44%)]	Loss: nan
10:34:54: Train Epoch: 95 [30000/45000 (67%)]	Loss: nan
10:34:54: Train Epoch: 95 [30000/45000 (67%)]	Loss: nan
10:34:54: Train Epoch: 95 [30000/45000 (67%)]	Loss: nan
10:34:59: Train Epoch: 95 [40000/45000 (89%)]	Loss: nan
10:34:59: Train Epoch: 95 [40000/45000 (89%)]	Loss: nan
10:34:59: Train Epoch: 95 [40000/45000 (89%)]	Loss: nan
10:35:03: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:35:03: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:35:03: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:35:03: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.90 seconds.

10:35:03: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.90 seconds.

10:35:03: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.90 seconds.

10:35:04: Train Epoch: 96 [0/45000 (0%)]	Loss: nan
10:35:04: Train Epoch: 96 [0/45000 (0%)]	Loss: nan
10:35:04: Train Epoch: 96 [0/45000 (0%)]	Loss: nan
10:35:08: Train Epoch: 96 [10000/45000 (22%)]	Loss: nan
10:35:08: Train Epoch: 96 [10000/45000 (22%)]	Loss: nan
10:35:08: Train Epoch: 96 [10000/45000 (22%)]	Loss: nan
10:35:13: Train Epoch: 96 [20000/45000 (44%)]	Loss: nan
10:35:13: Train Epoch: 96 [20000/45000 (44%)]	Loss: nan
10:35:13: Train Epoch: 96 [20000/45000 (44%)]	Loss: nan
10:35:18: Train Epoch: 96 [30000/45000 (67%)]	Loss: nan
10:35:18: Train Epoch: 96 [30000/45000 (67%)]	Loss: nan
10:35:18: Train Epoch: 96 [30000/45000 (67%)]	Loss: nan
10:35:22: Train Epoch: 96 [40000/45000 (89%)]	Loss: nan
10:35:22: Train Epoch: 96 [40000/45000 (89%)]	Loss: nan
10:35:22: Train Epoch: 96 [40000/45000 (89%)]	Loss: nan
10:35:26: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:35:26: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:35:26: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:35:26: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.51 seconds.

10:35:26: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.51 seconds.

10:35:26: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.51 seconds.

10:35:26: Train Epoch: 97 [0/45000 (0%)]	Loss: nan
10:35:26: Train Epoch: 97 [0/45000 (0%)]	Loss: nan
10:35:26: Train Epoch: 97 [0/45000 (0%)]	Loss: nan
10:35:31: Train Epoch: 97 [10000/45000 (22%)]	Loss: nan
10:35:31: Train Epoch: 97 [10000/45000 (22%)]	Loss: nan
10:35:31: Train Epoch: 97 [10000/45000 (22%)]	Loss: nan
10:35:35: Train Epoch: 97 [20000/45000 (44%)]	Loss: nan
10:35:35: Train Epoch: 97 [20000/45000 (44%)]	Loss: nan
10:35:35: Train Epoch: 97 [20000/45000 (44%)]	Loss: nan
10:35:40: Train Epoch: 97 [30000/45000 (67%)]	Loss: nan
10:35:40: Train Epoch: 97 [30000/45000 (67%)]	Loss: nan
10:35:40: Train Epoch: 97 [30000/45000 (67%)]	Loss: nan
10:35:44: Train Epoch: 97 [40000/45000 (89%)]	Loss: nan
10:35:44: Train Epoch: 97 [40000/45000 (89%)]	Loss: nan
10:35:44: Train Epoch: 97 [40000/45000 (89%)]	Loss: nan
10:35:48: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:35:48: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:35:48: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:35:48: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.34 seconds.

10:35:48: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.34 seconds.

10:35:48: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.34 seconds.

10:35:49: Train Epoch: 98 [0/45000 (0%)]	Loss: nan
10:35:49: Train Epoch: 98 [0/45000 (0%)]	Loss: nan
10:35:49: Train Epoch: 98 [0/45000 (0%)]	Loss: nan
10:35:53: Train Epoch: 98 [10000/45000 (22%)]	Loss: nan
10:35:53: Train Epoch: 98 [10000/45000 (22%)]	Loss: nan
10:35:53: Train Epoch: 98 [10000/45000 (22%)]	Loss: nan
10:35:58: Train Epoch: 98 [20000/45000 (44%)]	Loss: nan
10:35:58: Train Epoch: 98 [20000/45000 (44%)]	Loss: nan
10:35:58: Train Epoch: 98 [20000/45000 (44%)]	Loss: nan
10:36:03: Train Epoch: 98 [30000/45000 (67%)]	Loss: nan
10:36:03: Train Epoch: 98 [30000/45000 (67%)]	Loss: nan
10:36:03: Train Epoch: 98 [30000/45000 (67%)]	Loss: nan
10:36:07: Train Epoch: 98 [40000/45000 (89%)]	Loss: nan
10:36:07: Train Epoch: 98 [40000/45000 (89%)]	Loss: nan
10:36:07: Train Epoch: 98 [40000/45000 (89%)]	Loss: nan
10:36:11: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:36:11: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:36:11: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:36:11: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.92 seconds.

10:36:11: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.92 seconds.

10:36:11: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.92 seconds.

10:36:11: Train Epoch: 99 [0/45000 (0%)]	Loss: nan
10:36:11: Train Epoch: 99 [0/45000 (0%)]	Loss: nan
10:36:11: Train Epoch: 99 [0/45000 (0%)]	Loss: nan
10:36:17: Train Epoch: 99 [10000/45000 (22%)]	Loss: nan
10:36:17: Train Epoch: 99 [10000/45000 (22%)]	Loss: nan
10:36:17: Train Epoch: 99 [10000/45000 (22%)]	Loss: nan
10:36:21: Train Epoch: 99 [20000/45000 (44%)]	Loss: nan
10:36:21: Train Epoch: 99 [20000/45000 (44%)]	Loss: nan
10:36:21: Train Epoch: 99 [20000/45000 (44%)]	Loss: nan
10:36:26: Train Epoch: 99 [30000/45000 (67%)]	Loss: nan
10:36:26: Train Epoch: 99 [30000/45000 (67%)]	Loss: nan
10:36:26: Train Epoch: 99 [30000/45000 (67%)]	Loss: nan
10:36:30: Train Epoch: 99 [40000/45000 (89%)]	Loss: nan
10:36:30: Train Epoch: 99 [40000/45000 (89%)]	Loss: nan
10:36:30: Train Epoch: 99 [40000/45000 (89%)]	Loss: nan
10:36:34: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:36:34: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:36:34: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:36:34: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.64 seconds.

10:36:34: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.64 seconds.

10:36:34: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.64 seconds.

10:36:34: Train Epoch: 100 [0/45000 (0%)]	Loss: nan
10:36:34: Train Epoch: 100 [0/45000 (0%)]	Loss: nan
10:36:34: Train Epoch: 100 [0/45000 (0%)]	Loss: nan
10:36:39: Train Epoch: 100 [10000/45000 (22%)]	Loss: nan
10:36:39: Train Epoch: 100 [10000/45000 (22%)]	Loss: nan
10:36:39: Train Epoch: 100 [10000/45000 (22%)]	Loss: nan
10:36:43: Train Epoch: 100 [20000/45000 (44%)]	Loss: nan
10:36:43: Train Epoch: 100 [20000/45000 (44%)]	Loss: nan
10:36:43: Train Epoch: 100 [20000/45000 (44%)]	Loss: nan
10:36:48: Train Epoch: 100 [30000/45000 (67%)]	Loss: nan
10:36:48: Train Epoch: 100 [30000/45000 (67%)]	Loss: nan
10:36:48: Train Epoch: 100 [30000/45000 (67%)]	Loss: nan
10:36:52: Train Epoch: 100 [40000/45000 (89%)]	Loss: nan
10:36:52: Train Epoch: 100 [40000/45000 (89%)]	Loss: nan
10:36:52: Train Epoch: 100 [40000/45000 (89%)]	Loss: nan
10:36:57: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:36:57: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:36:57: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

10:36:57: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.98 seconds.

10:36:57: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.98 seconds.

10:36:57: Current learning rate: 0.010000000000000002. Time taken for epoch: 22.98 seconds.

10:36:59: 
Test evaluation: Average loss: nan, Accuracy: 1000/10000 (10.000%)

10:36:59: 
Test evaluation: Average loss: nan, Accuracy: 1000/10000 (10.000%)

10:36:59: 
Test evaluation: Average loss: nan, Accuracy: 1000/10000 (10.000%)

10:36:59: 
Iteration end: 1/1

10:36:59: 
Iteration end: 1/1

10:36:59: 
Iteration end: 1/1

20:45:25: Namespace(batch_size=100, bench=True, data='cifar', decay_frequency=25000, decay_schedule='cosine', dense=True, density=1.0, epochs=100, fp16=False, growth='momentum', iters=1, l1=0.0, l2=0.0005, log_interval=100, lr=0.1, max_threads=10, model='MobileNet', momentum=0.9, no_cuda=False, optimizer='sgd', prune='magnitude', prune_rate=0.7, redistribution='momentum', resume=None, save_features=False, save_model='./models/model.pt', seed=17, start_epoch=1, test_batch_size=100, valid_split=0.1, verbose=True)
20:45:25: 


20:45:25: ================================================================================
20:45:25: 
Iteration start: 1/1

20:45:36: MobileNet(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (dw2_1): DepthWiseBlock(
    (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
    (bn_dw): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw2_2): DepthWiseBlock(
    (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
    (bn_dw): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw3_1): DepthWiseBlock(
    (conv_dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
    (bn_dw): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw3_2): DepthWiseBlock(
    (conv_dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
    (bn_dw): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw4_1): DepthWiseBlock(
    (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
    (bn_dw): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw4_2): DepthWiseBlock(
    (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
    (bn_dw): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_1): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_2): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_3): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_4): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_5): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_6): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw6): DepthWiseBlock(
    (conv_dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)
    (bn_dw): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (fc): Linear(in_features=1024, out_features=100, bias=True)
)
20:45:36: ============================================================
20:45:36: MobileNet
20:45:36: ============================================================
20:45:36: ============================================================
20:45:36: Prune mode: magnitude
20:45:36: Growth mode: momentum
20:45:36: Redistribution mode: momentum
20:45:36: ============================================================
20:45:38: Train Epoch: 1 [0/45000 (0%)]	Loss: 4.641055
20:45:43: Train Epoch: 1 [10000/45000 (22%)]	Loss: 2.070999
20:45:48: Train Epoch: 1 [20000/45000 (44%)]	Loss: 2.191387
20:45:53: Train Epoch: 1 [30000/45000 (67%)]	Loss: 1.572804
20:45:58: Train Epoch: 1 [40000/45000 (89%)]	Loss: 1.645447
20:46:02: 
Evaluation: Average loss: -7.4973, Accuracy: 1916/5000 (38.320%)

20:46:02: Current learning rate: 0.1. Time taken for epoch: 25.53 seconds.

20:46:03: Train Epoch: 2 [0/45000 (0%)]	Loss: 1.539217
20:46:08: Train Epoch: 2 [10000/45000 (22%)]	Loss: 1.506656
20:46:12: Train Epoch: 2 [20000/45000 (44%)]	Loss: 1.660257
20:46:17: Train Epoch: 2 [30000/45000 (67%)]	Loss: 1.391268
20:46:22: Train Epoch: 2 [40000/45000 (89%)]	Loss: 1.605617
20:46:26: 
Evaluation: Average loss: -7.5427, Accuracy: 2206/5000 (44.120%)

20:46:26: Current learning rate: 0.1. Time taken for epoch: 23.75 seconds.

20:46:26: Train Epoch: 3 [0/45000 (0%)]	Loss: 1.405193
20:46:31: Train Epoch: 3 [10000/45000 (22%)]	Loss: 1.692926
20:46:36: Train Epoch: 3 [20000/45000 (44%)]	Loss: 1.380153
20:46:41: Train Epoch: 3 [30000/45000 (67%)]	Loss: 1.360908
20:46:46: Train Epoch: 3 [40000/45000 (89%)]	Loss: 1.352115
20:46:50: 
Evaluation: Average loss: -8.2287, Accuracy: 2511/5000 (50.220%)

20:46:50: Current learning rate: 0.1. Time taken for epoch: 24.62 seconds.

20:46:51: Train Epoch: 4 [0/45000 (0%)]	Loss: 1.296323
20:46:56: Train Epoch: 4 [10000/45000 (22%)]	Loss: 1.222319
20:47:01: Train Epoch: 4 [20000/45000 (44%)]	Loss: 1.121193
20:47:06: Train Epoch: 4 [30000/45000 (67%)]	Loss: 1.495911
20:47:11: Train Epoch: 4 [40000/45000 (89%)]	Loss: 1.157913
20:47:15: 
Evaluation: Average loss: -8.1894, Accuracy: 2618/5000 (52.360%)

20:47:15: Current learning rate: 0.1. Time taken for epoch: 24.42 seconds.

20:47:15: Train Epoch: 5 [0/45000 (0%)]	Loss: 1.220667
20:47:20: Train Epoch: 5 [10000/45000 (22%)]	Loss: 1.145001
20:47:25: Train Epoch: 5 [20000/45000 (44%)]	Loss: 1.251591
20:47:30: Train Epoch: 5 [30000/45000 (67%)]	Loss: 1.225516
20:47:34: Train Epoch: 5 [40000/45000 (89%)]	Loss: 1.051940
20:47:38: 
Evaluation: Average loss: -8.4317, Accuracy: 2835/5000 (56.700%)

20:47:38: Current learning rate: 0.1. Time taken for epoch: 23.54 seconds.

20:47:39: Train Epoch: 6 [0/45000 (0%)]	Loss: 1.173297
20:47:44: Train Epoch: 6 [10000/45000 (22%)]	Loss: 1.195373
20:47:49: Train Epoch: 6 [20000/45000 (44%)]	Loss: 1.166446
20:47:54: Train Epoch: 6 [30000/45000 (67%)]	Loss: 1.066273
20:47:59: Train Epoch: 6 [40000/45000 (89%)]	Loss: 1.092280
20:48:03: 
Evaluation: Average loss: -8.5108, Accuracy: 2941/5000 (58.820%)

20:48:03: Current learning rate: 0.1. Time taken for epoch: 24.75 seconds.

20:48:04: Train Epoch: 7 [0/45000 (0%)]	Loss: 1.209516
20:48:09: Train Epoch: 7 [10000/45000 (22%)]	Loss: 1.300370
20:48:14: Train Epoch: 7 [20000/45000 (44%)]	Loss: 1.019166
20:48:18: Train Epoch: 7 [30000/45000 (67%)]	Loss: 1.229778
20:48:23: Train Epoch: 7 [40000/45000 (89%)]	Loss: 0.902035
20:48:27: 
Evaluation: Average loss: -8.4379, Accuracy: 2861/5000 (57.220%)

20:48:27: Current learning rate: 0.1. Time taken for epoch: 24.39 seconds.

20:48:28: Train Epoch: 8 [0/45000 (0%)]	Loss: 1.122509
20:48:33: Train Epoch: 8 [10000/45000 (22%)]	Loss: 1.098168
20:48:38: Train Epoch: 8 [20000/45000 (44%)]	Loss: 1.152307
20:48:43: Train Epoch: 8 [30000/45000 (67%)]	Loss: 0.939621
20:48:48: Train Epoch: 8 [40000/45000 (89%)]	Loss: 0.934100
20:48:52: 
Evaluation: Average loss: -8.7891, Accuracy: 3246/5000 (64.920%)

20:48:52: Current learning rate: 0.1. Time taken for epoch: 24.39 seconds.

20:48:52: Train Epoch: 9 [0/45000 (0%)]	Loss: 1.052353
20:48:58: Train Epoch: 9 [10000/45000 (22%)]	Loss: 0.864117
20:49:03: Train Epoch: 9 [20000/45000 (44%)]	Loss: 0.877453
20:49:08: Train Epoch: 9 [30000/45000 (67%)]	Loss: 0.865385
20:49:12: Train Epoch: 9 [40000/45000 (89%)]	Loss: 1.255493
20:49:16: 
Evaluation: Average loss: -8.9968, Accuracy: 3197/5000 (63.940%)

20:49:16: Current learning rate: 0.1. Time taken for epoch: 24.52 seconds.

20:49:17: Train Epoch: 10 [0/45000 (0%)]	Loss: 1.041793
20:49:22: Train Epoch: 10 [10000/45000 (22%)]	Loss: 1.026022
20:49:27: Train Epoch: 10 [20000/45000 (44%)]	Loss: 0.984909
20:49:31: Train Epoch: 10 [30000/45000 (67%)]	Loss: 1.070504
20:49:36: Train Epoch: 10 [40000/45000 (89%)]	Loss: 1.037909
20:49:40: 
Evaluation: Average loss: -9.0468, Accuracy: 2935/5000 (58.700%)

20:49:40: Current learning rate: 0.1. Time taken for epoch: 24.10 seconds.

20:49:41: Train Epoch: 11 [0/45000 (0%)]	Loss: 0.903475
20:49:46: Train Epoch: 11 [10000/45000 (22%)]	Loss: 0.917096
20:49:51: Train Epoch: 11 [20000/45000 (44%)]	Loss: 1.066228
20:49:56: Train Epoch: 11 [30000/45000 (67%)]	Loss: 1.015116
20:50:01: Train Epoch: 11 [40000/45000 (89%)]	Loss: 0.892001
20:50:05: 
Evaluation: Average loss: -8.7921, Accuracy: 3232/5000 (64.640%)

20:50:05: Current learning rate: 0.1. Time taken for epoch: 24.87 seconds.

20:50:06: Train Epoch: 12 [0/45000 (0%)]	Loss: 0.896391
20:50:11: Train Epoch: 12 [10000/45000 (22%)]	Loss: 1.040409
20:50:16: Train Epoch: 12 [20000/45000 (44%)]	Loss: 0.872414
20:50:21: Train Epoch: 12 [30000/45000 (67%)]	Loss: 0.968072
20:50:26: Train Epoch: 12 [40000/45000 (89%)]	Loss: 0.936087
20:50:30: 
Evaluation: Average loss: -9.1224, Accuracy: 3245/5000 (64.900%)

20:50:30: Current learning rate: 0.1. Time taken for epoch: 24.42 seconds.

20:50:30: Train Epoch: 13 [0/45000 (0%)]	Loss: 0.952147
20:50:36: Train Epoch: 13 [10000/45000 (22%)]	Loss: 1.037147
20:50:40: Train Epoch: 13 [20000/45000 (44%)]	Loss: 0.859045
20:50:45: Train Epoch: 13 [30000/45000 (67%)]	Loss: 0.726019
20:50:50: Train Epoch: 13 [40000/45000 (89%)]	Loss: 0.897180
20:50:54: 
Evaluation: Average loss: -8.5863, Accuracy: 3078/5000 (61.560%)

20:50:54: Current learning rate: 0.1. Time taken for epoch: 24.48 seconds.

20:50:55: Train Epoch: 14 [0/45000 (0%)]	Loss: 1.077231
20:51:00: Train Epoch: 14 [10000/45000 (22%)]	Loss: 0.973345
20:51:05: Train Epoch: 14 [20000/45000 (44%)]	Loss: 0.934396
20:51:10: Train Epoch: 14 [30000/45000 (67%)]	Loss: 0.761494
20:51:15: Train Epoch: 14 [40000/45000 (89%)]	Loss: 0.946728
20:51:19: 
Evaluation: Average loss: -9.4048, Accuracy: 3311/5000 (66.220%)

20:51:19: Current learning rate: 0.1. Time taken for epoch: 25.06 seconds.

20:51:20: Train Epoch: 15 [0/45000 (0%)]	Loss: 0.837940
20:51:25: Train Epoch: 15 [10000/45000 (22%)]	Loss: 0.951340
20:51:30: Train Epoch: 15 [20000/45000 (44%)]	Loss: 0.859055
20:51:35: Train Epoch: 15 [30000/45000 (67%)]	Loss: 0.914733
20:51:40: Train Epoch: 15 [40000/45000 (89%)]	Loss: 0.820461
20:51:44: 
Evaluation: Average loss: -9.1695, Accuracy: 3138/5000 (62.760%)

20:51:44: Current learning rate: 0.1. Time taken for epoch: 24.39 seconds.

20:51:44: Train Epoch: 16 [0/45000 (0%)]	Loss: 0.949087
20:51:49: Train Epoch: 16 [10000/45000 (22%)]	Loss: 1.030977
20:51:54: Train Epoch: 16 [20000/45000 (44%)]	Loss: 1.101172
20:52:00: Train Epoch: 16 [30000/45000 (67%)]	Loss: 0.774793
20:52:05: Train Epoch: 16 [40000/45000 (89%)]	Loss: 1.014955
20:52:09: 
Evaluation: Average loss: -9.5841, Accuracy: 3270/5000 (65.400%)

20:52:09: Current learning rate: 0.1. Time taken for epoch: 25.16 seconds.

20:52:09: Train Epoch: 17 [0/45000 (0%)]	Loss: 0.911990
20:52:14: Train Epoch: 17 [10000/45000 (22%)]	Loss: 1.022918
20:52:19: Train Epoch: 17 [20000/45000 (44%)]	Loss: 0.746091
20:52:24: Train Epoch: 17 [30000/45000 (67%)]	Loss: 1.168708
20:52:29: Train Epoch: 17 [40000/45000 (89%)]	Loss: 0.960175
20:52:33: 
Evaluation: Average loss: -9.5038, Accuracy: 3340/5000 (66.800%)

20:52:33: Current learning rate: 0.1. Time taken for epoch: 24.16 seconds.

20:52:34: Train Epoch: 18 [0/45000 (0%)]	Loss: 0.910733
20:52:39: Train Epoch: 18 [10000/45000 (22%)]	Loss: 0.963349
20:52:43: Train Epoch: 18 [20000/45000 (44%)]	Loss: 0.970611
20:52:48: Train Epoch: 18 [30000/45000 (67%)]	Loss: 0.635672
20:52:53: Train Epoch: 18 [40000/45000 (89%)]	Loss: 0.885292
20:52:57: 
Evaluation: Average loss: -9.2881, Accuracy: 3197/5000 (63.940%)

20:52:57: Current learning rate: 0.1. Time taken for epoch: 24.01 seconds.

20:52:58: Train Epoch: 19 [0/45000 (0%)]	Loss: 0.665436
20:53:03: Train Epoch: 19 [10000/45000 (22%)]	Loss: 0.843730
20:53:08: Train Epoch: 19 [20000/45000 (44%)]	Loss: 1.012961
20:53:13: Train Epoch: 19 [30000/45000 (67%)]	Loss: 1.097223
20:53:18: Train Epoch: 19 [40000/45000 (89%)]	Loss: 0.896979
20:53:22: 
Evaluation: Average loss: -9.6329, Accuracy: 3267/5000 (65.340%)

20:53:22: Current learning rate: 0.1. Time taken for epoch: 24.59 seconds.

20:53:22: Train Epoch: 20 [0/45000 (0%)]	Loss: 0.907050
20:53:27: Train Epoch: 20 [10000/45000 (22%)]	Loss: 0.844941
20:53:32: Train Epoch: 20 [20000/45000 (44%)]	Loss: 0.865420
20:53:37: Train Epoch: 20 [30000/45000 (67%)]	Loss: 0.913547
20:53:42: Train Epoch: 20 [40000/45000 (89%)]	Loss: 0.710975
20:53:46: 
Evaluation: Average loss: -10.0408, Accuracy: 3258/5000 (65.160%)

20:53:46: Current learning rate: 0.1. Time taken for epoch: 24.42 seconds.

20:53:47: Train Epoch: 21 [0/45000 (0%)]	Loss: 0.837515
20:53:52: Train Epoch: 21 [10000/45000 (22%)]	Loss: 0.916667
20:53:57: Train Epoch: 21 [20000/45000 (44%)]	Loss: 0.918610
20:54:02: Train Epoch: 21 [30000/45000 (67%)]	Loss: 0.798182
20:54:07: Train Epoch: 21 [40000/45000 (89%)]	Loss: 0.962809
20:54:10: 
Evaluation: Average loss: -9.0330, Accuracy: 3228/5000 (64.560%)

20:54:11: Current learning rate: 0.1. Time taken for epoch: 24.54 seconds.

20:54:11: Train Epoch: 22 [0/45000 (0%)]	Loss: 0.833723
20:54:16: Train Epoch: 22 [10000/45000 (22%)]	Loss: 0.836489
20:54:21: Train Epoch: 22 [20000/45000 (44%)]	Loss: 1.052675
20:54:26: Train Epoch: 22 [30000/45000 (67%)]	Loss: 0.899131
20:54:30: Train Epoch: 22 [40000/45000 (89%)]	Loss: 0.845989
20:54:34: 
Evaluation: Average loss: -9.4607, Accuracy: 3328/5000 (66.560%)

20:54:34: Current learning rate: 0.1. Time taken for epoch: 23.87 seconds.

20:54:35: Train Epoch: 23 [0/45000 (0%)]	Loss: 0.881526
20:54:40: Train Epoch: 23 [10000/45000 (22%)]	Loss: 0.890483
20:54:45: Train Epoch: 23 [20000/45000 (44%)]	Loss: 0.833632
20:54:50: Train Epoch: 23 [30000/45000 (67%)]	Loss: 0.922766
20:54:54: Train Epoch: 23 [40000/45000 (89%)]	Loss: 0.823278
20:54:58: 
Evaluation: Average loss: -9.2678, Accuracy: 3141/5000 (62.820%)

20:54:58: Current learning rate: 0.1. Time taken for epoch: 23.93 seconds.

20:54:59: Train Epoch: 24 [0/45000 (0%)]	Loss: 0.845918
20:55:04: Train Epoch: 24 [10000/45000 (22%)]	Loss: 0.945922
20:55:09: Train Epoch: 24 [20000/45000 (44%)]	Loss: 0.831198
20:55:14: Train Epoch: 24 [30000/45000 (67%)]	Loss: 0.755331
20:55:19: Train Epoch: 24 [40000/45000 (89%)]	Loss: 0.951326
20:55:23: 
Evaluation: Average loss: -9.4712, Accuracy: 3447/5000 (68.940%)

20:55:23: Current learning rate: 0.1. Time taken for epoch: 24.36 seconds.

20:55:23: Train Epoch: 25 [0/45000 (0%)]	Loss: 0.792527
20:55:28: Train Epoch: 25 [10000/45000 (22%)]	Loss: 0.658052
20:55:33: Train Epoch: 25 [20000/45000 (44%)]	Loss: 0.789372
20:55:38: Train Epoch: 25 [30000/45000 (67%)]	Loss: 1.022682
20:55:43: Train Epoch: 25 [40000/45000 (89%)]	Loss: 0.875415
20:55:47: 
Evaluation: Average loss: -9.8164, Accuracy: 3017/5000 (60.340%)

20:55:47: Current learning rate: 0.1. Time taken for epoch: 24.12 seconds.

20:55:47: Train Epoch: 26 [0/45000 (0%)]	Loss: 0.738042
20:55:52: Train Epoch: 26 [10000/45000 (22%)]	Loss: 0.909108
20:55:57: Train Epoch: 26 [20000/45000 (44%)]	Loss: 0.734611
20:56:02: Train Epoch: 26 [30000/45000 (67%)]	Loss: 0.771405
20:56:07: Train Epoch: 26 [40000/45000 (89%)]	Loss: 0.839975
20:56:11: 
Evaluation: Average loss: -9.5607, Accuracy: 2669/5000 (53.380%)

20:56:11: Current learning rate: 0.1. Time taken for epoch: 24.23 seconds.

20:56:12: Train Epoch: 27 [0/45000 (0%)]	Loss: 0.917115
20:56:17: Train Epoch: 27 [10000/45000 (22%)]	Loss: 0.851246
20:56:21: Train Epoch: 27 [20000/45000 (44%)]	Loss: 0.709013
20:56:26: Train Epoch: 27 [30000/45000 (67%)]	Loss: 0.699592
20:56:31: Train Epoch: 27 [40000/45000 (89%)]	Loss: 0.905364
20:56:35: 
Evaluation: Average loss: -9.0212, Accuracy: 2722/5000 (54.440%)

20:56:35: Current learning rate: 0.1. Time taken for epoch: 23.81 seconds.

20:56:35: Train Epoch: 28 [0/45000 (0%)]	Loss: 0.787978
20:56:41: Train Epoch: 28 [10000/45000 (22%)]	Loss: 0.935257
20:56:45: Train Epoch: 28 [20000/45000 (44%)]	Loss: 0.883613
20:56:50: Train Epoch: 28 [30000/45000 (67%)]	Loss: 0.972525
20:56:55: Train Epoch: 28 [40000/45000 (89%)]	Loss: 0.794681
20:56:59: 
Evaluation: Average loss: -10.8921, Accuracy: 2750/5000 (55.000%)

20:56:59: Current learning rate: 0.1. Time taken for epoch: 24.18 seconds.

20:57:00: Train Epoch: 29 [0/45000 (0%)]	Loss: 0.833611
20:57:05: Train Epoch: 29 [10000/45000 (22%)]	Loss: 0.692330
20:57:10: Train Epoch: 29 [20000/45000 (44%)]	Loss: 0.827153
20:57:15: Train Epoch: 29 [30000/45000 (67%)]	Loss: 0.932952
20:57:19: Train Epoch: 29 [40000/45000 (89%)]	Loss: 0.940653
20:57:23: 
Evaluation: Average loss: -9.3758, Accuracy: 3263/5000 (65.260%)

20:57:23: Current learning rate: 0.1. Time taken for epoch: 24.34 seconds.

20:57:24: Train Epoch: 30 [0/45000 (0%)]	Loss: 0.937819
20:57:29: Train Epoch: 30 [10000/45000 (22%)]	Loss: 0.857799
20:57:34: Train Epoch: 30 [20000/45000 (44%)]	Loss: 1.163558
20:57:39: Train Epoch: 30 [30000/45000 (67%)]	Loss: 0.833850
20:57:43: Train Epoch: 30 [40000/45000 (89%)]	Loss: 0.885407
20:57:47: 
Evaluation: Average loss: -9.6063, Accuracy: 3470/5000 (69.400%)

20:57:47: Current learning rate: 0.1. Time taken for epoch: 24.07 seconds.

20:57:48: Train Epoch: 31 [0/45000 (0%)]	Loss: 0.871370
20:57:53: Train Epoch: 31 [10000/45000 (22%)]	Loss: 0.671322
20:57:58: Train Epoch: 31 [20000/45000 (44%)]	Loss: 1.013191
20:58:03: Train Epoch: 31 [30000/45000 (67%)]	Loss: 0.843251
20:58:08: Train Epoch: 31 [40000/45000 (89%)]	Loss: 0.784754
20:58:12: 
Evaluation: Average loss: -9.4486, Accuracy: 3294/5000 (65.880%)

20:58:12: Current learning rate: 0.1. Time taken for epoch: 24.29 seconds.

20:58:12: Train Epoch: 32 [0/45000 (0%)]	Loss: 0.599605
20:58:17: Train Epoch: 32 [10000/45000 (22%)]	Loss: 0.847491
20:58:22: Train Epoch: 32 [20000/45000 (44%)]	Loss: 0.777141
20:58:27: Train Epoch: 32 [30000/45000 (67%)]	Loss: 0.759699
20:58:31: Train Epoch: 32 [40000/45000 (89%)]	Loss: 0.827045
20:58:35: 
Evaluation: Average loss: -9.4856, Accuracy: 3271/5000 (65.420%)

20:58:36: Current learning rate: 0.1. Time taken for epoch: 23.78 seconds.

20:58:36: Train Epoch: 33 [0/45000 (0%)]	Loss: 0.873183
20:58:41: Train Epoch: 33 [10000/45000 (22%)]	Loss: 0.757277
20:58:46: Train Epoch: 33 [20000/45000 (44%)]	Loss: 1.135375
20:58:51: Train Epoch: 33 [30000/45000 (67%)]	Loss: 0.679234
20:58:56: Train Epoch: 33 [40000/45000 (89%)]	Loss: 1.003927
20:59:00: 
Evaluation: Average loss: -9.3968, Accuracy: 3241/5000 (64.820%)

20:59:00: Current learning rate: 0.1. Time taken for epoch: 24.42 seconds.

20:59:00: Train Epoch: 34 [0/45000 (0%)]	Loss: 0.932820
20:59:06: Train Epoch: 34 [10000/45000 (22%)]	Loss: 0.730107
20:59:11: Train Epoch: 34 [20000/45000 (44%)]	Loss: 0.683252
20:59:15: Train Epoch: 34 [30000/45000 (67%)]	Loss: 0.946380
20:59:20: Train Epoch: 34 [40000/45000 (89%)]	Loss: 1.063204
20:59:24: 
Evaluation: Average loss: -9.1705, Accuracy: 2810/5000 (56.200%)

20:59:24: Current learning rate: 0.1. Time taken for epoch: 24.25 seconds.

20:59:25: Train Epoch: 35 [0/45000 (0%)]	Loss: 0.794502
20:59:30: Train Epoch: 35 [10000/45000 (22%)]	Loss: 0.838742
20:59:34: Train Epoch: 35 [20000/45000 (44%)]	Loss: 0.837833
20:59:39: Train Epoch: 35 [30000/45000 (67%)]	Loss: 0.934575
20:59:44: Train Epoch: 35 [40000/45000 (89%)]	Loss: 0.818465
20:59:48: 
Evaluation: Average loss: -9.8930, Accuracy: 3072/5000 (61.440%)

20:59:48: Current learning rate: 0.1. Time taken for epoch: 23.77 seconds.

20:59:49: Train Epoch: 36 [0/45000 (0%)]	Loss: 0.823507
20:59:54: Train Epoch: 36 [10000/45000 (22%)]	Loss: 0.691665
20:59:58: Train Epoch: 36 [20000/45000 (44%)]	Loss: 1.036543
21:00:03: Train Epoch: 36 [30000/45000 (67%)]	Loss: 0.988652
21:00:08: Train Epoch: 36 [40000/45000 (89%)]	Loss: 0.859677
21:00:12: 
Evaluation: Average loss: -9.6574, Accuracy: 3162/5000 (63.240%)

21:00:12: Current learning rate: 0.1. Time taken for epoch: 24.25 seconds.

21:00:13: Train Epoch: 37 [0/45000 (0%)]	Loss: 0.901618
21:00:18: Train Epoch: 37 [10000/45000 (22%)]	Loss: 0.931756
21:00:23: Train Epoch: 37 [20000/45000 (44%)]	Loss: 0.716700
21:00:27: Train Epoch: 37 [30000/45000 (67%)]	Loss: 1.002655
21:00:32: Train Epoch: 37 [40000/45000 (89%)]	Loss: 0.882712
21:00:36: 
Evaluation: Average loss: -9.8682, Accuracy: 3207/5000 (64.140%)

21:00:36: Current learning rate: 0.1. Time taken for epoch: 23.95 seconds.

21:00:37: Train Epoch: 38 [0/45000 (0%)]	Loss: 0.899268
21:00:42: Train Epoch: 38 [10000/45000 (22%)]	Loss: 0.759776
21:00:46: Train Epoch: 38 [20000/45000 (44%)]	Loss: 0.896081
21:00:51: Train Epoch: 38 [30000/45000 (67%)]	Loss: 0.849068
21:00:56: Train Epoch: 38 [40000/45000 (89%)]	Loss: 0.715200
21:01:00: 
Evaluation: Average loss: -9.6149, Accuracy: 3373/5000 (67.460%)

21:01:00: Current learning rate: 0.1. Time taken for epoch: 24.14 seconds.

21:01:01: Train Epoch: 39 [0/45000 (0%)]	Loss: 1.055743
21:01:06: Train Epoch: 39 [10000/45000 (22%)]	Loss: 0.834283
21:01:11: Train Epoch: 39 [20000/45000 (44%)]	Loss: 0.817829
21:01:15: Train Epoch: 39 [30000/45000 (67%)]	Loss: 0.617335
21:01:20: Train Epoch: 39 [40000/45000 (89%)]	Loss: 0.853859
21:01:24: 
Evaluation: Average loss: -9.5809, Accuracy: 3272/5000 (65.440%)

21:01:24: Current learning rate: 0.1. Time taken for epoch: 23.90 seconds.

21:01:25: Train Epoch: 40 [0/45000 (0%)]	Loss: 0.808496
21:01:30: Train Epoch: 40 [10000/45000 (22%)]	Loss: 0.699672
21:01:34: Train Epoch: 40 [20000/45000 (44%)]	Loss: 0.895852
21:01:39: Train Epoch: 40 [30000/45000 (67%)]	Loss: 0.890089
21:01:44: Train Epoch: 40 [40000/45000 (89%)]	Loss: 0.652685
21:01:48: 
Evaluation: Average loss: -9.6341, Accuracy: 3224/5000 (64.480%)

21:01:48: Current learning rate: 0.1. Time taken for epoch: 24.10 seconds.

21:01:49: Train Epoch: 41 [0/45000 (0%)]	Loss: 0.729517
21:01:54: Train Epoch: 41 [10000/45000 (22%)]	Loss: 0.804201
21:01:59: Train Epoch: 41 [20000/45000 (44%)]	Loss: 0.814223
21:02:04: Train Epoch: 41 [30000/45000 (67%)]	Loss: 0.870869
21:02:09: Train Epoch: 41 [40000/45000 (89%)]	Loss: 0.798965
21:02:13: 
Evaluation: Average loss: -9.1282, Accuracy: 3162/5000 (63.240%)

21:02:13: Current learning rate: 0.1. Time taken for epoch: 24.31 seconds.

21:02:13: Train Epoch: 42 [0/45000 (0%)]	Loss: 0.761838
21:02:18: Train Epoch: 42 [10000/45000 (22%)]	Loss: 0.842254
21:02:23: Train Epoch: 42 [20000/45000 (44%)]	Loss: 0.804861
21:02:28: Train Epoch: 42 [30000/45000 (67%)]	Loss: 0.765744
21:02:32: Train Epoch: 42 [40000/45000 (89%)]	Loss: 0.947836
21:02:37: 
Evaluation: Average loss: -9.9166, Accuracy: 3135/5000 (62.700%)

21:02:37: Current learning rate: 0.1. Time taken for epoch: 24.06 seconds.

21:02:37: Train Epoch: 43 [0/45000 (0%)]	Loss: 0.722403
21:02:42: Train Epoch: 43 [10000/45000 (22%)]	Loss: 0.782312
21:02:47: Train Epoch: 43 [20000/45000 (44%)]	Loss: 0.860266
21:02:51: Train Epoch: 43 [30000/45000 (67%)]	Loss: 0.763400
21:02:56: Train Epoch: 43 [40000/45000 (89%)]	Loss: 0.831771
21:03:00: 
Evaluation: Average loss: -10.0861, Accuracy: 3411/5000 (68.220%)

21:03:00: Current learning rate: 0.1. Time taken for epoch: 23.81 seconds.

21:03:01: Train Epoch: 44 [0/45000 (0%)]	Loss: 0.796684
21:03:06: Train Epoch: 44 [10000/45000 (22%)]	Loss: 0.834370
21:03:11: Train Epoch: 44 [20000/45000 (44%)]	Loss: 0.680265
21:03:16: Train Epoch: 44 [30000/45000 (67%)]	Loss: 0.812426
21:03:20: Train Epoch: 44 [40000/45000 (89%)]	Loss: 0.894662
21:03:24: 
Evaluation: Average loss: -9.2832, Accuracy: 3290/5000 (65.800%)

21:03:24: Current learning rate: 0.1. Time taken for epoch: 23.88 seconds.

21:03:25: Train Epoch: 45 [0/45000 (0%)]	Loss: 0.772931
21:03:30: Train Epoch: 45 [10000/45000 (22%)]	Loss: 0.717635
21:03:35: Train Epoch: 45 [20000/45000 (44%)]	Loss: 0.881490
21:03:40: Train Epoch: 45 [30000/45000 (67%)]	Loss: 1.073661
21:03:44: Train Epoch: 45 [40000/45000 (89%)]	Loss: 0.798676
21:03:48: 
Evaluation: Average loss: -10.0017, Accuracy: 3192/5000 (63.840%)

21:03:48: Current learning rate: 0.1. Time taken for epoch: 23.86 seconds.

21:03:49: Train Epoch: 46 [0/45000 (0%)]	Loss: 0.831712
21:03:54: Train Epoch: 46 [10000/45000 (22%)]	Loss: 0.870676
21:03:59: Train Epoch: 46 [20000/45000 (44%)]	Loss: 0.873074
21:04:04: Train Epoch: 46 [30000/45000 (67%)]	Loss: 0.706044
21:04:09: Train Epoch: 46 [40000/45000 (89%)]	Loss: 0.759745
21:04:13: 
Evaluation: Average loss: -9.3096, Accuracy: 2967/5000 (59.340%)

21:04:13: Current learning rate: 0.1. Time taken for epoch: 24.47 seconds.

21:04:13: Train Epoch: 47 [0/45000 (0%)]	Loss: 0.827818
21:04:18: Train Epoch: 47 [10000/45000 (22%)]	Loss: 1.092931
21:04:23: Train Epoch: 47 [20000/45000 (44%)]	Loss: 0.951133
21:04:28: Train Epoch: 47 [30000/45000 (67%)]	Loss: 0.862689
21:04:33: Train Epoch: 47 [40000/45000 (89%)]	Loss: 0.699109
21:04:37: 
Evaluation: Average loss: -9.4960, Accuracy: 3319/5000 (66.380%)

21:04:37: Current learning rate: 0.1. Time taken for epoch: 23.95 seconds.

21:04:37: Train Epoch: 48 [0/45000 (0%)]	Loss: 0.702721
21:04:42: Train Epoch: 48 [10000/45000 (22%)]	Loss: 0.678853
21:04:47: Train Epoch: 48 [20000/45000 (44%)]	Loss: 0.904146
21:04:52: Train Epoch: 48 [30000/45000 (67%)]	Loss: 0.956339
21:04:56: Train Epoch: 48 [40000/45000 (89%)]	Loss: 0.842904
21:05:00: 
Evaluation: Average loss: -9.5424, Accuracy: 3183/5000 (63.660%)

21:05:00: Current learning rate: 0.1. Time taken for epoch: 23.74 seconds.

21:05:01: Train Epoch: 49 [0/45000 (0%)]	Loss: 0.968836
21:05:06: Train Epoch: 49 [10000/45000 (22%)]	Loss: 0.849136
21:05:11: Train Epoch: 49 [20000/45000 (44%)]	Loss: 0.810968
21:05:15: Train Epoch: 49 [30000/45000 (67%)]	Loss: 0.692862
21:05:20: Train Epoch: 49 [40000/45000 (89%)]	Loss: 0.884590
21:05:24: 
Evaluation: Average loss: -9.2117, Accuracy: 3487/5000 (69.740%)

21:05:24: Current learning rate: 0.1. Time taken for epoch: 23.54 seconds.

21:05:24: Train Epoch: 50 [0/45000 (0%)]	Loss: 1.013162
21:05:29: Train Epoch: 50 [10000/45000 (22%)]	Loss: 0.799088
21:05:34: Train Epoch: 50 [20000/45000 (44%)]	Loss: 0.718768
21:05:39: Train Epoch: 50 [30000/45000 (67%)]	Loss: 0.724173
21:05:44: Train Epoch: 50 [40000/45000 (89%)]	Loss: 1.014933
21:05:48: 
Evaluation: Average loss: -9.7527, Accuracy: 3325/5000 (66.500%)

21:05:48: Current learning rate: 0.1. Time taken for epoch: 23.96 seconds.

21:05:48: Train Epoch: 51 [0/45000 (0%)]	Loss: 0.809999
21:05:54: Train Epoch: 51 [10000/45000 (22%)]	Loss: 0.915864
21:05:59: Train Epoch: 51 [20000/45000 (44%)]	Loss: 0.975752
21:06:04: Train Epoch: 51 [30000/45000 (67%)]	Loss: 0.982616
21:06:08: Train Epoch: 51 [40000/45000 (89%)]	Loss: 0.790018
21:06:12: 
Evaluation: Average loss: -9.6201, Accuracy: 3412/5000 (68.240%)

21:06:12: Current learning rate: 0.1. Time taken for epoch: 24.28 seconds.

21:06:13: Train Epoch: 52 [0/45000 (0%)]	Loss: 0.979719
21:06:18: Train Epoch: 52 [10000/45000 (22%)]	Loss: 0.630918
21:06:23: Train Epoch: 52 [20000/45000 (44%)]	Loss: 0.803056
21:06:27: Train Epoch: 52 [30000/45000 (67%)]	Loss: 1.067648
21:06:32: Train Epoch: 52 [40000/45000 (89%)]	Loss: 0.974786
21:06:36: 
Evaluation: Average loss: -9.0973, Accuracy: 3075/5000 (61.500%)

21:06:36: Current learning rate: 0.1. Time taken for epoch: 23.71 seconds.

21:06:36: Train Epoch: 53 [0/45000 (0%)]	Loss: 1.056797
21:06:41: Train Epoch: 53 [10000/45000 (22%)]	Loss: 0.923104
21:06:46: Train Epoch: 53 [20000/45000 (44%)]	Loss: 1.043985
21:06:51: Train Epoch: 53 [30000/45000 (67%)]	Loss: 1.213677
21:06:55: Train Epoch: 53 [40000/45000 (89%)]	Loss: 0.704834
21:07:00: 
Evaluation: Average loss: -9.4589, Accuracy: 3113/5000 (62.260%)

21:07:00: Current learning rate: 0.1. Time taken for epoch: 23.83 seconds.

21:07:00: Train Epoch: 54 [0/45000 (0%)]	Loss: 0.795203
21:07:05: Train Epoch: 54 [10000/45000 (22%)]	Loss: 0.776334
21:07:10: Train Epoch: 54 [20000/45000 (44%)]	Loss: 0.834865
21:07:15: Train Epoch: 54 [30000/45000 (67%)]	Loss: 0.682831
21:07:20: Train Epoch: 54 [40000/45000 (89%)]	Loss: 0.792435
21:07:24: 
Evaluation: Average loss: -9.6607, Accuracy: 3148/5000 (62.960%)

21:07:24: Current learning rate: 0.1. Time taken for epoch: 23.92 seconds.

21:07:24: Train Epoch: 55 [0/45000 (0%)]	Loss: 0.900107
21:07:29: Train Epoch: 55 [10000/45000 (22%)]	Loss: 0.823159
21:07:34: Train Epoch: 55 [20000/45000 (44%)]	Loss: 0.772552
21:07:39: Train Epoch: 55 [30000/45000 (67%)]	Loss: 1.022007
21:07:43: Train Epoch: 55 [40000/45000 (89%)]	Loss: 0.820592
21:07:47: 
Evaluation: Average loss: -9.1224, Accuracy: 3215/5000 (64.300%)

21:07:47: Current learning rate: 0.1. Time taken for epoch: 23.37 seconds.

21:07:48: Train Epoch: 56 [0/45000 (0%)]	Loss: 0.947375
21:07:53: Train Epoch: 56 [10000/45000 (22%)]	Loss: 1.065654
21:07:57: Train Epoch: 56 [20000/45000 (44%)]	Loss: 0.785336
21:08:02: Train Epoch: 56 [30000/45000 (67%)]	Loss: 0.735297
21:08:07: Train Epoch: 56 [40000/45000 (89%)]	Loss: 0.692828
21:08:11: 
Evaluation: Average loss: -10.5049, Accuracy: 3910/5000 (78.200%)

21:08:11: Current learning rate: 0.010000000000000002. Time taken for epoch: 24.14 seconds.

21:08:12: Train Epoch: 57 [0/45000 (0%)]	Loss: 0.583791
21:08:17: Train Epoch: 57 [10000/45000 (22%)]	Loss: 0.610727
21:08:21: Train Epoch: 57 [20000/45000 (44%)]	Loss: 0.820201
21:08:26: Train Epoch: 57 [30000/45000 (67%)]	Loss: 0.676615
21:08:31: Train Epoch: 57 [40000/45000 (89%)]	Loss: 0.602517
21:08:35: 
Evaluation: Average loss: -11.0471, Accuracy: 3969/5000 (79.380%)

21:08:35: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.44 seconds.

21:08:35: Train Epoch: 58 [0/45000 (0%)]	Loss: 0.492001
21:08:40: Train Epoch: 58 [10000/45000 (22%)]	Loss: 0.495505
21:08:45: Train Epoch: 58 [20000/45000 (44%)]	Loss: 0.896689
21:08:49: Train Epoch: 58 [30000/45000 (67%)]	Loss: 0.674426
21:08:54: Train Epoch: 58 [40000/45000 (89%)]	Loss: 0.851511
21:08:58: 
Evaluation: Average loss: -11.2745, Accuracy: 3957/5000 (79.140%)

21:08:58: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.60 seconds.

21:08:59: Train Epoch: 59 [0/45000 (0%)]	Loss: 0.581436
21:09:04: Train Epoch: 59 [10000/45000 (22%)]	Loss: 0.599435
21:09:09: Train Epoch: 59 [20000/45000 (44%)]	Loss: 0.584830
21:09:14: Train Epoch: 59 [30000/45000 (67%)]	Loss: 0.397078
21:09:18: Train Epoch: 59 [40000/45000 (89%)]	Loss: 0.483286
21:09:22: 
Evaluation: Average loss: -11.3811, Accuracy: 4048/5000 (80.960%)

21:09:22: Current learning rate: 0.010000000000000002. Time taken for epoch: 24.04 seconds.

21:09:23: Train Epoch: 60 [0/45000 (0%)]	Loss: 0.640749
21:09:28: Train Epoch: 60 [10000/45000 (22%)]	Loss: 0.433853
21:09:32: Train Epoch: 60 [20000/45000 (44%)]	Loss: 0.619624
21:09:37: Train Epoch: 60 [30000/45000 (67%)]	Loss: 0.614465
21:09:42: Train Epoch: 60 [40000/45000 (89%)]	Loss: 0.805747
21:09:46: 
Evaluation: Average loss: -11.5315, Accuracy: 4019/5000 (80.380%)

21:09:46: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.59 seconds.

21:09:46: Train Epoch: 61 [0/45000 (0%)]	Loss: 0.486132
21:09:52: Train Epoch: 61 [10000/45000 (22%)]	Loss: 0.629847
21:09:56: Train Epoch: 61 [20000/45000 (44%)]	Loss: 0.510730
21:10:01: Train Epoch: 61 [30000/45000 (67%)]	Loss: 0.728454
21:10:06: Train Epoch: 61 [40000/45000 (89%)]	Loss: 0.620467
21:10:10: 
Evaluation: Average loss: -11.4023, Accuracy: 4009/5000 (80.180%)

21:10:10: Current learning rate: 0.010000000000000002. Time taken for epoch: 24.30 seconds.

21:10:11: Train Epoch: 62 [0/45000 (0%)]	Loss: 0.451859
21:10:15: Train Epoch: 62 [10000/45000 (22%)]	Loss: 0.392848
21:10:20: Train Epoch: 62 [20000/45000 (44%)]	Loss: 0.755310
21:10:25: Train Epoch: 62 [30000/45000 (67%)]	Loss: 0.565164
21:10:29: Train Epoch: 62 [40000/45000 (89%)]	Loss: 0.787594
21:10:33: 
Evaluation: Average loss: -11.5007, Accuracy: 4011/5000 (80.220%)

21:10:33: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.20 seconds.

21:10:34: Train Epoch: 63 [0/45000 (0%)]	Loss: 0.529918
21:10:39: Train Epoch: 63 [10000/45000 (22%)]	Loss: 0.652862
21:10:44: Train Epoch: 63 [20000/45000 (44%)]	Loss: 0.339425
21:10:48: Train Epoch: 63 [30000/45000 (67%)]	Loss: 0.498719
21:10:53: Train Epoch: 63 [40000/45000 (89%)]	Loss: 0.614959
21:10:57: 
Evaluation: Average loss: -11.6574, Accuracy: 4052/5000 (81.040%)

21:10:57: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.47 seconds.

21:10:58: Train Epoch: 64 [0/45000 (0%)]	Loss: 0.577086
21:11:03: Train Epoch: 64 [10000/45000 (22%)]	Loss: 0.727477
21:11:08: Train Epoch: 64 [20000/45000 (44%)]	Loss: 0.549417
21:11:12: Train Epoch: 64 [30000/45000 (67%)]	Loss: 0.643668
21:11:17: Train Epoch: 64 [40000/45000 (89%)]	Loss: 0.642414
21:11:21: 
Evaluation: Average loss: -11.5819, Accuracy: 4039/5000 (80.780%)

21:11:21: Current learning rate: 0.010000000000000002. Time taken for epoch: 24.14 seconds.

21:11:21: Train Epoch: 65 [0/45000 (0%)]	Loss: 0.537754
21:11:26: Train Epoch: 65 [10000/45000 (22%)]	Loss: 0.521063
21:11:31: Train Epoch: 65 [20000/45000 (44%)]	Loss: 0.461598
21:11:35: Train Epoch: 65 [30000/45000 (67%)]	Loss: 0.606288
21:11:40: Train Epoch: 65 [40000/45000 (89%)]	Loss: 0.512658
21:11:44: 
Evaluation: Average loss: -11.6362, Accuracy: 4022/5000 (80.440%)

21:11:44: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.35 seconds.

21:11:45: Train Epoch: 66 [0/45000 (0%)]	Loss: 0.605291
21:11:50: Train Epoch: 66 [10000/45000 (22%)]	Loss: 0.421721
21:11:54: Train Epoch: 66 [20000/45000 (44%)]	Loss: 0.617288
21:11:59: Train Epoch: 66 [30000/45000 (67%)]	Loss: 0.643846
21:12:04: Train Epoch: 66 [40000/45000 (89%)]	Loss: 0.609619
21:12:08: 
Evaluation: Average loss: -11.5528, Accuracy: 4055/5000 (81.100%)

21:12:08: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.71 seconds.

21:12:09: Train Epoch: 67 [0/45000 (0%)]	Loss: 0.384435
21:12:13: Train Epoch: 67 [10000/45000 (22%)]	Loss: 0.460006
21:12:18: Train Epoch: 67 [20000/45000 (44%)]	Loss: 0.633705
21:12:23: Train Epoch: 67 [30000/45000 (67%)]	Loss: 0.383606
21:12:27: Train Epoch: 67 [40000/45000 (89%)]	Loss: 0.560009
21:12:31: 
Evaluation: Average loss: -11.6383, Accuracy: 4030/5000 (80.600%)

21:12:31: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.16 seconds.

21:12:32: Train Epoch: 68 [0/45000 (0%)]	Loss: 0.463185
21:12:37: Train Epoch: 68 [10000/45000 (22%)]	Loss: 0.561657
21:12:42: Train Epoch: 68 [20000/45000 (44%)]	Loss: 0.457468
21:12:46: Train Epoch: 68 [30000/45000 (67%)]	Loss: 0.596860
21:12:51: Train Epoch: 68 [40000/45000 (89%)]	Loss: 0.590753
21:12:55: 
Evaluation: Average loss: -11.5879, Accuracy: 4055/5000 (81.100%)

21:12:55: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.84 seconds.

21:12:56: Train Epoch: 69 [0/45000 (0%)]	Loss: 0.640822
21:13:01: Train Epoch: 69 [10000/45000 (22%)]	Loss: 0.482310
21:13:05: Train Epoch: 69 [20000/45000 (44%)]	Loss: 0.746504
21:13:10: Train Epoch: 69 [30000/45000 (67%)]	Loss: 0.469187
21:13:15: Train Epoch: 69 [40000/45000 (89%)]	Loss: 0.432705
21:13:19: 
Evaluation: Average loss: -11.7390, Accuracy: 4079/5000 (81.580%)

21:13:19: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.77 seconds.

21:13:20: Train Epoch: 70 [0/45000 (0%)]	Loss: 0.341194
21:13:24: Train Epoch: 70 [10000/45000 (22%)]	Loss: 0.417211
21:13:29: Train Epoch: 70 [20000/45000 (44%)]	Loss: 0.549300
21:13:33: Train Epoch: 70 [30000/45000 (67%)]	Loss: 0.609718
21:13:38: Train Epoch: 70 [40000/45000 (89%)]	Loss: 0.504609
21:13:42: 
Evaluation: Average loss: -11.4931, Accuracy: 4067/5000 (81.340%)

21:13:42: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.21 seconds.

21:13:43: Train Epoch: 71 [0/45000 (0%)]	Loss: 0.401327
21:13:48: Train Epoch: 71 [10000/45000 (22%)]	Loss: 0.349390
21:13:52: Train Epoch: 71 [20000/45000 (44%)]	Loss: 0.533454
21:13:57: Train Epoch: 71 [30000/45000 (67%)]	Loss: 0.538099
21:14:02: Train Epoch: 71 [40000/45000 (89%)]	Loss: 0.718706
21:14:06: 
Evaluation: Average loss: -11.4077, Accuracy: 4025/5000 (80.500%)

21:14:06: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.90 seconds.

21:14:06: Train Epoch: 72 [0/45000 (0%)]	Loss: 0.461353
21:14:11: Train Epoch: 72 [10000/45000 (22%)]	Loss: 0.563096
21:14:16: Train Epoch: 72 [20000/45000 (44%)]	Loss: 0.569688
21:14:21: Train Epoch: 72 [30000/45000 (67%)]	Loss: 0.483049
21:14:26: Train Epoch: 72 [40000/45000 (89%)]	Loss: 0.377638
21:14:29: 
Evaluation: Average loss: -11.4577, Accuracy: 4031/5000 (80.620%)

21:14:29: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.47 seconds.

21:14:30: Train Epoch: 73 [0/45000 (0%)]	Loss: 0.367784
21:14:35: Train Epoch: 73 [10000/45000 (22%)]	Loss: 0.520718
21:14:40: Train Epoch: 73 [20000/45000 (44%)]	Loss: 0.492455
21:14:44: Train Epoch: 73 [30000/45000 (67%)]	Loss: 0.619479
21:14:49: Train Epoch: 73 [40000/45000 (89%)]	Loss: 0.534110
21:14:53: 
Evaluation: Average loss: -11.5063, Accuracy: 4078/5000 (81.560%)

21:14:53: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.40 seconds.

21:14:53: Train Epoch: 74 [0/45000 (0%)]	Loss: 0.434684
21:14:58: Train Epoch: 74 [10000/45000 (22%)]	Loss: 0.495098
21:15:04: Train Epoch: 74 [20000/45000 (44%)]	Loss: 0.490578
21:15:08: Train Epoch: 74 [30000/45000 (67%)]	Loss: 0.607585
21:15:13: Train Epoch: 74 [40000/45000 (89%)]	Loss: 0.689729
21:15:17: 
Evaluation: Average loss: -11.5388, Accuracy: 4052/5000 (81.040%)

21:15:17: Current learning rate: 0.010000000000000002. Time taken for epoch: 24.22 seconds.

21:15:18: Train Epoch: 75 [0/45000 (0%)]	Loss: 0.407369
21:15:23: Train Epoch: 75 [10000/45000 (22%)]	Loss: 0.571946
21:15:27: Train Epoch: 75 [20000/45000 (44%)]	Loss: 0.502725
21:15:32: Train Epoch: 75 [30000/45000 (67%)]	Loss: 0.543659
21:15:37: Train Epoch: 75 [40000/45000 (89%)]	Loss: 0.432488
21:15:41: 
Evaluation: Average loss: -11.3130, Accuracy: 4029/5000 (80.580%)

21:15:41: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.73 seconds.

21:15:41: Train Epoch: 76 [0/45000 (0%)]	Loss: 0.422704
21:15:46: Train Epoch: 76 [10000/45000 (22%)]	Loss: 0.579334
21:15:51: Train Epoch: 76 [20000/45000 (44%)]	Loss: 0.507109
21:15:56: Train Epoch: 76 [30000/45000 (67%)]	Loss: 0.411414
21:16:01: Train Epoch: 76 [40000/45000 (89%)]	Loss: 0.769988
21:16:05: 
Evaluation: Average loss: -11.2919, Accuracy: 3996/5000 (79.920%)

21:16:05: Current learning rate: 0.010000000000000002. Time taken for epoch: 24.28 seconds.

21:16:06: Train Epoch: 77 [0/45000 (0%)]	Loss: 0.394493
21:16:10: Train Epoch: 77 [10000/45000 (22%)]	Loss: 0.488260
21:16:15: Train Epoch: 77 [20000/45000 (44%)]	Loss: 0.449688
21:16:20: Train Epoch: 77 [30000/45000 (67%)]	Loss: 0.440614
21:16:25: Train Epoch: 77 [40000/45000 (89%)]	Loss: 0.411205
21:16:28: 
Evaluation: Average loss: -11.4529, Accuracy: 4050/5000 (81.000%)

21:16:28: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.35 seconds.

21:16:29: Train Epoch: 78 [0/45000 (0%)]	Loss: 0.484380
21:16:34: Train Epoch: 78 [10000/45000 (22%)]	Loss: 0.486632
21:16:39: Train Epoch: 78 [20000/45000 (44%)]	Loss: 0.501275
21:16:44: Train Epoch: 78 [30000/45000 (67%)]	Loss: 0.435507
21:16:48: Train Epoch: 78 [40000/45000 (89%)]	Loss: 0.628320
21:16:52: 
Evaluation: Average loss: -11.3768, Accuracy: 4007/5000 (80.140%)

21:16:52: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.82 seconds.

21:16:53: Train Epoch: 79 [0/45000 (0%)]	Loss: 0.536366
21:16:58: Train Epoch: 79 [10000/45000 (22%)]	Loss: 0.542151
21:17:03: Train Epoch: 79 [20000/45000 (44%)]	Loss: 0.461753
21:17:08: Train Epoch: 79 [30000/45000 (67%)]	Loss: 0.615222
21:17:12: Train Epoch: 79 [40000/45000 (89%)]	Loss: 0.641577
21:17:16: 
Evaluation: Average loss: -11.3724, Accuracy: 4052/5000 (81.040%)

21:17:16: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.85 seconds.

21:17:17: Train Epoch: 80 [0/45000 (0%)]	Loss: 0.504757
21:17:22: Train Epoch: 80 [10000/45000 (22%)]	Loss: 0.408265
21:17:26: Train Epoch: 80 [20000/45000 (44%)]	Loss: 0.615605
21:17:31: Train Epoch: 80 [30000/45000 (67%)]	Loss: 0.494923
21:17:36: Train Epoch: 80 [40000/45000 (89%)]	Loss: 0.445864
21:17:40: 
Evaluation: Average loss: -11.2753, Accuracy: 4013/5000 (80.260%)

21:17:40: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.69 seconds.

21:17:40: Train Epoch: 81 [0/45000 (0%)]	Loss: 0.378788
21:17:45: Train Epoch: 81 [10000/45000 (22%)]	Loss: 0.471908
21:17:50: Train Epoch: 81 [20000/45000 (44%)]	Loss: 0.584665
21:17:55: Train Epoch: 81 [30000/45000 (67%)]	Loss: 0.496690
21:18:00: Train Epoch: 81 [40000/45000 (89%)]	Loss: 0.380599
21:18:04: 
Evaluation: Average loss: -11.2359, Accuracy: 3975/5000 (79.500%)

21:18:04: Current learning rate: 0.010000000000000002. Time taken for epoch: 24.46 seconds.

21:18:05: Train Epoch: 82 [0/45000 (0%)]	Loss: 0.519852
21:18:10: Train Epoch: 82 [10000/45000 (22%)]	Loss: 0.664890
21:18:14: Train Epoch: 82 [20000/45000 (44%)]	Loss: 0.502063
21:18:19: Train Epoch: 82 [30000/45000 (67%)]	Loss: 0.340471
21:18:23: Train Epoch: 82 [40000/45000 (89%)]	Loss: 0.691184
21:18:27: 
Evaluation: Average loss: -11.3756, Accuracy: 4069/5000 (81.380%)

21:18:27: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.05 seconds.

21:18:28: Train Epoch: 83 [0/45000 (0%)]	Loss: 0.577453
21:18:33: Train Epoch: 83 [10000/45000 (22%)]	Loss: 0.414960
21:18:38: Train Epoch: 83 [20000/45000 (44%)]	Loss: 0.523018
21:18:42: Train Epoch: 83 [30000/45000 (67%)]	Loss: 0.491507
21:18:47: Train Epoch: 83 [40000/45000 (89%)]	Loss: 0.705093
21:18:51: 
Evaluation: Average loss: -11.2410, Accuracy: 3983/5000 (79.660%)

21:18:51: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.64 seconds.

21:18:52: Train Epoch: 84 [0/45000 (0%)]	Loss: 0.697139
21:18:56: Train Epoch: 84 [10000/45000 (22%)]	Loss: 0.411445
21:19:02: Train Epoch: 84 [20000/45000 (44%)]	Loss: 0.404508
21:19:06: Train Epoch: 84 [30000/45000 (67%)]	Loss: 0.813503
21:19:11: Train Epoch: 84 [40000/45000 (89%)]	Loss: 0.444083
21:19:15: 
Evaluation: Average loss: -11.2097, Accuracy: 4026/5000 (80.520%)

21:19:15: Current learning rate: 0.010000000000000002. Time taken for epoch: 24.09 seconds.

21:19:16: Train Epoch: 85 [0/45000 (0%)]	Loss: 0.387804
21:19:21: Train Epoch: 85 [10000/45000 (22%)]	Loss: 0.423696
21:19:25: Train Epoch: 85 [20000/45000 (44%)]	Loss: 0.452612
21:19:30: Train Epoch: 85 [30000/45000 (67%)]	Loss: 0.326870
21:19:34: Train Epoch: 85 [40000/45000 (89%)]	Loss: 0.633443
21:19:38: 
Evaluation: Average loss: -11.3785, Accuracy: 4039/5000 (80.780%)

21:19:38: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.54 seconds.

21:19:39: Train Epoch: 86 [0/45000 (0%)]	Loss: 0.403101
21:19:44: Train Epoch: 86 [10000/45000 (22%)]	Loss: 0.623949
21:19:49: Train Epoch: 86 [20000/45000 (44%)]	Loss: 0.523925
21:19:53: Train Epoch: 86 [30000/45000 (67%)]	Loss: 0.616334
21:19:58: Train Epoch: 86 [40000/45000 (89%)]	Loss: 0.554088
21:20:02: 
Evaluation: Average loss: -11.2648, Accuracy: 4011/5000 (80.220%)

21:20:02: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.91 seconds.

21:20:03: Train Epoch: 87 [0/45000 (0%)]	Loss: 0.343066
21:20:08: Train Epoch: 87 [10000/45000 (22%)]	Loss: 0.572756
21:20:12: Train Epoch: 87 [20000/45000 (44%)]	Loss: 0.492405
21:20:17: Train Epoch: 87 [30000/45000 (67%)]	Loss: 0.589418
21:20:22: Train Epoch: 87 [40000/45000 (89%)]	Loss: 0.499916
21:20:26: 
Evaluation: Average loss: -11.1745, Accuracy: 4014/5000 (80.280%)

21:20:26: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.28 seconds.

21:20:26: Train Epoch: 88 [0/45000 (0%)]	Loss: 0.486705
21:20:31: Train Epoch: 88 [10000/45000 (22%)]	Loss: 0.469551
21:20:36: Train Epoch: 88 [20000/45000 (44%)]	Loss: 0.664535
21:20:41: Train Epoch: 88 [30000/45000 (67%)]	Loss: 0.696349
21:20:45: Train Epoch: 88 [40000/45000 (89%)]	Loss: 0.559893
21:20:49: 
Evaluation: Average loss: -11.2917, Accuracy: 4042/5000 (80.840%)

21:20:49: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.35 seconds.

21:20:50: Train Epoch: 89 [0/45000 (0%)]	Loss: 0.751724
21:20:54: Train Epoch: 89 [10000/45000 (22%)]	Loss: 0.383828
21:20:59: Train Epoch: 89 [20000/45000 (44%)]	Loss: 0.659308
21:21:04: Train Epoch: 89 [30000/45000 (67%)]	Loss: 0.433164
21:21:09: Train Epoch: 89 [40000/45000 (89%)]	Loss: 0.435577
21:21:13: 
Evaluation: Average loss: -11.3779, Accuracy: 4032/5000 (80.640%)

21:21:13: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.55 seconds.

21:21:13: Train Epoch: 90 [0/45000 (0%)]	Loss: 0.478370
21:21:18: Train Epoch: 90 [10000/45000 (22%)]	Loss: 0.598984
21:21:23: Train Epoch: 90 [20000/45000 (44%)]	Loss: 0.333060
21:21:27: Train Epoch: 90 [30000/45000 (67%)]	Loss: 0.492867
21:21:32: Train Epoch: 90 [40000/45000 (89%)]	Loss: 0.594300
21:21:36: 
Evaluation: Average loss: -11.0592, Accuracy: 4042/5000 (80.840%)

21:21:36: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.41 seconds.

21:21:37: Train Epoch: 91 [0/45000 (0%)]	Loss: 0.509566
21:21:42: Train Epoch: 91 [10000/45000 (22%)]	Loss: 0.505443
21:21:46: Train Epoch: 91 [20000/45000 (44%)]	Loss: 0.401623
21:21:51: Train Epoch: 91 [30000/45000 (67%)]	Loss: 0.543174
21:21:56: Train Epoch: 91 [40000/45000 (89%)]	Loss: 0.572450
21:22:00: 
Evaluation: Average loss: -11.2158, Accuracy: 3978/5000 (79.560%)

21:22:00: Current learning rate: 0.010000000000000002. Time taken for epoch: 24.04 seconds.

21:22:01: Train Epoch: 92 [0/45000 (0%)]	Loss: 0.475910
21:22:06: Train Epoch: 92 [10000/45000 (22%)]	Loss: 0.529942
21:22:10: Train Epoch: 92 [20000/45000 (44%)]	Loss: 0.420506
21:22:15: Train Epoch: 92 [30000/45000 (67%)]	Loss: 0.473290
21:22:20: Train Epoch: 92 [40000/45000 (89%)]	Loss: 0.443720
21:22:24: 
Evaluation: Average loss: -11.1245, Accuracy: 3973/5000 (79.460%)

21:22:24: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.58 seconds.

21:22:24: Train Epoch: 93 [0/45000 (0%)]	Loss: 0.569682
21:22:29: Train Epoch: 93 [10000/45000 (22%)]	Loss: 0.471302
21:22:34: Train Epoch: 93 [20000/45000 (44%)]	Loss: 0.482904
21:22:38: Train Epoch: 93 [30000/45000 (67%)]	Loss: 0.649148
21:22:43: Train Epoch: 93 [40000/45000 (89%)]	Loss: 0.490077
21:22:47: 
Evaluation: Average loss: -11.1002, Accuracy: 4023/5000 (80.460%)

21:22:47: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.34 seconds.

21:22:48: Train Epoch: 94 [0/45000 (0%)]	Loss: 0.531129
21:22:53: Train Epoch: 94 [10000/45000 (22%)]	Loss: 0.402649
21:22:57: Train Epoch: 94 [20000/45000 (44%)]	Loss: 0.598096
21:23:03: Train Epoch: 94 [30000/45000 (67%)]	Loss: 0.377462
21:23:07: Train Epoch: 94 [40000/45000 (89%)]	Loss: 0.579531
21:23:11: 
Evaluation: Average loss: -11.1098, Accuracy: 3988/5000 (79.760%)

21:23:11: Current learning rate: 0.010000000000000002. Time taken for epoch: 24.14 seconds.

21:23:12: Train Epoch: 95 [0/45000 (0%)]	Loss: 0.430056
21:23:17: Train Epoch: 95 [10000/45000 (22%)]	Loss: 0.579389
21:23:21: Train Epoch: 95 [20000/45000 (44%)]	Loss: 0.428335
21:23:26: Train Epoch: 95 [30000/45000 (67%)]	Loss: 0.366472
21:23:30: Train Epoch: 95 [40000/45000 (89%)]	Loss: 0.571695
21:23:34: 
Evaluation: Average loss: -11.2177, Accuracy: 4024/5000 (80.480%)

21:23:34: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.38 seconds.

21:23:35: Train Epoch: 96 [0/45000 (0%)]	Loss: 0.598104
21:23:40: Train Epoch: 96 [10000/45000 (22%)]	Loss: 0.430543
21:23:45: Train Epoch: 96 [20000/45000 (44%)]	Loss: 0.425732
21:23:49: Train Epoch: 96 [30000/45000 (67%)]	Loss: 0.572051
21:23:54: Train Epoch: 96 [40000/45000 (89%)]	Loss: 0.456477
21:23:58: 
Evaluation: Average loss: -11.2202, Accuracy: 4008/5000 (80.160%)

21:23:58: Current learning rate: 0.010000000000000002. Time taken for epoch: 23.54 seconds.

21:23:59: Train Epoch: 97 [0/45000 (0%)]	Loss: 0.394244
21:24:30: Namespace(batch_size=100, bench=True, data='cifar', decay_frequency=25000, decay_schedule='cosine', dense=False, density=0.05, epochs=100, fp16=False, growth='momentum', iters=1, l1=0.0, l2=0.0005, log_interval=100, lr=0.1, max_threads=10, model='MobileNet', momentum=0.9, no_cuda=False, optimizer='sgd', prune='magnitude', prune_rate=0.5, redistribution='momentum', resume=None, save_features=False, save_model='./models/model.pt', seed=17, start_epoch=1, test_batch_size=100, valid_split=0.1, verbose=False)
21:24:30: 


21:24:30: ================================================================================
21:24:30: 
Iteration start: 1/1

21:24:31: MobileNet(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (dw2_1): DepthWiseBlock(
    (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
    (bn_dw): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw2_2): DepthWiseBlock(
    (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
    (bn_dw): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw3_1): DepthWiseBlock(
    (conv_dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
    (bn_dw): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw3_2): DepthWiseBlock(
    (conv_dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
    (bn_dw): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw4_1): DepthWiseBlock(
    (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
    (bn_dw): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw4_2): DepthWiseBlock(
    (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
    (bn_dw): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_1): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_2): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_3): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_4): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_5): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_6): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw6): DepthWiseBlock(
    (conv_dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)
    (bn_dw): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (fc): Linear(in_features=1024, out_features=100, bias=True)
)
21:24:31: ============================================================
21:24:31: MobileNet
21:24:31: ============================================================
21:24:31: ============================================================
21:24:31: Prune mode: magnitude
21:24:31: Growth mode: momentum
21:24:31: Redistribution mode: momentum
21:24:31: ============================================================
21:24:32: Train Epoch: 1 [0/45000 (0%)]	Loss: 4.604723
21:24:37: Train Epoch: 1 [10000/45000 (22%)]	Loss: nan
21:24:42: Train Epoch: 1 [20000/45000 (44%)]	Loss: nan
21:24:46: Train Epoch: 1 [30000/45000 (67%)]	Loss: nan
21:24:51: Train Epoch: 1 [40000/45000 (89%)]	Loss: nan
21:24:55: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

21:25:51: Namespace(batch_size=100, bench=True, data='cifar', decay_frequency=25000, decay_schedule='cosine', dense=True, density=1.0, epochs=100, fp16=False, growth='momentum', iters=1, l1=0.0, l2=0.0005, log_interval=100, lr=0.1, max_threads=10, model='MobileNet', momentum=0.9, no_cuda=False, optimizer='sgd', prune='magnitude', prune_rate=0.5, redistribution='momentum', resume=None, save_features=False, save_model='./models/model.pt', seed=17, start_epoch=1, test_batch_size=100, valid_split=0.1, verbose=True)
21:25:51: 


21:25:51: ================================================================================
21:25:51: 
Iteration start: 1/1

21:25:52: MobileNet(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (dw2_1): DepthWiseBlock(
    (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
    (bn_dw): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw2_2): DepthWiseBlock(
    (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
    (bn_dw): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw3_1): DepthWiseBlock(
    (conv_dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
    (bn_dw): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw3_2): DepthWiseBlock(
    (conv_dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
    (bn_dw): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw4_1): DepthWiseBlock(
    (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
    (bn_dw): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw4_2): DepthWiseBlock(
    (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
    (bn_dw): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_1): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_2): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_3): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_4): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_5): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_6): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw6): DepthWiseBlock(
    (conv_dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)
    (bn_dw): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (fc): Linear(in_features=1024, out_features=100, bias=True)
)
21:25:52: ============================================================
21:25:52: MobileNet
21:25:52: ============================================================
21:25:52: ============================================================
21:25:52: Prune mode: magnitude
21:25:52: Growth mode: momentum
21:25:52: Redistribution mode: momentum
21:25:52: ============================================================
21:25:53: Train Epoch: 1 [0/45000 (0%)]	Loss: 4.641055
21:25:58: Train Epoch: 1 [10000/45000 (22%)]	Loss: 2.070999
21:26:03: Train Epoch: 1 [20000/45000 (44%)]	Loss: 2.191387
21:26:08: Train Epoch: 1 [30000/45000 (67%)]	Loss: 1.572804
21:26:12: Train Epoch: 1 [40000/45000 (89%)]	Loss: 1.645447
21:26:16: 
Evaluation: Average loss: -7.4973, Accuracy: 1916/5000 (38.320%)

21:26:16: Current learning rate: 0.1. Time taken for epoch: 23.98 seconds.

21:26:17: Train Epoch: 2 [0/45000 (0%)]	Loss: 1.539217
21:26:22: Train Epoch: 2 [10000/45000 (22%)]	Loss: 1.506656
21:26:26: Train Epoch: 2 [20000/45000 (44%)]	Loss: 1.660257
21:28:33: Namespace(batch_size=100, bench=True, data='cifar', decay_frequency=25000, decay_schedule='cosine', dense=True, density=1.0, epochs=100, fp16=False, growth='momentum', iters=1, l1=0.0, l2=0.0005, log_interval=100, lr=0.1, max_threads=10, model='MobileNet', momentum=0.9, no_cuda=False, optimizer='sgd', prune='magnitude', prune_rate=0.5, redistribution='momentum', resume=None, save_features=False, save_model='./models/model.pt', seed=17, start_epoch=1, test_batch_size=100, valid_split=0.1, verbose=True)
21:28:33: 


21:28:33: ================================================================================
21:28:33: 
Iteration start: 1/1

21:28:34: MobileNet(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (dw2_1): DepthWiseBlock(
    (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
    (bn_dw): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw2_2): DepthWiseBlock(
    (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
    (bn_dw): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw3_1): DepthWiseBlock(
    (conv_dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
    (bn_dw): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw3_2): DepthWiseBlock(
    (conv_dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
    (bn_dw): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw4_1): DepthWiseBlock(
    (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
    (bn_dw): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw4_2): DepthWiseBlock(
    (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
    (bn_dw): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_1): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_2): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_3): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_4): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_5): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_6): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw6): DepthWiseBlock(
    (conv_dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)
    (bn_dw): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (fc): Linear(in_features=1024, out_features=100, bias=True)
)
21:28:34: ============================================================
21:28:34: MobileNet
21:28:34: ============================================================
21:28:34: ============================================================
21:28:34: Prune mode: magnitude
21:28:34: Growth mode: momentum
21:28:34: Redistribution mode: momentum
21:28:34: ============================================================
21:28:35: Train Epoch: 1 [0/45000 (0%)]	Loss: 4.641055
21:28:40: Train Epoch: 1 [10000/45000 (22%)]	Loss: 2.070999
21:28:44: Train Epoch: 1 [20000/45000 (44%)]	Loss: 2.191387
21:28:49: Train Epoch: 1 [30000/45000 (67%)]	Loss: 1.572804
21:28:54: Train Epoch: 1 [40000/45000 (89%)]	Loss: 1.645447
21:28:58: 
Evaluation: Average loss: -7.4973, Accuracy: 1916/5000 (38.320%)

21:28:58: Current learning rate: 0.1. Time taken for epoch: 23.76 seconds.

21:28:59: Train Epoch: 2 [0/45000 (0%)]	Loss: 1.539217
21:29:04: Train Epoch: 2 [10000/45000 (22%)]	Loss: 1.506656
21:29:14: Namespace(batch_size=100, bench=True, data='cifar', decay_frequency=25000, decay_schedule='cosine', dense=True, density=0.05, epochs=100, fp16=False, growth='momentum', iters=1, l1=0.0, l2=0.0005, log_interval=100, lr=0.1, max_threads=10, model='MobileNet', momentum=0.9, no_cuda=False, optimizer='sgd', prune='magnitude', prune_rate=0.5, redistribution='momentum', resume=None, save_features=False, save_model='./models/model.pt', seed=17, start_epoch=1, test_batch_size=100, valid_split=0.1, verbose=True)
21:29:14: 


21:29:14: ================================================================================
21:29:14: 
Iteration start: 1/1

21:29:15: MobileNet(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (dw2_1): DepthWiseBlock(
    (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
    (bn_dw): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw2_2): DepthWiseBlock(
    (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
    (bn_dw): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw3_1): DepthWiseBlock(
    (conv_dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
    (bn_dw): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw3_2): DepthWiseBlock(
    (conv_dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
    (bn_dw): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw4_1): DepthWiseBlock(
    (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
    (bn_dw): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw4_2): DepthWiseBlock(
    (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
    (bn_dw): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_1): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_2): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_3): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_4): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_5): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_6): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw6): DepthWiseBlock(
    (conv_dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)
    (bn_dw): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (fc): Linear(in_features=1024, out_features=100, bias=True)
)
21:29:15: ============================================================
21:29:15: MobileNet
21:29:15: ============================================================
21:29:15: ============================================================
21:29:15: Prune mode: magnitude
21:29:15: Growth mode: momentum
21:29:15: Redistribution mode: momentum
21:29:15: ============================================================
21:29:16: Train Epoch: 1 [0/45000 (0%)]	Loss: 4.641055
21:29:21: Train Epoch: 1 [10000/45000 (22%)]	Loss: 2.070999
21:29:26: Train Epoch: 1 [20000/45000 (44%)]	Loss: 2.191387
21:29:30: Train Epoch: 1 [30000/45000 (67%)]	Loss: 1.572804
21:29:35: Train Epoch: 1 [40000/45000 (89%)]	Loss: 1.645447
21:29:39: 
Evaluation: Average loss: -7.4973, Accuracy: 1916/5000 (38.320%)

21:29:39: Current learning rate: 0.1. Time taken for epoch: 23.74 seconds.

21:29:40: Train Epoch: 2 [0/45000 (0%)]	Loss: 1.539217
21:29:50: Namespace(batch_size=100, bench=True, data='cifar', decay_frequency=25000, decay_schedule='cosine', dense=False, density=0.05, epochs=100, fp16=False, growth='momentum', iters=1, l1=0.0, l2=0.0005, log_interval=100, lr=0.1, max_threads=10, model='MobileNet', momentum=0.9, no_cuda=False, optimizer='sgd', prune='magnitude', prune_rate=0.5, redistribution='momentum', resume=None, save_features=False, save_model='./models/model.pt', seed=17, start_epoch=1, test_batch_size=100, valid_split=0.1, verbose=True)
21:29:50: 


21:29:50: ================================================================================
21:29:50: 
Iteration start: 1/1

21:29:51: MobileNet(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (dw2_1): DepthWiseBlock(
    (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
    (bn_dw): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw2_2): DepthWiseBlock(
    (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
    (bn_dw): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw3_1): DepthWiseBlock(
    (conv_dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
    (bn_dw): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw3_2): DepthWiseBlock(
    (conv_dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
    (bn_dw): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw4_1): DepthWiseBlock(
    (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
    (bn_dw): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw4_2): DepthWiseBlock(
    (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
    (bn_dw): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_1): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_2): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_3): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_4): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_5): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_6): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw6): DepthWiseBlock(
    (conv_dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)
    (bn_dw): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (fc): Linear(in_features=1024, out_features=100, bias=True)
)
21:29:51: ============================================================
21:29:51: MobileNet
21:29:51: ============================================================
21:29:51: ============================================================
21:29:51: Prune mode: magnitude
21:29:51: Growth mode: momentum
21:29:51: Redistribution mode: momentum
21:29:51: ============================================================
21:29:52: Train Epoch: 1 [0/45000 (0%)]	Loss: 4.604723
21:29:57: Train Epoch: 1 [10000/45000 (22%)]	Loss: nan
21:30:02: Train Epoch: 1 [20000/45000 (44%)]	Loss: nan
21:30:07: Train Epoch: 1 [30000/45000 (67%)]	Loss: nan
21:30:12: Train Epoch: 1 [40000/45000 (89%)]	Loss: nan
21:30:16: 
Evaluation: Average loss: nan, Accuracy: 493/5000 (9.860%)

21:44:13: Namespace(batch_size=100, bench=True, data='cifar', decay_frequency=25000, decay_schedule='cosine', dense=True, density=0.05, epochs=100, fp16=False, growth='momentum', iters=1, l1=0.0, l2=0.0005, log_interval=100, lr=0.1, max_threads=10, model='MobileNet', momentum=0.9, no_cuda=False, optimizer='sgd', prune='magnitude', prune_rate=0.5, redistribution='momentum', resume=None, save_features=False, save_model='./models/model.pt', seed=17, start_epoch=1, test_batch_size=100, valid_split=0.1, verbose=True)
21:44:13: 


21:44:13: ================================================================================
21:44:13: 
Iteration start: 1/1

21:44:14: MobileNet(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (dw2_1): DepthWiseBlock(
    (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
    (bn_dw): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw2_2): DepthWiseBlock(
    (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
    (bn_dw): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw3_1): DepthWiseBlock(
    (conv_dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
    (bn_dw): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw3_2): DepthWiseBlock(
    (conv_dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
    (bn_dw): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw4_1): DepthWiseBlock(
    (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
    (bn_dw): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw4_2): DepthWiseBlock(
    (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
    (bn_dw): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_1): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_2): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_3): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_4): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_5): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_6): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw6): DepthWiseBlock(
    (conv_dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)
    (bn_dw): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (fc): Linear(in_features=1024, out_features=100, bias=True)
)
21:44:14: ============================================================
21:44:14: MobileNet
21:44:14: ============================================================
21:44:14: ============================================================
21:44:14: Prune mode: magnitude
21:44:14: Growth mode: momentum
21:44:14: Redistribution mode: momentum
21:44:14: ============================================================
21:44:15: Train Epoch: 1 [0/45000 (0%)]	Loss: 4.641055
21:44:20: Train Epoch: 1 [10000/45000 (22%)]	Loss: 2.070999
21:44:25: Train Epoch: 1 [20000/45000 (44%)]	Loss: 2.191387
21:44:30: Train Epoch: 1 [30000/45000 (67%)]	Loss: 1.572804
21:44:35: Train Epoch: 1 [40000/45000 (89%)]	Loss: 1.645447
21:44:39: 
Evaluation: Average loss: -7.4973, Accuracy: 1916/5000 (38.320%)

21:44:39: Current learning rate: 0.1. Time taken for epoch: 24.69 seconds.

21:44:40: Train Epoch: 2 [0/45000 (0%)]	Loss: 1.539217
21:44:45: Train Epoch: 2 [10000/45000 (22%)]	Loss: 1.506656
21:44:50: Train Epoch: 2 [20000/45000 (44%)]	Loss: 1.660257
21:44:55: Train Epoch: 2 [30000/45000 (67%)]	Loss: 1.391268
21:45:00: Train Epoch: 2 [40000/45000 (89%)]	Loss: 1.605617
21:45:04: 
Evaluation: Average loss: -7.5427, Accuracy: 2206/5000 (44.120%)

21:45:04: Current learning rate: 0.1. Time taken for epoch: 24.98 seconds.

21:45:05: Train Epoch: 3 [0/45000 (0%)]	Loss: 1.405193
21:45:10: Train Epoch: 3 [10000/45000 (22%)]	Loss: 1.692926
21:45:14: Train Epoch: 3 [20000/45000 (44%)]	Loss: 1.380153
21:45:19: Train Epoch: 3 [30000/45000 (67%)]	Loss: 1.360908
21:45:24: Train Epoch: 3 [40000/45000 (89%)]	Loss: 1.352115
21:45:28: 
Evaluation: Average loss: -8.2287, Accuracy: 2511/5000 (50.220%)

21:45:28: Current learning rate: 0.1. Time taken for epoch: 24.04 seconds.

21:45:29: Train Epoch: 4 [0/45000 (0%)]	Loss: 1.296323
21:45:34: Train Epoch: 4 [10000/45000 (22%)]	Loss: 1.222319
21:45:39: Train Epoch: 4 [20000/45000 (44%)]	Loss: 1.121193
21:45:43: Train Epoch: 4 [30000/45000 (67%)]	Loss: 1.495911
21:45:48: Train Epoch: 4 [40000/45000 (89%)]	Loss: 1.157913
21:45:52: 
Evaluation: Average loss: -8.1894, Accuracy: 2618/5000 (52.360%)

21:45:52: Current learning rate: 0.1. Time taken for epoch: 24.28 seconds.

21:45:53: Train Epoch: 5 [0/45000 (0%)]	Loss: 1.220667
21:45:58: Train Epoch: 5 [10000/45000 (22%)]	Loss: 1.145001
21:46:03: Train Epoch: 5 [20000/45000 (44%)]	Loss: 1.251591
21:46:08: Train Epoch: 5 [30000/45000 (67%)]	Loss: 1.225516
21:46:13: Train Epoch: 5 [40000/45000 (89%)]	Loss: 1.051940
21:46:17: 
Evaluation: Average loss: -8.4317, Accuracy: 2835/5000 (56.700%)

21:46:17: Current learning rate: 0.1. Time taken for epoch: 24.49 seconds.

21:46:17: Train Epoch: 6 [0/45000 (0%)]	Loss: 1.173297
21:46:23: Train Epoch: 6 [10000/45000 (22%)]	Loss: 1.195373
21:46:27: Train Epoch: 6 [20000/45000 (44%)]	Loss: 1.166446
21:46:32: Train Epoch: 6 [30000/45000 (67%)]	Loss: 1.066273
21:46:37: Train Epoch: 6 [40000/45000 (89%)]	Loss: 1.092280
21:46:41: 
Evaluation: Average loss: -8.5108, Accuracy: 2941/5000 (58.820%)

21:46:41: Current learning rate: 0.1. Time taken for epoch: 24.24 seconds.

21:46:41: Train Epoch: 7 [0/45000 (0%)]	Loss: 1.209516
21:46:47: Train Epoch: 7 [10000/45000 (22%)]	Loss: 1.300370
21:46:51: Train Epoch: 7 [20000/45000 (44%)]	Loss: 1.019166
21:46:56: Train Epoch: 7 [30000/45000 (67%)]	Loss: 1.229778
21:47:01: Train Epoch: 7 [40000/45000 (89%)]	Loss: 0.902035
21:47:05: 
Evaluation: Average loss: -8.4379, Accuracy: 2861/5000 (57.220%)

21:47:05: Current learning rate: 0.1. Time taken for epoch: 24.50 seconds.

21:47:06: Train Epoch: 8 [0/45000 (0%)]	Loss: 1.122509
21:47:11: Train Epoch: 8 [10000/45000 (22%)]	Loss: 1.098168
21:47:16: Train Epoch: 8 [20000/45000 (44%)]	Loss: 1.152307
21:47:21: Train Epoch: 8 [30000/45000 (67%)]	Loss: 0.939621
21:47:26: Train Epoch: 8 [40000/45000 (89%)]	Loss: 0.934100
21:47:29: 
Evaluation: Average loss: -8.7891, Accuracy: 3246/5000 (64.920%)

21:47:30: Current learning rate: 0.1. Time taken for epoch: 24.06 seconds.

21:47:30: Train Epoch: 9 [0/45000 (0%)]	Loss: 1.052353
21:47:35: Train Epoch: 9 [10000/45000 (22%)]	Loss: 0.864117
21:47:40: Train Epoch: 9 [20000/45000 (44%)]	Loss: 0.877453
21:47:45: Train Epoch: 9 [30000/45000 (67%)]	Loss: 0.865385
21:47:50: Train Epoch: 9 [40000/45000 (89%)]	Loss: 1.255493
21:47:54: 
Evaluation: Average loss: -8.9968, Accuracy: 3197/5000 (63.940%)

21:47:54: Current learning rate: 0.1. Time taken for epoch: 24.10 seconds.

21:47:54: Train Epoch: 10 [0/45000 (0%)]	Loss: 1.041793
21:48:00: Train Epoch: 10 [10000/45000 (22%)]	Loss: 1.026022
21:48:05: Train Epoch: 10 [20000/45000 (44%)]	Loss: 0.984909
21:48:09: Train Epoch: 10 [30000/45000 (67%)]	Loss: 1.070504
21:48:14: Train Epoch: 10 [40000/45000 (89%)]	Loss: 1.037909
21:48:18: 
Evaluation: Average loss: -9.0468, Accuracy: 2935/5000 (58.700%)

21:48:18: Current learning rate: 0.1. Time taken for epoch: 24.31 seconds.

21:48:19: Train Epoch: 11 [0/45000 (0%)]	Loss: 0.903475
21:48:24: Train Epoch: 11 [10000/45000 (22%)]	Loss: 0.917096
21:48:28: Train Epoch: 11 [20000/45000 (44%)]	Loss: 1.066228
21:48:33: Train Epoch: 11 [30000/45000 (67%)]	Loss: 1.015116
21:48:39: Train Epoch: 11 [40000/45000 (89%)]	Loss: 0.892001
21:48:43: 
Evaluation: Average loss: -8.7921, Accuracy: 3232/5000 (64.640%)

21:48:43: Current learning rate: 0.1. Time taken for epoch: 24.66 seconds.

21:48:43: Train Epoch: 12 [0/45000 (0%)]	Loss: 0.896391
21:48:48: Train Epoch: 12 [10000/45000 (22%)]	Loss: 1.040409
21:48:53: Train Epoch: 12 [20000/45000 (44%)]	Loss: 0.872414
21:48:58: Train Epoch: 12 [30000/45000 (67%)]	Loss: 0.968072
21:49:03: Train Epoch: 12 [40000/45000 (89%)]	Loss: 0.936087
21:49:07: 
Evaluation: Average loss: -9.1224, Accuracy: 3245/5000 (64.900%)

21:49:07: Current learning rate: 0.1. Time taken for epoch: 24.45 seconds.

21:49:08: Train Epoch: 13 [0/45000 (0%)]	Loss: 0.952147
21:49:12: Train Epoch: 13 [10000/45000 (22%)]	Loss: 1.037147
21:52:49: Namespace(batch_size=100, bench=True, data='cifar', decay_frequency=25000, decay_schedule='cosine', dense=True, density=1.0, epochs=100, fp16=False, growth='momentum', iters=1, l1=0.0, l2=0.0005, log_interval=100, lr=0.1, max_threads=10, model='vgg-d', momentum=0.9, no_cuda=False, optimizer='sgd', prune='magnitude', prune_rate=0.7, redistribution='momentum', resume=None, save_features=False, save_model='./models/model.pt', seed=17, start_epoch=1, test_batch_size=100, valid_split=0.1, verbose=True)
21:52:49: 


21:52:49: ================================================================================
21:52:49: 
Iteration start: 1/1

21:52:50: VGG16(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (9): ReLU(inplace=True)
    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): ReLU(inplace=True)
    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): ReLU(inplace=True)
    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (19): ReLU(inplace=True)
    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): ReLU(inplace=True)
    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (29): ReLU(inplace=True)
    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): ReLU(inplace=True)
    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): ReLU(inplace=True)
    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (39): ReLU(inplace=True)
    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): ReLU(inplace=True)
    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (classifier): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU(inplace=True)
    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): Linear(in_features=512, out_features=512, bias=True)
    (4): ReLU(inplace=True)
    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Linear(in_features=512, out_features=10, bias=True)
  )
)
21:52:50: ============================================================
21:52:50: vgg-d
21:52:50: ============================================================
21:52:50: ============================================================
21:52:50: Prune mode: magnitude
21:52:50: Growth mode: momentum
21:52:50: Redistribution mode: momentum
21:52:50: ============================================================
21:52:52: Train Epoch: 1 [0/45000 (0%)]	Loss: 2.493264
21:52:58: Train Epoch: 1 [10000/45000 (22%)]	Loss: 3.538699
21:53:05: Train Epoch: 1 [20000/45000 (44%)]	Loss: 1.976608
21:53:11: Train Epoch: 1 [30000/45000 (67%)]	Loss: 1.881186
21:53:18: Train Epoch: 1 [40000/45000 (89%)]	Loss: 1.843703
21:53:24: 
Evaluation: Average loss: 1.8959, Accuracy: 1242/5000 (24.840%)

21:53:24: Current learning rate: 0.1. Time taken for epoch: 33.84 seconds.

21:53:25: Train Epoch: 2 [0/45000 (0%)]	Loss: 1.915445
21:53:31: Train Epoch: 2 [10000/45000 (22%)]	Loss: 1.863323
21:53:38: Train Epoch: 2 [20000/45000 (44%)]	Loss: 1.873123
21:53:44: Train Epoch: 2 [30000/45000 (67%)]	Loss: 1.695018
21:53:51: Train Epoch: 2 [40000/45000 (89%)]	Loss: 1.553206
21:53:57: 
Evaluation: Average loss: 1.6408, Accuracy: 1828/5000 (36.560%)

21:53:57: Current learning rate: 0.1. Time taken for epoch: 33.16 seconds.

21:53:58: Train Epoch: 3 [0/45000 (0%)]	Loss: 1.825549
22:10:52: Namespace(batch_size=100, bench=True, data='cifar', decay_frequency=25000, decay_schedule='cosine', dense=True, density=0.05, epochs=100, fp16=False, growth='momentum', iters=1, l1=0.0, l2=0.0005, log_interval=100, lr=0.1, max_threads=10, model='MobileNet', momentum=0.9, no_cuda=False, optimizer='sgd', prune='magnitude', prune_rate=0.5, redistribution='momentum', resume=None, save_features=False, save_model='./models/model.pt', seed=17, start_epoch=1, test_batch_size=100, valid_split=0.1, verbose=True)
22:10:52: 


22:10:52: ================================================================================
22:10:52: 
Iteration start: 1/1

22:10:53: MobileNet(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (dw2_1): DepthWiseBlock(
    (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
    (bn_dw): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw2_2): DepthWiseBlock(
    (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
    (bn_dw): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw3_1): DepthWiseBlock(
    (conv_dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
    (bn_dw): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw3_2): DepthWiseBlock(
    (conv_dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
    (bn_dw): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw4_1): DepthWiseBlock(
    (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
    (bn_dw): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw4_2): DepthWiseBlock(
    (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
    (bn_dw): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_1): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_2): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_3): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_4): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_5): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw5_6): DepthWiseBlock(
    (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)
    (bn_dw): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (dw6): DepthWiseBlock(
    (conv_dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)
    (bn_dw): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_sep): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn_sep): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (fc): Linear(in_features=1024, out_features=100, bias=True)
)
22:10:53: ============================================================
22:10:53: MobileNet
22:10:53: ============================================================
22:10:53: ============================================================
22:10:53: Prune mode: magnitude
22:10:53: Growth mode: momentum
22:10:53: Redistribution mode: momentum
22:10:53: ============================================================
22:10:54: Train Epoch: 1 [0/45000 (0%)]	Loss: 4.641055
22:10:59: Train Epoch: 1 [10000/45000 (22%)]	Loss: 2.070999
22:11:04: Train Epoch: 1 [20000/45000 (44%)]	Loss: 2.191387
22:11:08: Train Epoch: 1 [30000/45000 (67%)]	Loss: 1.572804
22:11:13: Train Epoch: 1 [40000/45000 (89%)]	Loss: 1.645447
22:11:17: 
Evaluation: Average loss: 0.0168, Accuracy: 1916/5000 (38.320%)

22:11:17: Current learning rate: 0.1. Time taken for epoch: 24.05 seconds.

22:11:18: Train Epoch: 2 [0/45000 (0%)]	Loss: 1.539217
22:11:23: Train Epoch: 2 [10000/45000 (22%)]	Loss: 1.506656
22:11:27: Train Epoch: 2 [20000/45000 (44%)]	Loss: 1.660257
22:11:32: Train Epoch: 2 [30000/45000 (67%)]	Loss: 1.391268
22:11:37: Train Epoch: 2 [40000/45000 (89%)]	Loss: 1.605617
22:11:41: 
Evaluation: Average loss: 0.0155, Accuracy: 2206/5000 (44.120%)

22:11:41: Current learning rate: 0.1. Time taken for epoch: 23.91 seconds.

22:11:42: Train Epoch: 3 [0/45000 (0%)]	Loss: 1.405193
22:11:46: Train Epoch: 3 [10000/45000 (22%)]	Loss: 1.692926
22:11:51: Train Epoch: 3 [20000/45000 (44%)]	Loss: 1.380153
22:11:56: Train Epoch: 3 [30000/45000 (67%)]	Loss: 1.360908
22:12:00: Train Epoch: 3 [40000/45000 (89%)]	Loss: 1.352115
22:12:04: 
Evaluation: Average loss: 0.0138, Accuracy: 2511/5000 (50.220%)

22:12:05: Current learning rate: 0.1. Time taken for epoch: 23.65 seconds.

22:12:05: Train Epoch: 4 [0/45000 (0%)]	Loss: 1.296323
22:12:10: Train Epoch: 4 [10000/45000 (22%)]	Loss: 1.222319
22:12:15: Train Epoch: 4 [20000/45000 (44%)]	Loss: 1.121193
22:12:19: Train Epoch: 4 [30000/45000 (67%)]	Loss: 1.495911
22:12:24: Train Epoch: 4 [40000/45000 (89%)]	Loss: 1.157913
22:12:28: 
Evaluation: Average loss: 0.0132, Accuracy: 2618/5000 (52.360%)

22:12:28: Current learning rate: 0.1. Time taken for epoch: 23.31 seconds.

22:12:29: Train Epoch: 5 [0/45000 (0%)]	Loss: 1.220667
22:12:34: Train Epoch: 5 [10000/45000 (22%)]	Loss: 1.145001
22:12:39: Train Epoch: 5 [20000/45000 (44%)]	Loss: 1.251591
22:12:43: Train Epoch: 5 [30000/45000 (67%)]	Loss: 1.225516
22:12:48: Train Epoch: 5 [40000/45000 (89%)]	Loss: 1.051940
22:12:52: 
Evaluation: Average loss: 0.0121, Accuracy: 2835/5000 (56.700%)

22:12:52: Current learning rate: 0.1. Time taken for epoch: 24.34 seconds.

22:12:53: Train Epoch: 6 [0/45000 (0%)]	Loss: 1.173297
22:12:58: Train Epoch: 6 [10000/45000 (22%)]	Loss: 1.195373
22:13:03: Train Epoch: 6 [20000/45000 (44%)]	Loss: 1.166446
22:13:08: Train Epoch: 6 [30000/45000 (67%)]	Loss: 1.066273
22:13:13: Train Epoch: 6 [40000/45000 (89%)]	Loss: 1.092280
22:13:17: 
Evaluation: Average loss: 0.0118, Accuracy: 2941/5000 (58.820%)

22:13:17: Current learning rate: 0.1. Time taken for epoch: 24.38 seconds.

22:13:17: Train Epoch: 7 [0/45000 (0%)]	Loss: 1.209516
22:13:22: Train Epoch: 7 [10000/45000 (22%)]	Loss: 1.300370
22:13:27: Train Epoch: 7 [20000/45000 (44%)]	Loss: 1.019166
22:13:32: Train Epoch: 7 [30000/45000 (67%)]	Loss: 1.229778
22:13:37: Train Epoch: 7 [40000/45000 (89%)]	Loss: 0.902035
22:13:40: 
Evaluation: Average loss: 0.0120, Accuracy: 2861/5000 (57.220%)

22:13:40: Current learning rate: 0.1. Time taken for epoch: 23.90 seconds.

22:13:41: Train Epoch: 8 [0/45000 (0%)]	Loss: 1.122509
22:13:46: Train Epoch: 8 [10000/45000 (22%)]	Loss: 1.098168
22:13:51: Train Epoch: 8 [20000/45000 (44%)]	Loss: 1.152307
22:13:55: Train Epoch: 8 [30000/45000 (67%)]	Loss: 0.939621
22:14:00: Train Epoch: 8 [40000/45000 (89%)]	Loss: 0.934100
22:14:04: 
Evaluation: Average loss: 0.0102, Accuracy: 3246/5000 (64.920%)

22:14:04: Current learning rate: 0.1. Time taken for epoch: 23.74 seconds.

22:14:05: Train Epoch: 9 [0/45000 (0%)]	Loss: 1.052353
22:14:10: Train Epoch: 9 [10000/45000 (22%)]	Loss: 0.864117
22:14:15: Train Epoch: 9 [20000/45000 (44%)]	Loss: 0.877453
22:14:19: Train Epoch: 9 [30000/45000 (67%)]	Loss: 0.865385
22:14:24: Train Epoch: 9 [40000/45000 (89%)]	Loss: 1.255493
22:14:28: 
Evaluation: Average loss: 0.0104, Accuracy: 3197/5000 (63.940%)

22:14:28: Current learning rate: 0.1. Time taken for epoch: 23.89 seconds.

22:14:29: Train Epoch: 10 [0/45000 (0%)]	Loss: 1.041793
22:14:34: Train Epoch: 10 [10000/45000 (22%)]	Loss: 1.026022
22:14:38: Train Epoch: 10 [20000/45000 (44%)]	Loss: 0.984909
22:14:43: Train Epoch: 10 [30000/45000 (67%)]	Loss: 1.070504
22:14:48: Train Epoch: 10 [40000/45000 (89%)]	Loss: 1.037909
22:14:52: 
Evaluation: Average loss: 0.0120, Accuracy: 2935/5000 (58.700%)

22:14:52: Current learning rate: 0.1. Time taken for epoch: 23.65 seconds.

22:14:52: Train Epoch: 11 [0/45000 (0%)]	Loss: 0.903475
22:14:57: Train Epoch: 11 [10000/45000 (22%)]	Loss: 0.917096
22:15:02: Train Epoch: 11 [20000/45000 (44%)]	Loss: 1.066228
22:15:07: Train Epoch: 11 [30000/45000 (67%)]	Loss: 1.015116
22:15:12: Train Epoch: 11 [40000/45000 (89%)]	Loss: 0.892001
22:15:15: 
Evaluation: Average loss: 0.0103, Accuracy: 3232/5000 (64.640%)

22:15:16: Current learning rate: 0.1. Time taken for epoch: 23.79 seconds.

22:15:16: Train Epoch: 12 [0/45000 (0%)]	Loss: 0.896391
22:15:21: Train Epoch: 12 [10000/45000 (22%)]	Loss: 1.040409
22:15:26: Train Epoch: 12 [20000/45000 (44%)]	Loss: 0.872414
22:15:31: Train Epoch: 12 [30000/45000 (67%)]	Loss: 0.968072
22:15:35: Train Epoch: 12 [40000/45000 (89%)]	Loss: 0.936087
22:15:39: 
Evaluation: Average loss: 0.0102, Accuracy: 3245/5000 (64.900%)

22:15:39: Current learning rate: 0.1. Time taken for epoch: 23.70 seconds.

22:15:40: Train Epoch: 13 [0/45000 (0%)]	Loss: 0.952147
22:15:45: Train Epoch: 13 [10000/45000 (22%)]	Loss: 1.037147
22:15:49: Train Epoch: 13 [20000/45000 (44%)]	Loss: 0.859045
22:15:54: Train Epoch: 13 [30000/45000 (67%)]	Loss: 0.726019
22:15:59: Train Epoch: 13 [40000/45000 (89%)]	Loss: 0.897180
22:16:03: 
Evaluation: Average loss: 0.0111, Accuracy: 3078/5000 (61.560%)

22:16:03: Current learning rate: 0.1. Time taken for epoch: 23.88 seconds.

22:16:04: Train Epoch: 14 [0/45000 (0%)]	Loss: 1.077231
22:16:08: Train Epoch: 14 [10000/45000 (22%)]	Loss: 0.973345
22:16:13: Train Epoch: 14 [20000/45000 (44%)]	Loss: 0.934396
22:16:18: Train Epoch: 14 [30000/45000 (67%)]	Loss: 0.761494
22:16:23: Train Epoch: 14 [40000/45000 (89%)]	Loss: 0.946728
22:16:26: 
Evaluation: Average loss: 0.0097, Accuracy: 3311/5000 (66.220%)

22:16:26: Current learning rate: 0.1. Time taken for epoch: 23.38 seconds.

22:16:27: Train Epoch: 15 [0/45000 (0%)]	Loss: 0.837940
22:16:32: Train Epoch: 15 [10000/45000 (22%)]	Loss: 0.951340
22:16:37: Train Epoch: 15 [20000/45000 (44%)]	Loss: 0.859055
22:16:41: Train Epoch: 15 [30000/45000 (67%)]	Loss: 0.914733
22:16:46: Train Epoch: 15 [40000/45000 (89%)]	Loss: 0.820461
22:16:50: 
Evaluation: Average loss: 0.0108, Accuracy: 3138/5000 (62.760%)

22:16:50: Current learning rate: 0.1. Time taken for epoch: 23.35 seconds.

22:16:51: Train Epoch: 16 [0/45000 (0%)]	Loss: 0.949087
22:16:55: Train Epoch: 16 [10000/45000 (22%)]	Loss: 1.030977
22:17:00: Train Epoch: 16 [20000/45000 (44%)]	Loss: 1.101172
22:17:05: Train Epoch: 16 [30000/45000 (67%)]	Loss: 0.774793
22:17:09: Train Epoch: 16 [40000/45000 (89%)]	Loss: 1.014955
22:17:13: 
Evaluation: Average loss: 0.0106, Accuracy: 3270/5000 (65.400%)

22:17:13: Current learning rate: 0.1. Time taken for epoch: 23.42 seconds.

22:17:14: Train Epoch: 17 [0/45000 (0%)]	Loss: 0.911990
22:17:19: Train Epoch: 17 [10000/45000 (22%)]	Loss: 1.022918
22:17:23: Train Epoch: 17 [20000/45000 (44%)]	Loss: 0.746091
22:17:28: Train Epoch: 17 [30000/45000 (67%)]	Loss: 1.168708
22:17:32: Train Epoch: 17 [40000/45000 (89%)]	Loss: 0.960175
22:17:37: 
Evaluation: Average loss: 0.0102, Accuracy: 3340/5000 (66.800%)

22:17:37: Current learning rate: 0.1. Time taken for epoch: 23.46 seconds.

22:17:37: Train Epoch: 18 [0/45000 (0%)]	Loss: 0.910733
22:17:42: Train Epoch: 18 [10000/45000 (22%)]	Loss: 0.963349
22:17:47: Train Epoch: 18 [20000/45000 (44%)]	Loss: 0.970611
22:17:52: Train Epoch: 18 [30000/45000 (67%)]	Loss: 0.635672
22:17:56: Train Epoch: 18 [40000/45000 (89%)]	Loss: 0.885292
22:18:01: 
Evaluation: Average loss: 0.0109, Accuracy: 3197/5000 (63.940%)

22:18:01: Current learning rate: 0.1. Time taken for epoch: 23.84 seconds.

22:18:01: Train Epoch: 19 [0/45000 (0%)]	Loss: 0.665436
22:18:06: Train Epoch: 19 [10000/45000 (22%)]	Loss: 0.843730
22:18:11: Train Epoch: 19 [20000/45000 (44%)]	Loss: 1.012961
22:18:15: Train Epoch: 19 [30000/45000 (67%)]	Loss: 1.097223
22:18:20: Namespace(batch_size=100, bench=True, data='mnist', decay_frequency=25000, decay_schedule='cosine', dense=True, density=1.0, epochs=100, fp16=False, growth='momentum', iters=1, l1=0.0, l2=0.0005, log_interval=100, lr=0.1, max_threads=10, model='lenet300-100', momentum=0.9, no_cuda=False, optimizer='sgd', prune='magnitude', prune_rate=0.7, redistribution='momentum', resume=None, save_features=False, save_model='./models/model.pt', seed=17, start_epoch=1, test_batch_size=100, valid_split=0.1, verbose=True)
22:18:20: 


22:18:20: ================================================================================
22:18:20: 
Iteration start: 1/1

22:18:22: LeNet_300_100(
  (fc1): Linear(in_features=784, out_features=300, bias=True)
  (fc2): Linear(in_features=300, out_features=100, bias=True)
  (fc3): Linear(in_features=100, out_features=10, bias=True)
)
22:18:22: ============================================================
22:18:22: lenet300-100
22:18:22: ============================================================
22:18:22: ============================================================
22:18:22: Prune mode: magnitude
22:18:22: Growth mode: momentum
22:18:22: Redistribution mode: momentum
22:18:22: ============================================================
22:18:22: Train Epoch: 1 [0/54000 (0%)]	Loss: 2.304187
22:18:23: Train Epoch: 1 [10000/54000 (19%)]	Loss: 0.241723
22:18:25: Train Epoch: 1 [20000/54000 (37%)]	Loss: 0.240182
22:18:42: Namespace(batch_size=100, bench=True, data='cifar', decay_frequency=25000, decay_schedule='cosine', dense=True, density=1.0, epochs=100, fp16=False, growth='momentum', iters=1, l1=0.0, l2=0.0005, log_interval=100, lr=0.1, max_threads=10, model='vgg-d', momentum=0.9, no_cuda=False, optimizer='sgd', prune='magnitude', prune_rate=0.7, redistribution='momentum', resume=None, save_features=False, save_model='./models/model.pt', seed=17, start_epoch=1, test_batch_size=100, valid_split=0.1, verbose=True)
22:18:42: 


22:18:42: ================================================================================
22:18:42: 
Iteration start: 1/1

22:18:44: VGG16(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (9): ReLU(inplace=True)
    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): ReLU(inplace=True)
    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): ReLU(inplace=True)
    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (19): ReLU(inplace=True)
    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): ReLU(inplace=True)
    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (29): ReLU(inplace=True)
    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): ReLU(inplace=True)
    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): ReLU(inplace=True)
    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (39): ReLU(inplace=True)
    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): ReLU(inplace=True)
    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (classifier): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU(inplace=True)
    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): Linear(in_features=512, out_features=512, bias=True)
    (4): ReLU(inplace=True)
    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Linear(in_features=512, out_features=10, bias=True)
  )
)
22:18:44: ============================================================
22:18:44: vgg-d
22:18:44: ============================================================
22:18:44: ============================================================
22:18:44: Prune mode: magnitude
22:18:44: Growth mode: momentum
22:18:44: Redistribution mode: momentum
22:18:44: ============================================================
22:18:44: Train Epoch: 1 [0/45000 (0%)]	Loss: 2.493264
22:18:51: Train Epoch: 1 [10000/45000 (22%)]	Loss: 3.538699
22:18:57: Train Epoch: 1 [20000/45000 (44%)]	Loss: 1.976608
22:19:04: Train Epoch: 1 [30000/45000 (67%)]	Loss: 1.881186
22:19:11: Train Epoch: 1 [40000/45000 (89%)]	Loss: 1.843703
22:19:17: 
Evaluation: Average loss: 0.0190, Accuracy: 1242/5000 (24.840%)

22:19:17: Current learning rate: 0.1. Time taken for epoch: 33.04 seconds.

22:19:17: Train Epoch: 2 [0/45000 (0%)]	Loss: 1.915445
22:19:24: Train Epoch: 2 [10000/45000 (22%)]	Loss: 1.863323
22:19:31: Train Epoch: 2 [20000/45000 (44%)]	Loss: 1.873123
22:19:37: Train Epoch: 2 [30000/45000 (67%)]	Loss: 1.695018
22:19:44: Train Epoch: 2 [40000/45000 (89%)]	Loss: 1.553206
22:19:49: 
Evaluation: Average loss: 0.0164, Accuracy: 1828/5000 (36.560%)

22:19:50: Current learning rate: 0.1. Time taken for epoch: 32.83 seconds.

22:19:50: Train Epoch: 3 [0/45000 (0%)]	Loss: 1.825549
22:19:57: Train Epoch: 3 [10000/45000 (22%)]	Loss: 1.526147
22:20:04: Train Epoch: 3 [20000/45000 (44%)]	Loss: 1.675588
22:20:10: Train Epoch: 3 [30000/45000 (67%)]	Loss: 1.463606
22:20:16: Train Epoch: 3 [40000/45000 (89%)]	Loss: 1.686426
22:20:22: 
Evaluation: Average loss: 0.0155, Accuracy: 2021/5000 (40.420%)

22:20:22: Current learning rate: 0.1. Time taken for epoch: 32.87 seconds.

22:20:23: Train Epoch: 4 [0/45000 (0%)]	Loss: 1.457540
22:20:30: Train Epoch: 4 [10000/45000 (22%)]	Loss: 1.472508
22:20:36: Train Epoch: 4 [20000/45000 (44%)]	Loss: 1.503165
22:20:43: Train Epoch: 4 [30000/45000 (67%)]	Loss: 1.230379
22:20:49: Train Epoch: 4 [40000/45000 (89%)]	Loss: 1.479682
22:20:55: 
Evaluation: Average loss: 0.0139, Accuracy: 2413/5000 (48.260%)

22:20:55: Current learning rate: 0.1. Time taken for epoch: 32.81 seconds.

22:20:56: Train Epoch: 5 [0/45000 (0%)]	Loss: 1.192894
22:25:07: Namespace(batch_size=100, bench=True, data='cifar', decay_frequency=25000, decay_schedule='cosine', dense=True, density=1.0, epochs=100, fp16=False, growth='momentum', iters=1, l1=0.0, l2=0.0005, log_interval=100, lr=0.1, max_threads=10, model='vgg-d', momentum=0.9, no_cuda=False, optimizer='sgd', prune='magnitude', prune_rate=0.7, redistribution='momentum', resume=None, save_features=False, save_model='./models/model.pt', seed=17, start_epoch=1, test_batch_size=100, valid_split=0.1, verbose=True)
22:25:07: 


22:25:07: ================================================================================
22:25:07: 
Iteration start: 1/1

22:25:08: VGG16(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (9): ReLU(inplace=True)
    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): ReLU(inplace=True)
    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): ReLU(inplace=True)
    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (19): ReLU(inplace=True)
    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): ReLU(inplace=True)
    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (29): ReLU(inplace=True)
    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): ReLU(inplace=True)
    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): ReLU(inplace=True)
    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (39): ReLU(inplace=True)
    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): ReLU(inplace=True)
    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (classifier): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU(inplace=True)
    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): Linear(in_features=512, out_features=512, bias=True)
    (4): ReLU(inplace=True)
    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Linear(in_features=512, out_features=10, bias=True)
  )
)
22:25:08: ============================================================
22:25:08: vgg-d
22:25:08: ============================================================
22:25:08: ============================================================
22:25:08: Prune mode: magnitude
22:25:08: Growth mode: momentum
22:25:08: Redistribution mode: momentum
22:25:08: ============================================================
22:25:09: Train Epoch: 1 [0/45000 (0%)]	Loss: 2.493264
22:25:15: Train Epoch: 1 [10000/45000 (22%)]	Loss: 3.538699
22:25:22: Train Epoch: 1 [20000/45000 (44%)]	Loss: 1.976608
22:25:28: Train Epoch: 1 [30000/45000 (67%)]	Loss: 1.881186
22:25:35: Train Epoch: 1 [40000/45000 (89%)]	Loss: 1.843703
22:25:41: 
Evaluation: Average loss: 0.0190, Accuracy: 1242/5000 (24.840%)

22:25:41: Current learning rate: 0.1. Time taken for epoch: 32.85 seconds.

22:25:42: Train Epoch: 2 [0/45000 (0%)]	Loss: 1.915445
22:25:48: Train Epoch: 2 [10000/45000 (22%)]	Loss: 1.863323
22:25:55: Train Epoch: 2 [20000/45000 (44%)]	Loss: 1.873123
22:26:02: Train Epoch: 2 [30000/45000 (67%)]	Loss: 1.695018
22:26:08: Train Epoch: 2 [40000/45000 (89%)]	Loss: 1.553206
22:26:14: 
Evaluation: Average loss: 0.0164, Accuracy: 1828/5000 (36.560%)

22:26:14: Current learning rate: 0.1. Time taken for epoch: 33.36 seconds.

22:26:15: Train Epoch: 3 [0/45000 (0%)]	Loss: 1.825549
